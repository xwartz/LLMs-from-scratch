Search.setIndex({"alltitles": {"1) 124 M GPT-2 Baseline": [[68, "m-gpt-2-baseline"]], "1) A brief introduction to DPO": [[84, "a-brief-introduction-to-dpo"]], "1) CausalAttention MHA wrapper class from chapter 3": [[31, "causalattention-mha-wrapper-class-from-chapter-3"]], "1) Download the dataset": [[48, "download-the-dataset"]], "1) Installation": [[52, "installation"]], "1. Adding special tokens": [[60, "adding-special-tokens"]], "1. Architecture code": [[55, "architecture-code"], [56, "architecture-code"]], "1. Benchmark utilities": [[58, "benchmark-utilities"]], "1. Convert the GPT model implementation step by step": [[53, "convert-the-gpt-model-implementation-step-by-step"]], "1. Convert the Llama model implementation step by step": [[54, "convert-the-llama-model-implementation-step-by-step"]], "1. Create causal mask on the fly": [[61, "create-causal-mask-on-the-fly"]], "1. Download and install Miniforge": [[91, "download-and-install-miniforge"]], "1. Install Python": [[92, "install-python"]], "1. Install Python (if not installed)": [[91, "install-python-if-not-installed"]], "1. Install pixi": [[92, "install-pixi"]], "1. Install uv": [[93, "install-uv"]], "1. The main idea behind byte pair encoding (BPE)": [[24, "the-main-idea-behind-byte-pair-encoding-bpe"]], "1.1 Bits and bytes": [[24, "bits-and-bytes"]], "1.1 Replace LayerNorm with RMSNorm layer": [[53, "replace-layernorm-with-rmsnorm-layer"]], "1.1 Reusing Llama 2 components": [[54, "reusing-llama-2-components"]], "1.2 Building the vocabulary": [[24, "building-the-vocabulary"]], "1.2 Modified RoPE": [[54, "modified-rope"]], "1.2 Replace GELU with SiLU activation": [[53, "replace-gelu-with-silu-activation"]], "1.3 BPE algorithm outline": [[24, "bpe-algorithm-outline"]], "1.3 Grouped-query attention": [[54, "grouped-query-attention"]], "1.3 Update the FeedForward module": [[53, "update-the-feedforward-module"]], "1.4 BPE algorithm example": [[24, "bpe-algorithm-example"]], "1.4 Implement RoPE": [[53, "implement-rope"]], "1.4 Update the TransformerBlock module": [[54, "update-the-transformerblock-module"]], "1.4.1 Concrete example of the encoding part (steps 1 & 2 in section 1.3)": [[24, "concrete-example-of-the-encoding-part-steps-1-2-in-section-1-3"]], "1.4.2 Concrete example of the decoding part (step 3 in section 1.3)": [[24, "concrete-example-of-the-decoding-part-step-3-in-section-1-3"]], "1.5 Add RoPE to MultiHeadAttention module": [[53, "add-rope-to-multiheadattention-module"]], "1.5 Defining the model class": [[54, "defining-the-model-class"]], "1.6 Update the TransformerBlock module": [[53, "update-the-transformerblock-module"]], "1.7 Update the model class": [[53, "update-the-model-class"]], "10. Increasing the batch size": [[61, "increasing-the-batch-size"]], "2) 340 M BERT": [[68, "m-bert"]], "2) Model and text generation settings": [[52, "model-and-text-generation-settings"]], "2) Prepare the dataset": [[48, "prepare-the-dataset"]], "2) Preparing a preference dataset for DPO": [[84, "preparing-a-preference-dataset-for-dpo"]], "2) The multi-head attention class from chapter 3": [[31, "the-multi-head-attention-class-from-chapter-3"]], "2. A simple BPE implementation": [[24, "a-simple-bpe-implementation"]], "2. Create a new virtual environment": [[91, "create-a-new-virtual-environment"]], "2. Create a virtual environment": [[91, "create-a-virtual-environment"]], "2. Initialize model": [[53, "initialize-model"], [54, "initialize-model"], [55, "initialize-model"], [56, "initialize-model"]], "2. Install Python packages and dependencies": [[93, "install-python-packages-and-dependencies"]], "2. Model setup": [[58, "model-setup"]], "2. Updating a pretrained LLM": [[60, "updating-a-pretrained-llm"]], "2. Use  tensor cores": [[61, "use-tensor-cores"]], "2.1 Loading a pretrained GPT model": [[60, "loading-a-pretrained-gpt-model"]], "2.1 Understanding word embeddings": [[14, "understanding-word-embeddings"]], "2.1) Loading a preference dataset": [[84, "loading-a-preference-dataset"]], "2.2 Tokenizing text": [[14, "tokenizing-text"]], "2.2 Using the pretrained GPT model": [[60, "using-the-pretrained-gpt-model"]], "2.2) Creating training, validation, and test splits": [[84, "creating-training-validation-and-test-splits"]], "2.3 Converting tokens into token IDs": [[14, "converting-tokens-into-token-ids"]], "2.3 Updating the embedding layer": [[60, "updating-the-embedding-layer"]], "2.3) Developing a PreferenceDataset class and batch processing function": [[84, "developing-a-preferencedataset-class-and-batch-processing-function"]], "2.4 Adding special context tokens": [[14, "adding-special-context-tokens"]], "2.4 Updating the output layer": [[60, "updating-the-output-layer"]], "2.4) Creating training, validation, and test set data loaders": [[84, "creating-training-validation-and-test-set-data-loaders"]], "2.5 BytePair encoding": [[14, "bytepair-encoding"]], "2.6 Data sampling with a sliding window": [[14, "data-sampling-with-a-sliding-window"]], "2.7 Creating token embeddings": [[14, "creating-token-embeddings"]], "2.8 Encoding word positions": [[14, "encoding-word-positions"]], "3) 66 M DistilBERT": [[68, "m-distilbert"]], "3) An alternative multi-head attention with combined weights": [[31, "an-alternative-multi-head-attention-with-combined-weights"]], "3) Loading a finetuned LLM for DPO alignment": [[84, "loading-a-finetuned-llm-for-dpo-alignment"]], "3) Run the pretraining script": [[48, "run-the-pretraining-script"]], "3) Weight download and loading": [[52, "weight-download-and-loading"]], "3. BPE implementation walkthrough": [[24, "bpe-implementation-walkthrough"]], "3. Fused AdamW optimizer": [[61, "fused-adamw-optimizer"]], "3. Install Python packages and dependencies": [[92, "install-python-packages-and-dependencies"]], "3. Install new Python libraries": [[91, "install-new-python-libraries"]], "3. Install packages": [[91, "install-packages"]], "3. Load tokenizer": [[53, "load-tokenizer"], [54, "load-tokenizer"], [55, "load-tokenizer"], [56, "load-tokenizer"]], "3. Run Python code": [[93, "run-python-code"]], "3. Weight loading": [[58, "weight-loading"]], "3.1 The problem with modeling long sequences": [[27, "the-problem-with-modeling-long-sequences"]], "3.1 Training, encoding, and decoding": [[24, "training-encoding-and-decoding"]], "3.2 Capturing data dependencies with attention mechanisms": [[27, "capturing-data-dependencies-with-attention-mechanisms"]], "3.2 Saving and loading the tokenizer": [[24, "saving-and-loading-the-tokenizer"]], "3.3 Attending to different parts of the input with self-attention": [[27, "attending-to-different-parts-of-the-input-with-self-attention"]], "3.3 Loading the original GPT-2 BPE tokenizer from OpenAI": [[24, "loading-the-original-gpt-2-bpe-tokenizer-from-openai"]], "3.3.1 A simple self-attention mechanism without trainable weights": [[27, "a-simple-self-attention-mechanism-without-trainable-weights"]], "3.3.2 Computing attention weights for all input tokens": [[27, "computing-attention-weights-for-all-input-tokens"]], "3.4 Implementing self-attention with trainable weights": [[27, "implementing-self-attention-with-trainable-weights"]], "3.4.1 Computing the attention weights step by step": [[27, "computing-the-attention-weights-step-by-step"]], "3.4.2 Implementing a compact SelfAttention class": [[27, "implementing-a-compact-selfattention-class"]], "3.5 Hiding future words with causal attention": [[27, "hiding-future-words-with-causal-attention"]], "3.5.1 Applying a causal attention mask": [[27, "applying-a-causal-attention-mask"]], "3.5.2 Masking additional attention weights with dropout": [[27, "masking-additional-attention-weights-with-dropout"]], "3.5.3 Implementing a compact causal self-attention class": [[27, "implementing-a-compact-causal-self-attention-class"]], "3.6 Extending single-head attention to multi-head attention": [[27, "extending-single-head-attention-to-multi-head-attention"]], "3.6.1 Stacking multiple single-head attention layers": [[27, "stacking-multiple-single-head-attention-layers"]], "3.6.2 Implementing multi-head attention with weight splits": [[27, "implementing-multi-head-attention-with-weight-splits"]], "4) 355 M RoBERTa": [[68, "m-roberta"]], "4) Coding the DPO Loss Function": [[84, "coding-the-dpo-loss-function"]], "4) Initialize tokenizer": [[52, "initialize-tokenizer"]], "4) Multi-head attention with Einsum": [[31, "multi-head-attention-with-einsum"]], "4. Conclusion": [[24, "conclusion"]], "4. Install PyTorch": [[91, "install-pytorch"]], "4. Load pretrained weights": [[53, "load-pretrained-weights"], [54, "load-pretrained-weights"], [55, "load-pretrained-weights"], [56, "load-pretrained-weights"]], "4. Loading weights sequentially": [[58, "loading-weights-sequentially"]], "4. Pinned memory in the data loader": [[61, "pinned-memory-in-the-data-loader"]], "4. Run Python code": [[92, "run-python-code"]], "4.1 Coding an LLM architecture": [[36, "coding-an-llm-architecture"]], "4.2 Normalizing activations with layer normalization": [[36, "normalizing-activations-with-layer-normalization"]], "4.3 Implementing a feed forward network with GELU activations": [[36, "implementing-a-feed-forward-network-with-gelu-activations"]], "4.4 Adding shortcut connections": [[36, "adding-shortcut-connections"]], "4.5 Connecting attention and linear layers in a transformer block": [[36, "connecting-attention-and-linear-layers-in-a-transformer-block"]], "4.6 Coding the GPT model": [[36, "coding-the-gpt-model"]], "4.7 Generating text": [[36, "generating-text"]], "5) 149 M ModernBERT Base": [[68, "m-modernbert-base"]], "5) Generating text": [[52, "generating-text"]], "5) Multi-head attention with PyTorch\u2019s scaled dot product attention and FlashAttention": [[31, "multi-head-attention-with-pytorch-s-scaled-dot-product-attention-and-flashattention"]], "5) Training the model": [[84, "training-the-model"]], "5. Generate text": [[55, "generate-text"], [56, "generate-text"]], "5. Installing Python packages and libraries used in this book": [[91, "installing-python-packages-and-libraries-used-in-this-book"]], "5. Loading the model with low CPU memory": [[58, "loading-the-model-with-low-cpu-memory"]], "5. Using bfloat16 precision": [[61, "using-bfloat16-precision"]], "5. Using the instruction-finetuned model": [[53, "using-the-instruction-finetuned-model"], [54, "using-the-instruction-finetuned-model"]], "5.1 Evaluating generative text models": [[42, "evaluating-generative-text-models"]], "5.1.1 Using GPT to generate text": [[42, "using-gpt-to-generate-text"]], "5.1.2 Calculating the text generation loss: cross-entropy and perplexity": [[42, "calculating-the-text-generation-loss-cross-entropy-and-perplexity"]], "5.1.3 Calculating the training and validation set losses": [[42, "calculating-the-training-and-validation-set-losses"]], "5.2 Training an LLM": [[42, "training-an-llm"]], "5.3 Decoding strategies to control randomness": [[42, "decoding-strategies-to-control-randomness"]], "5.3.1 Temperature scaling": [[42, "temperature-scaling"]], "5.3.2 Top-k sampling": [[42, "top-k-sampling"]], "5.3.3 Modifying the text generation function": [[42, "modifying-the-text-generation-function"]], "5.4 Loading and saving model weights in PyTorch": [[42, "loading-and-saving-model-weights-in-pytorch"]], "5.5 Loading pretrained weights from OpenAI": [[42, "loading-pretrained-weights-from-openai"]], "6) 395 M ModernBERT Large": [[68, "m-modernbert-large"]], "6) Analyzing the results": [[84, "analyzing-the-results"]], "6) PyTorch\u2019s scaled dot product attention without FlashAttention": [[31, "pytorch-s-scaled-dot-product-attention-without-flashattention"]], "6. Replacing from-scratch code by PyTorch classes": [[61, "replacing-from-scratch-code-by-pytorch-classes"]], "6. Using mmap=True (recommmended)": [[58, "using-mmap-true-recommmended"]], "6.1 Different categories of finetuning": [[64, "different-categories-of-finetuning"]], "6.2 Preparing the dataset": [[64, "preparing-the-dataset"]], "6.3 Creating data loaders": [[64, "creating-data-loaders"]], "6.4 Initializing a model with pretrained weights": [[64, "initializing-a-model-with-pretrained-weights"]], "6.5 Adding a classification head": [[64, "adding-a-classification-head"]], "6.6 Calculating the classification loss and accuracy": [[64, "calculating-the-classification-loss-and-accuracy"]], "6.7 Finetuning the model on supervised data": [[64, "finetuning-the-model-on-supervised-data"]], "6.8 Using the LLM as a spam classifier": [[64, "using-the-llm-as-a-spam-classifier"]], "7) Logistic Regression Baseline": [[68, "logistic-regression-baseline"]], "7) Using PyTorch\u2019s torch.nn.MultiheadAttention": [[31, "using-pytorch-s-torch-nn-multiheadattention"]], "7. Other methods": [[58, "other-methods"]], "7. Using FlashAttention": [[61, "using-flashattention"]], "7.1 Introduction to instruction finetuning": [[73, "introduction-to-instruction-finetuning"]], "7.2 Preparing a dataset for supervised instruction finetuning": [[73, "preparing-a-dataset-for-supervised-instruction-finetuning"]], "7.3 Organizing data into training batches": [[73, "organizing-data-into-training-batches"]], "7.4 Creating data loaders for an instruction dataset": [[73, "creating-data-loaders-for-an-instruction-dataset"]], "7.5 Loading a pretrained LLM": [[73, "loading-a-pretrained-llm"]], "7.6 Finetuning the LLM on instruction data": [[73, "finetuning-the-llm-on-instruction-data"]], "7.7 Extracting and saving responses": [[73, "extracting-and-saving-responses"]], "7.8 Evaluating the finetuned LLM": [[73, "evaluating-the-finetuned-llm"]], "7.9 Conclusions": [[73, "conclusions"]], "7.9.1 What\u2019s next": [[73, "what-s-next"]], "7.9.2 Staying up to date in a fast-moving field": [[73, "staying-up-to-date-in-a-fast-moving-field"]], "7.9.3 Final words": [[73, "final-words"]], "8) Using PyTorch\u2019s torch.nn.MultiheadAttention with scaled_dot_product_attention": [[31, "using-pytorch-s-torch-nn-multiheadattention-with-scaled-dot-product-attention"]], "8. Using pytorch.compile": [[61, "using-pytorch-compile"]], "9) Using PyTorch\u2019s FlexAttention": [[31, "using-pytorch-s-flexattention"]], "9. Vocabulary padding": [[61, "vocabulary-padding"]], "A quick performance benchmark": [[18, "a-quick-performance-benchmark"]], "A.1 What is PyTorch": [[3, "a-1-what-is-pytorch"]], "A.2 Understanding tensors": [[3, "a-2-understanding-tensors"]], "A.2.1 Scalars, vectors, matrices, and tensors": [[3, "a-2-1-scalars-vectors-matrices-and-tensors"]], "A.2.2 Tensor data types": [[3, "a-2-2-tensor-data-types"]], "A.2.3 Common PyTorch tensor operations": [[3, "a-2-3-common-pytorch-tensor-operations"]], "A.3 Seeing models as computation graphs": [[3, "a-3-seeing-models-as-computation-graphs"]], "A.4 Automatic differentiation made easy": [[3, "a-4-automatic-differentiation-made-easy"]], "A.5 Implementing multilayer neural networks": [[3, "a-5-implementing-multilayer-neural-networks"]], "A.6 Setting up efficient data loaders": [[3, "a-6-setting-up-efficient-data-loaders"]], "A.7 A typical training loop": [[3, "a-7-a-typical-training-loop"]], "A.8 Saving and loading models": [[3, "a-8-saving-and-loading-models"]], "A.9 Optimizing training performance with GPUs": [[3, "a-9-optimizing-training-performance-with-gpus"], [4, "a-9-optimizing-training-performance-with-gpus"]], "A.9.1 PyTorch computations on GPU devices": [[3, "a-9-1-pytorch-computations-on-gpu-devices"], [4, "a-9-1-pytorch-computations-on-gpu-devices"]], "A.9.2 Single-GPU training": [[3, "a-9-2-single-gpu-training"], [4, "a-9-2-single-gpu-training"]], "A.9.3 Training with multiple GPUs": [[3, "a-9-3-training-with-multiple-gpus"], [4, "a-9-3-training-with-multiple-gpus"]], "AWS CloudFormation Template: Jupyter Notebook with LLMs-from-scratch Repo": [[98, null]], "Adding Bells and Whistles to the Training Loop": [[49, null]], "Additional Classification Finetuning Experiments": [[67, null]], "Additional Experiments Classifying the Sentiment of 50k IMDB Movie Reviews": [[68, null]], "Alternative Approaches to Loading Pretrained Weights": [[44, null]], "Alternative Weight Loading from Hugging Face Model Hub Via safetensors": [[45, "alternative-weight-loading-from-hugging-face-model-hub-via-safetensors"]], "Alternative Weight Loading from Hugging Face Model Hub using Transformers": [[46, "alternative-weight-loading-from-hugging-face-model-hub-using-transformers"]], "Alternative Weight Loading from PyTorch state dicts": [[47, "alternative-weight-loading-from-pytorch-state-dicts"]], "Alternative safetensors file": [[47, "alternative-safetensors-file"]], "An example with buffers": [[33, "an-example-with-buffers"]], "An example without buffers": [[33, "an-example-without-buffers"]], "Appendix A: Introduction to PyTorch": [[2, null], [7, null]], "Appendix A: Introduction to PyTorch (Part 1)": [[3, null]], "Appendix A: Introduction to PyTorch (Part 2)": [[4, null]], "Appendix D: Adding Bells and Whistles to the Training Loop": [[8, null], [9, null]], "Appendix E: Parameter-efficient Finetuning with LoRA": [[10, null], [11, null]], "Baseline": [[61, "baseline"]], "Benchmark with automatic batch size finding and Model FLOP Utilization (MFU)": [[39, "benchmark-with-automatic-batch-size-finding-and-model-flop-utilization-mfu"]], "Bonus Code for Chapter 5": [[45, null], [46, null], [47, null]], "Bonus Material": [[1, "bonus-material"]], "Bonus Materials": [[7, "bonus-materials"], [12, "bonus-materials"], [25, "bonus-materials"], [34, "bonus-materials"], [40, "bonus-materials"], [62, "bonus-materials"], [71, "bonus-materials"], [89, "bonus-materials"]], "Buffers and state_dict": [[33, "buffers-and-state-dict"]], "Build a Large Language Model (From Scratch)": [[1, null]], "Building a User Interface to Interact With the GPT-based Spam Classifier": [[70, null]], "Building a User Interface to Interact With the Instruction Finetuned GPT Model": [[88, null]], "Building a User Interface to Interact With the Pretrained LLM": [[51, null]], "Byte Pair Encoding (BPE) Tokenizer From Scratch": [[23, null], [24, null]], "Chapter 1: Understanding Large Language Models": [[12, null]], "Chapter 2 Exercise solutions": [[16, null]], "Chapter 2: Working with Text Data": [[13, null], [14, null], [17, null], [19, null], [21, null], [25, null]], "Chapter 3 Exercise solutions": [[28, null]], "Chapter 3: Coding Attention Mechanisms": [[26, null], [27, null], [34, null]], "Chapter 4 Exercise solutions": [[37, null]], "Chapter 4: Implementing a GPT Model from Scratch To Generate Text": [[35, null], [38, null]], "Chapter 4: Implementing a GPT Model from Scratch to Generate Text": [[40, null]], "Chapter 4: Implementing a GPT model from Scratch To Generate Text": [[36, null]], "Chapter 5 Exercise solutions": [[43, null]], "Chapter 5: Pretraining on Unlabeled Data": [[41, null], [42, null], [62, null]], "Chapter 6 Exercise solutions": [[65, null]], "Chapter 6: Finetuning for Classification": [[63, null], [71, null]], "Chapter 6: Finetuning for Text Classification": [[64, null]], "Chapter 7 Exercise solutions": [[74, null]], "Chapter 7: Finetuning To Follow Instructions": [[73, null]], "Chapter 7: Finetuning to Follow Instructions": [[72, null], [76, null], [78, null], [82, null], [89, null]], "Choose model": [[47, "choose-model"]], "Citation": [[1, "citation"]], "Cloud Resources": [[99, "cloud-resources"]], "Comparing Efficient Multi-Head Attention Implementations": [[31, null]], "Comparing Various Byte Pair Encoding (BPE) Implementations": [[18, null]], "Converting GPT to Llama": [[52, null]], "Converting Llama 2 to Llama 3.2 From Scratch": [[54, null]], "Converting a From-Scratch GPT Architecture to Llama 2": [[53, null]], "Correlation Coefficients": [[81, "correlation-coefficients"]], "Create JSON Entries": [[77, "create-json-entries"]], "Create \u201cPassive Voice\u201d Entries for an Instruction Dataset": [[77, null]], "Creating Improved Instruction Data": [[87, "creating-improved-instruction-data"]], "Creating Passive Voice Entries": [[76, "creating-passive-voice-entries"]], "D.1 Learning rate warmup": [[8, "d-1-learning-rate-warmup"]], "D.2 Cosine decay": [[8, "d-2-cosine-decay"]], "D.3 Gradient clipping": [[8, "d-3-gradient-clipping"]], "D.4 The modified training function": [[8, "d-4-the-modified-training-function"]], "Data Loader from Chapter 2": [[29, "data-loader-from-chapter-2"]], "Data sampling with a sliding window with number data": [[22, null]], "Design Decisions and Improvements": [[48, "design-decisions-and-improvements"]], "Direct Preference Optimization (DPO) for LLM Alignment (From Scratch)": [[84, null]], "Docker Environment Setup Guide": [[97, null]], "Download file": [[47, "download-file"]], "Download instructions for Linux and macOS users": [[48, "download-instructions-for-linux-and-macos-users"]], "Downloading and installing Docker": [[97, "downloading-and-installing-docker"]], "E.1 Introduction to LoRA": [[10, "e-1-introduction-to-lora"]], "E.2 Preparing the dataset": [[10, "e-2-preparing-the-dataset"]], "E.3 Initializing the model": [[10, "e-3-initializing-the-model"]], "E.4 Parameter-efficient finetuning with LoRA": [[10, "e-4-parameter-efficient-finetuning-with-lora"]], "Editable Install from GitHub": [[90, "editable-install-from-github"]], "Evaluating Instruction Responses Locally Using Ollama": [[78, "evaluating-instruction-responses-locally-using-ollama"]], "Evaluating Instruction Responses Locally Using a Llama 3 Model Via Ollama": [[79, null]], "Evaluating Instruction Responses Using the OpenAI API": [[78, "evaluating-instruction-responses-using-the-openai-api"], [80, null]], "Exercise 2.1": [[16, "exercise-2-1"]], "Exercise 2.2": [[16, "exercise-2-2"]], "Exercise 3.1": [[28, "exercise-3-1"]], "Exercise 3.2": [[28, "exercise-3-2"]], "Exercise 3.3": [[28, "exercise-3-3"]], "Exercise 4.1: Parameters in the feed forward versus attention module": [[37, "exercise-4-1-parameters-in-the-feed-forward-versus-attention-module"]], "Exercise 4.2: Initialize larger GPT models": [[37, "exercise-4-2-initialize-larger-gpt-models"]], "Exercise 4.3: Using separate dropout parameters": [[37, "exercise-4-3-using-separate-dropout-parameters"]], "Exercise 5.1: Temperature-scaled softmax scores and sampling probabilities": [[43, "exercise-5-1-temperature-scaled-softmax-scores-and-sampling-probabilities"]], "Exercise 5.2: Different temperature and top-k settings": [[43, "exercise-5-2-different-temperature-and-top-k-settings"]], "Exercise 5.3: Deterministic behavior in the decoding functions": [[43, "exercise-5-3-deterministic-behavior-in-the-decoding-functions"]], "Exercise 5.4: Continued pretraining": [[43, "exercise-5-4-continued-pretraining"]], "Exercise 5.5: Training and validation set losses of the pretrained model": [[43, "exercise-5-5-training-and-validation-set-losses-of-the-pretrained-model"]], "Exercise 5.6: Trying larger models": [[43, "exercise-5-6-trying-larger-models"]], "Exercise 6.1: Increasing the context length": [[65, "exercise-6-1-increasing-the-context-length"]], "Exercise 6.2: Finetuning the whole model": [[65, "exercise-6-2-finetuning-the-whole-model"]], "Exercise 6.3: Finetuning the first versus last token": [[65, "exercise-6-3-finetuning-the-first-versus-last-token"]], "Exercise 7.1: Changing prompt styles": [[74, "exercise-7-1-changing-prompt-styles"]], "Exercise 7.2: Instruction and input masking": [[74, "exercise-7-2-instruction-and-input-masking"]], "Exercise 7.3: Finetuning on the original Alpaca dataset": [[74, "exercise-7-3-finetuning-on-the-original-alpaca-dataset"]], "Exercise 7.4: Parameter-efficient finetuning with LoRA": [[74, "exercise-7-4-parameter-efficient-finetuning-with-lora"]], "Exercise A.1": [[5, null]], "Exercise A.2": [[5, "exercise-a-2"]], "Exercise A.3": [[5, "exercise-a-3"]], "Exercise A.4": [[5, "exercise-a-4"]], "Extending the Tiktoken BPE Tokenizer with New Tokens": [[59, null], [60, null]], "Extract Instructions": [[86, "extract-instructions"]], "FLOPS Analysis": [[39, null]], "Finding Near Duplicates": [[76, "finding-near-duplicates"]], "Forward and backward pass": [[30, "forward-and-backward-pass"]], "Forward and backward pass after compilation": [[30, "forward-and-backward-pass-after-compilation"]], "Forward pass only": [[30, "forward-pass-only"]], "From PyPI": [[90, "from-pypi"]], "GPT-4 vs Llama 3 8B": [[81, "gpt-4-vs-llama-3-8b"]], "Generalize to all input sequence tokens:": [[27, "generalize-to-all-input-sequence-tokens"]], "Generate Dataset": [[86, "generate-dataset"]], "Generate Responses": [[86, "generate-responses"]], "Generate text": [[47, "generate-text"]], "Generating A Preference Dataset With Llama 3.1 70B And Ollama": [[83, null]], "Generating An Instruction Dataset via Llama 3 and Ollama": [[86, null]], "Generating Datasets for Instruction Finetuning": [[85, null]], "Hardware Requirements": [[1, "hardware-requirements"]], "How to Use This Code": [[48, "how-to-use-this-code"]], "How to use:": [[98, "how-to-use"]], "Hugging Face OpenAI GPT-2 tokenizer": [[18, "hugging-face-openai-gpt-2-tokenizer"]], "Improve Instructions": [[87, "improve-instructions"]], "Improve Responses": [[87, "improve-responses"]], "Improving Instruction-Data Via Reflection-Tuning Using GPT-4": [[87, null]], "Improving the Dataset": [[87, "improving-the-dataset"]], "Input": [[83, "input"]], "Installation": [[90, "installation"]], "Installing Ollama and Downloading Llama 3": [[79, "installing-ollama-and-downloading-llama-3"], [86, "installing-ollama-and-downloading-llama-3"]], "Installing Ollama and Downloading Llama 3.1": [[83, "installing-ollama-and-downloading-llama-3-1"]], "Installing PyTorch": [[94, "installing-pytorch"]], "Installing Python Packages and Libraries Used In This Book": [[94, null]], "Interpretation": [[67, "interpretation"]], "Kendall-Tau": [[81, "kendall-tau"]], "Key Points:": [[98, "key-points"]], "Llama 3.1 8B": [[54, "llama-3-1-8b"]], "Llama 3.2 1B": [[54, "llama-3-2-1b"]], "Llama 3.2 From Scratch (A Standalone Notebook)": [[55, null], [56, null]], "Load And Use Finetuned Model": [[66, null], [75, null]], "Load JSON Entries": [[79, "load-json-entries"], [80, "load-json-entries"], [83, "load-json-entries"], [87, "load-json-entries"]], "Load weights": [[47, "load-weights"]], "Local Setup": [[99, "local-setup"]], "Main Chapter Code": [[2, "main-chapter-code"], [7, "main-chapter-code"], [12, "main-chapter-code"], [13, "main-chapter-code"], [25, "main-chapter-code"], [26, "main-chapter-code"], [34, "main-chapter-code"], [35, "main-chapter-code"], [40, "main-chapter-code"], [41, "main-chapter-code"], [62, "main-chapter-code"], [63, "main-chapter-code"], [71, "main-chapter-code"], [72, "main-chapter-code"], [89, "main-chapter-code"]], "Memory-efficient Model Weight Loading": [[57, null], [58, null]], "More Efficient Multi-Head Attention Implementations": [[30, null]], "Multi-GPU speed comparisons": [[61, "multi-gpu-speed-comparisons"]], "Multi-head Attention Plus Data Loading": [[29, null]], "Multi-head Attention from Chapter 3": [[29, "multi-head-attention-from-chapter-3"]], "My own GPT-2 tokenizer (for educational purposes)": [[18, "my-own-gpt-2-tokenizer-for-educational-purposes"]], "Native pixi Python and package management": [[92, null]], "Native uv Python and package management": [[93, null]], "Optimizing Hyperparameters for Pretraining": [[50, null]], "Option 1: Using uv": [[91, "option-1-using-uv"]], "Option 2: Using Conda": [[91, "option-2-using-conda"]], "Optional Code": [[2, "optional-code"], [13, "optional-code"], [26, "optional-code"], [35, "optional-code"], [41, "optional-code"], [63, "optional-code"], [72, "optional-code"]], "Optional Docker Environment": [[96, null]], "Optional Setup Instructions": [[99, null]], "Optional: Manage virtual environments manually": [[93, "optional-manage-virtual-environments-manually"]], "Optional: styling your terminal": [[91, "optional-styling-your-terminal"]], "Original OpenAI GPT-2 tokenizer": [[18, "original-openai-gpt-2-tokenizer"]], "Output": [[83, "output"]], "Overview": [[68, "overview"]], "Pearson": [[81, "pearson"]], "Pretraining GPT on the Project Gutenberg Dataset": [[48, null]], "PyTorch Performance Tips for Faster LLM Training": [[61, null]], "Python Setup Tips": [[91, null]], "Python and Environment Setup Recommendations": [[6, null]], "Questions, Feedback, and Contributing to This Repository": [[1, "questions-feedback-and-contributing-to-this-repository"]], "Questions?": [[99, "questions"]], "Quick speed comparison (M3 Macbook Air CPU)": [[31, "quick-speed-comparison-m3-macbook-air-cpu"]], "Quick speed comparison (Nvidia A100 GPU)": [[31, "quick-speed-comparison-nvidia-a100-gpu"]], "Quickstart": [[99, "quickstart"]], "Reflect instructions": [[87, "reflect-instructions"]], "Reflect responses": [[87, "reflect-responses"]], "Scikit-learn Logistic Regression Model": [[69, null]], "Scikit-learn baseline": [[69, "scikit-learn-baseline"]], "Score Correlation Analysis": [[81, null]], "Setting up Python": [[99, "setting-up-python"]], "Simple benchmark with automatic batch size finding": [[39, "simple-benchmark-with-automatic-batch-size-finding"]], "Simple benchmark with fixed batch size": [[39, "simple-benchmark-with-fixed-batch-size"]], "Single GPU speed comparisons": [[61, "single-gpu-speed-comparisons"]], "Spearman": [[81, "spearman"]], "Special instructions for Windows users": [[48, "special-instructions-for-windows-users"]], "Speed comparison (Nvidia A100 GPU) with warmup (forward and backward pass)": [[31, "speed-comparison-nvidia-a100-gpu-with-warmup-forward-and-backward-pass"]], "Speed comparison (Nvidia A100 GPU) with warmup (forward pass only)": [[31, "speed-comparison-nvidia-a100-gpu-with-warmup-forward-pass-only"]], "Speed comparison (Nvidia A100 GPU) with warmup and compilation (forward and backward pass)": [[31, "speed-comparison-nvidia-a100-gpu-with-warmup-and-compilation-forward-and-backward-pass"]], "Step 1: Install Dependencies": [[68, "step-1-install-dependencies"]], "Step 1: Install dependencies": [[51, "step-1-install-dependencies"], [70, "step-1-install-dependencies"], [88, "step-1-install-dependencies"]], "Step 2: Download Dataset": [[68, "step-2-download-dataset"]], "Step 2: Run app code": [[51, "step-2-run-app-code"], [70, "step-2-run-app-code"], [88, "step-2-run-app-code"]], "Step 3: Run Models": [[68, "step-3-run-models"]], "Summary": [[30, "summary"]], "Summary and takeaways": [[14, "summary-and-takeaways"], [27, "summary-and-takeaways"], [36, "summary-and-takeaways"], [42, "summary-and-takeaways"], [64, "summary-and-takeaways"], [73, "summary-and-takeaways"]], "Table of Contents": [[1, "table-of-contents"]], "Test OpenAI API": [[77, "test-openai-api"], [80, "test-openai-api"], [87, "test-openai-api"]], "The Main Data Loading Pipeline Summarized": [[15, null]], "Tiktoken OpenAI GPT-2 tokenizer": [[18, "tiktoken-openai-gpt-2-tokenizer"]], "Tip: Considering special tokens": [[74, "tip-considering-special-tokens"]], "Understanding PyTorch Buffers": [[32, null], [33, null]], "Understanding the Difference Between Embedding Layers and Linear Layers": [[20, null]], "Uninstalling Docker": [[97, "uninstalling-docker"]], "Uninstalling the Docker Image": [[97, "uninstalling-the-docker-image"]], "Usage": [[67, "usage"]], "Using BPE from tiktoken": [[18, "using-bpe-from-tiktoken"]], "Using Docker DevContainers": [[99, "using-docker-devcontainers"]], "Using Google Colab": [[99, "using-google-colab"]], "Using Lightning Studio": [[99, "using-lightning-studio"]], "Using Llama 3.2 via the llms-from-scratch package": [[52, "using-llama-3-2-via-the-llms-from-scratch-package"]], "Using Ollama\u2019s REST API": [[79, "using-ollama-s-rest-api"], [83, "using-ollama-s-rest-api"], [86, "using-ollama-s-rest-api"]], "Using a Docker DevContainer in Visual Studio Code": [[97, "using-a-docker-devcontainer-in-visual-studio-code"]], "Using my own from-scratch BPE tokenizer": [[18, "using-my-own-from-scratch-bpe-tokenizer"]], "Using nn.Embedding": [[20, "using-nn-embedding"]], "Using nn.Linear": [[20, "using-nn-linear"]], "Using the BPE via Hugging Face transformers": [[18, "using-the-bpe-via-hugging-face-transformers"]], "Using the Package": [[90, "using-the-package"]], "Using the original BPE implementation used in GPT-2": [[18, "using-the-original-bpe-implementation-used-in-gpt-2"]], "VSCode Extensions": [[99, "vscode-extensions"]], "Variant A: Simple implementation": [[29, "variant-a-simple-implementation"]], "Variant B: Alternative implementation": [[29, "variant-b-alternative-implementation"]], "Visual Studio Code Editor": [[99, "visual-studio-code-editor"]], "Visualizations": [[31, "visualizations"]], "What it does:": [[98, "what-it-does"]], "What\u2019s next?": [[53, "what-s-next"], [54, "what-s-next"], [55, "what-s-next"], [56, "what-s-next"], [73, "id1"]], "llms-from-scratch PyPI Package": [[90, null]]}, "docnames": [".github/ISSUE_TEMPLATE/ask-a-question", "README", "appendix-A/01_main-chapter-code/README", "appendix-A/01_main-chapter-code/code-part1", "appendix-A/01_main-chapter-code/code-part2", "appendix-A/01_main-chapter-code/exercise-solutions", "appendix-A/02_setup-recommendations/README", "appendix-A/README", "appendix-D/01_main-chapter-code/appendix-D", "appendix-D/README", "appendix-E/01_main-chapter-code/appendix-E", "appendix-E/README", "ch01/README", "ch02/01_main-chapter-code/README", "ch02/01_main-chapter-code/ch02", "ch02/01_main-chapter-code/dataloader", "ch02/01_main-chapter-code/exercise-solutions", "ch02/02_bonus_bytepair-encoder/README", "ch02/02_bonus_bytepair-encoder/compare-bpe-tiktoken", "ch02/03_bonus_embedding-vs-matmul/README", "ch02/03_bonus_embedding-vs-matmul/embeddings-and-linear-layers", "ch02/04_bonus_dataloader-intuition/README", "ch02/04_bonus_dataloader-intuition/dataloader-intuition", "ch02/05_bpe-from-scratch/README", "ch02/05_bpe-from-scratch/bpe-from-scratch", "ch02/README", "ch03/01_main-chapter-code/README", "ch03/01_main-chapter-code/ch03", "ch03/01_main-chapter-code/exercise-solutions", "ch03/01_main-chapter-code/multihead-attention", "ch03/02_bonus_efficient-multihead-attention/README", "ch03/02_bonus_efficient-multihead-attention/mha-implementations", "ch03/03_understanding-buffers/README", "ch03/03_understanding-buffers/understanding-buffers", "ch03/README", "ch04/01_main-chapter-code/README", "ch04/01_main-chapter-code/ch04", "ch04/01_main-chapter-code/exercise-solutions", "ch04/02_performance-analysis/README", "ch04/02_performance-analysis/flops-analysis", "ch04/README", "ch05/01_main-chapter-code/README", "ch05/01_main-chapter-code/ch05", "ch05/01_main-chapter-code/exercise-solutions", "ch05/02_alternative_weight_loading/README", "ch05/02_alternative_weight_loading/weight-loading-hf-safetensors", "ch05/02_alternative_weight_loading/weight-loading-hf-transformers", "ch05/02_alternative_weight_loading/weight-loading-pytorch", "ch05/03_bonus_pretraining_on_gutenberg/README", "ch05/04_learning_rate_schedulers/README", "ch05/05_bonus_hparam_tuning/README", "ch05/06_user_interface/README", "ch05/07_gpt_to_llama/README", "ch05/07_gpt_to_llama/converting-gpt-to-llama2", "ch05/07_gpt_to_llama/converting-llama2-to-llama3", "ch05/07_gpt_to_llama/standalone-llama32", "ch05/07_gpt_to_llama/standalone-llama32-mem-opt", "ch05/08_memory_efficient_weight_loading/README", "ch05/08_memory_efficient_weight_loading/memory-efficient-state-dict", "ch05/09_extending-tokenizers/README", "ch05/09_extending-tokenizers/extend-tiktoken", "ch05/10_llm-training-speed/README", "ch05/README", "ch06/01_main-chapter-code/README", "ch06/01_main-chapter-code/ch06", "ch06/01_main-chapter-code/exercise-solutions", "ch06/01_main-chapter-code/load-finetuned-model", "ch06/02_bonus_additional-experiments/README", "ch06/03_bonus_imdb-classification/README", "ch06/03_bonus_imdb-classification/sklearn-baseline", "ch06/04_user_interface/README", "ch06/README", "ch07/01_main-chapter-code/README", "ch07/01_main-chapter-code/ch07", "ch07/01_main-chapter-code/exercise-solutions", "ch07/01_main-chapter-code/load-finetuned-model", "ch07/02_dataset-utilities/README", "ch07/02_dataset-utilities/create-passive-voice-entries", "ch07/03_model-evaluation/README", "ch07/03_model-evaluation/llm-instruction-eval-ollama", "ch07/03_model-evaluation/llm-instruction-eval-openai", "ch07/03_model-evaluation/scores/correlation-analysis", "ch07/04_preference-tuning-with-dpo/README", "ch07/04_preference-tuning-with-dpo/create-preference-data-ollama", "ch07/04_preference-tuning-with-dpo/dpo-from-scratch", "ch07/05_dataset-generation/README", "ch07/05_dataset-generation/llama3-ollama", "ch07/05_dataset-generation/reflection-gpt4", "ch07/06_user_interface/README", "ch07/README", "pkg/llms_from_scratch/README", "setup/01_optional-python-setup-preferences/README", "setup/01_optional-python-setup-preferences/native-pixi", "setup/01_optional-python-setup-preferences/native-uv", "setup/02_installing-python-libraries/README", "setup/02_installing-python-libraries/python_environment_check", "setup/03_optional-docker-environment/.devcontainer/README", "setup/03_optional-docker-environment/README", "setup/04_optional-aws-sagemaker-notebook/README", "setup/README"], "envversion": {"sphinx": 62, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1}, "filenames": [".github/ISSUE_TEMPLATE/ask-a-question.md", "README.md", "appendix-A/01_main-chapter-code/README.md", "appendix-A/01_main-chapter-code/code-part1.ipynb", "appendix-A/01_main-chapter-code/code-part2.ipynb", "appendix-A/01_main-chapter-code/exercise-solutions.ipynb", "appendix-A/02_setup-recommendations/README.md", "appendix-A/README.md", "appendix-D/01_main-chapter-code/appendix-D.ipynb", "appendix-D/README.md", "appendix-E/01_main-chapter-code/appendix-E.ipynb", "appendix-E/README.md", "ch01/README.md", "ch02/01_main-chapter-code/README.md", "ch02/01_main-chapter-code/ch02.ipynb", "ch02/01_main-chapter-code/dataloader.ipynb", "ch02/01_main-chapter-code/exercise-solutions.ipynb", "ch02/02_bonus_bytepair-encoder/README.md", "ch02/02_bonus_bytepair-encoder/compare-bpe-tiktoken.ipynb", "ch02/03_bonus_embedding-vs-matmul/README.md", "ch02/03_bonus_embedding-vs-matmul/embeddings-and-linear-layers.ipynb", "ch02/04_bonus_dataloader-intuition/README.md", "ch02/04_bonus_dataloader-intuition/dataloader-intuition.ipynb", "ch02/05_bpe-from-scratch/README.md", "ch02/05_bpe-from-scratch/bpe-from-scratch.ipynb", "ch02/README.md", "ch03/01_main-chapter-code/README.md", "ch03/01_main-chapter-code/ch03.ipynb", "ch03/01_main-chapter-code/exercise-solutions.ipynb", "ch03/01_main-chapter-code/multihead-attention.ipynb", "ch03/02_bonus_efficient-multihead-attention/README.md", "ch03/02_bonus_efficient-multihead-attention/mha-implementations.ipynb", "ch03/03_understanding-buffers/README.md", "ch03/03_understanding-buffers/understanding-buffers.ipynb", "ch03/README.md", "ch04/01_main-chapter-code/README.md", "ch04/01_main-chapter-code/ch04.ipynb", "ch04/01_main-chapter-code/exercise-solutions.ipynb", "ch04/02_performance-analysis/README.md", "ch04/02_performance-analysis/flops-analysis.ipynb", "ch04/README.md", "ch05/01_main-chapter-code/README.md", "ch05/01_main-chapter-code/ch05.ipynb", "ch05/01_main-chapter-code/exercise-solutions.ipynb", "ch05/02_alternative_weight_loading/README.md", "ch05/02_alternative_weight_loading/weight-loading-hf-safetensors.ipynb", "ch05/02_alternative_weight_loading/weight-loading-hf-transformers.ipynb", "ch05/02_alternative_weight_loading/weight-loading-pytorch.ipynb", "ch05/03_bonus_pretraining_on_gutenberg/README.md", "ch05/04_learning_rate_schedulers/README.md", "ch05/05_bonus_hparam_tuning/README.md", "ch05/06_user_interface/README.md", "ch05/07_gpt_to_llama/README.md", "ch05/07_gpt_to_llama/converting-gpt-to-llama2.ipynb", "ch05/07_gpt_to_llama/converting-llama2-to-llama3.ipynb", "ch05/07_gpt_to_llama/standalone-llama32.ipynb", "ch05/07_gpt_to_llama/standalone-llama32-mem-opt.ipynb", "ch05/08_memory_efficient_weight_loading/README.md", "ch05/08_memory_efficient_weight_loading/memory-efficient-state-dict.ipynb", "ch05/09_extending-tokenizers/README.md", "ch05/09_extending-tokenizers/extend-tiktoken.ipynb", "ch05/10_llm-training-speed/README.md", "ch05/README.md", "ch06/01_main-chapter-code/README.md", "ch06/01_main-chapter-code/ch06.ipynb", "ch06/01_main-chapter-code/exercise-solutions.ipynb", "ch06/01_main-chapter-code/load-finetuned-model.ipynb", "ch06/02_bonus_additional-experiments/README.md", "ch06/03_bonus_imdb-classification/README.md", "ch06/03_bonus_imdb-classification/sklearn-baseline.ipynb", "ch06/04_user_interface/README.md", "ch06/README.md", "ch07/01_main-chapter-code/README.md", "ch07/01_main-chapter-code/ch07.ipynb", "ch07/01_main-chapter-code/exercise-solutions.ipynb", "ch07/01_main-chapter-code/load-finetuned-model.ipynb", "ch07/02_dataset-utilities/README.md", "ch07/02_dataset-utilities/create-passive-voice-entries.ipynb", "ch07/03_model-evaluation/README.md", "ch07/03_model-evaluation/llm-instruction-eval-ollama.ipynb", "ch07/03_model-evaluation/llm-instruction-eval-openai.ipynb", "ch07/03_model-evaluation/scores/correlation-analysis.ipynb", "ch07/04_preference-tuning-with-dpo/README.md", "ch07/04_preference-tuning-with-dpo/create-preference-data-ollama.ipynb", "ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb", "ch07/05_dataset-generation/README.md", "ch07/05_dataset-generation/llama3-ollama.ipynb", "ch07/05_dataset-generation/reflection-gpt4.ipynb", "ch07/06_user_interface/README.md", "ch07/README.md", "pkg/llms_from_scratch/README.md", "setup/01_optional-python-setup-preferences/README.md", "setup/01_optional-python-setup-preferences/native-pixi.md", "setup/01_optional-python-setup-preferences/native-uv.md", "setup/02_installing-python-libraries/README.md", "setup/02_installing-python-libraries/python_environment_check.ipynb", "setup/03_optional-docker-environment/.devcontainer/README.md", "setup/03_optional-docker-environment/README.md", "setup/04_optional-aws-sagemaker-notebook/README.md", "setup/README.md"], "indexentries": {}, "objects": {}, "objnames": {}, "objtypes": {}, "terms": {"": [0, 1, 8, 10, 14, 15, 16, 18, 20, 22, 24, 27, 29, 33, 36, 37, 38, 39, 40, 42, 43, 45, 46, 47, 48, 52, 57, 58, 60, 61, 62, 64, 66, 68, 72, 74, 75, 76, 77, 78, 80, 84, 87, 90, 91, 92, 93, 94, 97, 98, 99], "0": [2, 3, 4, 8, 10, 14, 15, 16, 18, 20, 22, 24, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 48, 52, 53, 54, 55, 56, 58, 60, 61, 64, 65, 66, 67, 68, 69, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 86, 87, 95, 98], "00": [3, 4, 10, 18, 28, 42, 43, 48, 53, 60, 64, 65, 67, 68, 72, 73, 74, 76, 77, 79, 80, 83, 86, 87], "000": [3, 4, 10, 18, 24, 37, 42, 54, 87], "0000": [27, 36, 42, 54, 55, 56], "00000": [10, 42, 43, 60, 64, 72, 73, 74], "000000": [8, 10, 42, 43, 61, 64, 68, 72, 73, 74, 84], "000005": [8, 42, 43, 72, 73, 84], "00001": [10, 42, 43, 60, 64, 72, 73, 74], "000010": [8, 42, 72, 73, 84], "000015": [8, 42, 61, 73, 84], "00002": [55, 56], "000020": [8, 42, 73, 84], "000025": [8, 42, 73, 84], "000030": [8, 42, 61, 73, 84], "000035": [8, 42, 73, 84], "00004": 54, "000040": [8, 42, 73, 84], "000045": [8, 42, 61, 73, 84], "00005": 73, "000050": [8, 10, 42, 64, 68, 73, 84], "000055": [8, 42, 73, 84], "000060": [8, 42, 73, 84], "000065": [8, 42, 73, 84], "000070": [8, 42, 73, 84], "000075": [8, 42, 73, 84], "000080": [8, 42, 73, 84], "000085": [8, 42, 73, 84], "000090": [8, 73, 84], "000095": [8, 73, 84], "0001": 8, "000100": [8, 10, 64, 73, 84], "000105": [8, 73, 84], "000110": [8, 73, 84], "000115": [8, 73, 84], "000120": [8, 73], "00012011159560643137": 36, "000125": [8, 73], "000130": [8, 73], "000135": 73, "000140": 73, "000145": 73, "000150": [10, 64, 73], "000155": 73, "000160": 73, "000165": 73, "000170": 73, "000175": 73, "000180": 73, "000185": 73, "000190": 73, "000195": 73, "000200": [10, 64, 73], "00020173587836325169": 36, "000205": 73, "000210": 73, "000215": 73, "000220": 73, "000225": 73, "000230": [73, 74], "000250": [10, 64], "000300": [10, 64], "000350": [10, 64], "0004": [42, 43], "000400": [10, 64], "000450": [10, 64], "0005": 42, "000500": [10, 64], "000550": [10, 64], "000600": [10, 64], "0007152039906941354": 36, "000735": 61, "000750": 61, "000765": 61, "0009": 3, "001": [3, 4, 8, 10], "0013988736318424344": 36, "0018": 3, "002": [3, 4, 81], "00253983": 42, "003": [3, 4, 10], "0037": 3, "0043": 3, "004300": 68, "004350": 68, "0047": 3, "005049645435065031": 36, "0051": 3, "0057": 36, "0058": [27, 33], "007": 10, "00704835": 42, "0076": 36, "0077": 36, "0078": 27, "008": 10, "009": [36, 37, 84], "00_orig": 61, "01": [8, 10, 28, 42, 68, 69, 72, 73, 74, 79, 80, 84], "010": 10, "0108": 3, "0111": 36, "0122": 3, "01225674": 42, "0138": 60, "0139": 36, "014": 84, "01506208": 42, "01535": 81, "0154": 3, "0162": 36, "0166": 20, "01665728": 42, "0175": 3, "0177": 27, "0185": 8, "0186": 27, "0189": 36, "019": [81, 84], "0191": 33, "0193": 60, "0198": 27, "01_main": [7, 8, 9, 11, 14, 18, 24, 25, 34, 40, 42, 43, 51, 62, 70, 71, 73, 74, 83, 84, 87, 88, 89], "01_opt_single_gpu": 61, "01_option": [94, 97, 99], "02": [28, 43, 68, 73, 74, 79, 86], "02073162414": 64, "021": 10, "022095": 74, "0222": 3, "02245961": 42, "0231": 36, "0249": 3, "025": 81, "0254": 3, "02768905": 42, "0293": 3, "0297": 3, "02_alternative_weight_load": [42, 62], "02_bonus_addit": [65, 71], "02_bonus_bytepair": [24, 25], "02_bonus_effici": 34, "02_dataset": 89, "02_instal": [91, 92, 93, 97, 99], "02_opt_multi_gpu_ddp": 61, "02_perform": 40, "02_setup": 7, "03": [3, 4, 28, 31, 73, 74, 77, 80, 87], "030": 54, "0307": 3, "0314": 60, "0318": 36, "032": 8, "03300": 73, "03310751": 42, "0332": 36, "0333": 3, "0335": 36, "0337e": 42, "0344": 3, "035": 8, "036": 81, "0367": 3, "037": 81, "038": 84, "0388": 36, "039": 8, "0390": 27, "03926672": 42, "0398": 36, "0399": 3, "03_bonus_embed": [14, 25], "03_bonus_imdb": [69, 71], "03_bonus_pretraining_on_gutenberg": [8, 42, 48, 62], "03_model": 89, "03_option": [97, 99], "03_understand": 34, "03d": [3, 4], "04": [31, 48, 54, 68, 74, 77], "040": 54, "0400": 36, "0402": 27, "04034033": 42, "0411": 8, "042": 81, "0424": 3, "0428": 3, "0430": 43, "04318958": 42, "0435": 36, "044": 81, "04453601": 42, "0447": 36, "044715": [36, 53], "04531523": 42, "04611587": 42, "04624869": 42, "0476": 3, "0477": 36, "04793796": 42, "048": 42, "04861503": 42, "0491": 3, "0494": 60, "0495": 60, "0499369": 42, "04_bonus_dataload": 25, "04_learning_rate_schedul": 62, "04_prefer": [73, 84, 89], "04_user_interfac": 71, "04m": [10, 42, 43, 60, 73], "04mit": 18, "05": [8, 27, 28, 31, 33, 42, 46], "0502": 3, "0509": 3, "0512": 3, "05135201": 42, "0518": 3, "052": [10, 84], "0532": 36, "0542": 36, "05483596": 42, "0549": 60, "055": 81, "0558": 36, "056": 10, "057": 8, "0571": 36, "0577": 3, "05844": 87, "0597": 3, "05_bonus_hparam_tun": 62, "05_bpe": [18, 25], "05_dataset": 89, "05it": 48, "05mib": 60, "06": [42, 43, 72, 80, 87], "0605": 27, "0606": 3, "0610": 36, "0612": 60, "0613": 81, "0615": 42, "0618": 3, "0620": 27, "0625e": 28, "0632": [27, 33], "064": 37, "0641": 3, "0645": 60, "065": 81, "0656": 60, "0668": 3, "0671": 60, "0673": 3, "0674": 3, "0675": 60, "0676": 3, "0679": 27, "0685": 27, "069": 42, "0693": 27, "06952604": 42, "06_user_interfac": [62, 89], "06d": [8, 42, 64, 84], "06it": 83, "07": [56, 68, 74, 87], "070": [81, 84], "0702": [3, 27], "0703": [27, 60], "071": 8, "0711": 36, "0713": 27, "0726": 60, "073": 10, "0735": 3, "0739": 27, "074": 64, "0748": 27, "0749": 27, "0754": 27, "0760": 27, "0762": 33, "0763": 27, "077": 10, "0772": 27, "07847701": 42, "0786": 27, "0787": 3, "0790": 3, "0795": 27, "07_gpt_to_llama": [40, 42, 55, 56, 62], "08": [31, 36, 54, 67, 72, 73, 74], "080": 10, "0803": 3, "0814": 60, "0817": 3, "083": [64, 68], "084": 81, "0842": 3, "0843": [27, 33], "08464": 86, "0849": 60, "0852": 3, "086": 68, "08605453": 42, "0864": 36, "0865": [3, 27], "0867": 3, "0869": 36, "0870": 64, "0874": 27, "0876": 36, "08785918": 42, "0882": 27, "089": 81, "0898": 3, "08991534": 42, "08_memory_efficient_weight_load": 62, "09": [42, 80], "0900": 36, "0906": 27, "0908": 36, "0923": 3, "093": 8, "094": [10, 48], "0944": 3, "0951": 3, "0954": 3, "096": [10, 54], "0960": 3, "09625227": 42, "097": 48, "0971": 3, "09783269": 42, "0979": 60, "0981": [27, 33, 60], "0983": 20, "0988": 27, "0990": 3, "09_extend": 62, "0d": 3, "0e": 39, "0kib": 10, "0x": 43, "1": [1, 2, 15, 18, 20, 22, 29, 33, 39, 45, 46, 47, 66, 67, 69, 72, 75, 76, 77, 78, 79, 80, 81, 82, 86, 87, 94, 95, 97, 98], "10": [8, 10, 14, 15, 16, 18, 22, 24, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 48, 53, 54, 55, 56, 58, 64, 66, 67, 68, 69, 73, 74, 75, 77, 79, 80, 83, 84, 86, 87, 91, 92, 93, 95], "100": [5, 10, 18, 22, 24, 27, 31, 36, 39, 42, 43, 48, 53, 60, 64, 67, 69, 72, 73, 74, 77, 79, 80, 83, 84, 86, 87], "1000": [3, 22, 24, 31, 42, 43, 64, 66, 69, 87], "10000": 5, "100000": 5, "1001": [3, 22], "1006": 3, "101": [22, 24, 84], "1013": 69, "1014": 69, "1015": 69, "1018": 36, "102": [22, 24], "1022": 69, "1023": 69, "1024": [8, 10, 15, 18, 27, 28, 29, 31, 36, 37, 39, 42, 43, 45, 46, 47, 52, 53, 54, 55, 56, 58, 60, 64, 65, 66, 67, 69, 73, 74, 75, 84], "1026": 69, "1027": 60, "103": [22, 42, 43, 73], "1030": [79, 83, 86], "1031": [79, 83, 86], "1033": [79, 83, 86], "1034": [79, 83, 86], "1035": [79, 83, 86], "1036": 27, "104": [22, 24, 42, 84], "10435229": 42, "1044": 60, "1045": 60, "1049": [3, 14], "105": [22, 24], "1051": 28, "1058": 3, "106": 22, "1061e": 42, "1063": [3, 27, 33], "1066": 28, "107": 22, "1076": [28, 33, 60], "1077": [3, 27], "1079": 28, "108": [22, 54, 84], "1080": [3, 28], "1081": [27, 28, 33], "1082": 27, "1085": 36, "10899": [14, 16], "109": [22, 24], "1091": 3, "1098": 36, "10_000": [53, 54, 55, 56], "10_llm": 62, "11": [8, 10, 14, 18, 22, 31, 36, 39, 42, 46, 55, 56, 60, 61, 64, 67, 73, 74, 84, 86, 91, 92, 93], "110": [22, 31, 72, 73, 74, 79, 84, 86], "1100": [73, 83, 84, 87], "11008": 53, "11010301": 42, "1106": 81, "1107": 42, "1108": [3, 14, 27, 60], "111": [10, 22, 24, 42], "1110": [36, 84], "112": 22, "112046": 61, "1121": 36, "11241": 60, "11246": 24, "1126": [3, 14], "1127": 14, "1128": 14, "1129": 14, "113": 22, "1130": 14, "1131": 14, "11311": 42, "1132": 14, "1134": 3, "1135": 33, "1137": 60, "114": 22, "11434": [73, 79, 83, 86], "1148": 18, "11491": 84, "115": [22, 24, 68, 84], "11599": 61, "116": [10, 22, 24, 68], "1168": 74, "117": 22, "1179": 20, "117m": [28, 36], "118": 22, "1182": 3, "119": 22, "1190": 36, "1192": 36, "1195147037506104": 43, "11_008": 54, "12": [8, 10, 14, 22, 28, 31, 36, 37, 39, 42, 43, 45, 46, 47, 52, 53, 58, 60, 61, 64, 66, 67, 68, 73, 74, 75, 77, 79, 80, 83, 84, 86, 93], "120": [10, 22, 24, 64, 66, 67], "1200": 60, "12064": [73, 74, 84], "12067825": 42, "121": [22, 24, 31, 84], "1212": 24, "121212": 31, "1218": 3, "122": [22, 48], "1220": 27, "12204": 74, "12248": 61, "12253": 61, "12259": 61, "123": [3, 4, 8, 10, 14, 20, 22, 27, 28, 29, 31, 33, 36, 37, 42, 43, 45, 46, 47, 52, 53, 54, 55, 56, 64, 73, 75, 79, 80, 83, 84, 86, 87], "1230": 36, "124": [10, 22, 24, 36, 37, 42, 73], "1240": 27, "1242": [3, 27], "12465": 39, "12490": 61, "124m": [10, 24, 36, 37, 39, 42, 43, 45, 46, 47, 53, 58, 60, 64, 66, 67, 68, 73, 75, 84], "125": 22, "12517": 61, "12525": 61, "12526": 61, "12530": 61, "12532": 61, "12541": 61, "12545": 61, "1258": 60, "1259": 3, "125kib": 73, "126": [22, 68, 84], "1260": 3, "1262": [3, 73], "1263": 27, "1269": 73, "127": [22, 84], "1270": [27, 79, 83, 86], "1271": [79, 83, 86], "127345": 61, "12746179": 42, "1275": [79, 83, 86], "1276": [79, 83, 86], "128": [14, 16, 22, 29, 39, 53], "1280": [10, 36, 37, 39, 42, 43, 45, 46, 47, 58, 60, 64, 66, 73, 75, 84], "128000": [54, 55, 56], "128001": [54, 55, 56], "128002": [54, 55, 56], "128006": [54, 55, 56], "128007": [54, 55, 56], "128009": [54, 55, 56], "1285": 27, "1288": 3, "128_256": [54, 55, 56], "129": 22, "1290": 27, "1292": 3, "1295": [3, 27], "1296": 36, "12972379": 42, "12x": 67, "13": [3, 4, 5, 8, 14, 18, 22, 31, 39, 60, 61, 67, 73, 74, 77, 84, 91, 92, 93], "130": [10, 22, 64], "1303": 3, "131": [22, 68, 72], "1311": 27, "131_072": [52, 54, 55, 56], "131k": [54, 61], "132": [22, 64], "1321": [79, 83, 86], "1322": [79, 83, 86], "1324": 36, "133": 22, "1332": 18, "1337": 60, "134": [22, 42], "1343": 3, "1347": [79, 83, 86], "1348": [79, 83, 86], "1349": [79, 83, 86], "135": [10, 22], "1350": [79, 83, 86], "1351": [3, 74, 79, 83, 86], "1352": [79, 83, 86], "1353": [79, 83, 86], "1355": 36, "1359": 3, "136": [22, 39, 64], "1361": 3, "1362": 33, "1363697": 42, "1367": 27, "137": [22, 64], "1376": [79, 83, 86], "1377": [79, 83, 86], "13779": 73, "138": [22, 68], "1380": 36, "1385": 27, "139": [8, 22], "1390": [3, 27], "13b": 81, "13e12": 39, "14": [3, 8, 14, 22, 39, 60, 67, 68, 69], "140": [22, 39], "1401": 60, "141": [22, 42, 81], "1418": 73, "142": [22, 39, 61], "1420": 27, "142156": 61, "14219": 73, "143": [22, 39, 64, 74], "1434": 36, "1435": 27, "14394": 74, "144": 22, "145": [22, 42, 81], "1452": 27, "1453": 36, "1455": 27, "1458": 60, "146": [22, 68, 81], "14610": 84, "1462": 27, "1464": [14, 16], "147": [10, 22], "1476e": 28, "1477": 27, "1479": 27, "148": [22, 81], "1480": 27, "1481": 14, "1487": 60, "149": 22, "1494": 64, "1496": 27, "1498": 27, "14_336": 54, "15": [8, 10, 14, 22, 27, 28, 33, 42, 43, 48, 53, 54, 61, 64, 67, 72, 73, 74, 98], "150": [22, 52, 54, 55, 56, 61, 81], "1500": 27, "1502": 74, "151": [8, 10, 22], "1510": 27, "1519": 60, "152": [22, 61], "1526": 27, "1529": 27, "153": [22, 64, 73], "1530": 33, "1531": 33, "1532": 33, "1533": 33, "1534": 33, "1539": 33, "154": [22, 68], "1540": 33, "1541": 33, "1542": [27, 33, 36], "1543": 33, "1544": 84, "1545": 84, "1549": 37, "15496": [14, 18, 36, 60], "155": [22, 48], "1550": 27, "15519823": 42, "1552": 36, "1558m": [10, 39, 42, 43, 45, 46, 47, 58, 60, 64, 66, 67, 73, 75, 84], "156": [22, 61], "1561": 60, "15632": [14, 16], "1563e": 42, "1564": 27, "1565": [27, 36], "157": [22, 81], "1571": 27, "1576": 14, "1579": 33, "158": 22, "1581": 27, "1585": 27, "1586": 36, "1588": 27, "1589": [14, 20], "159": [22, 61], "1590": 36, "15mib": [10, 42], "16": [10, 14, 22, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 60, 61, 64, 66, 67, 72, 73, 74, 75, 83, 84], "160": [22, 36, 37, 42, 68, 81], "1600": [10, 36, 37, 39, 42, 43, 45, 46, 47, 53, 58, 60, 64, 66, 73, 75, 84], "1605": 20, "1606": [14, 20], "161": [22, 72], "1617": 69, "1618": 3, "1619": 69, "161kib": 73, "162": 22, "1620": 69, "1624": 36, "163": [22, 36, 37], "1631": 27, "1633437166": 1, "163m": 36, "164": 22, "1646": 27, "165": [22, 42, 43], "1652": 27, "1656": 27, "1658": 27, "1659": [14, 27], "166": [22, 31], "1661": 27, "1662": 27, "1663": 27, "1664": 27, "1665": 27, "16657": 42, "1666": [27, 36], "1667": [27, 60], "1668": 27, "1669": 27, "167": 22, "1670": 27, "168": [22, 31, 68], "16833": 42, "169": 22, "1690": 14, "17": [14, 22, 24, 43, 53, 67, 73, 74, 83, 87], "170": [22, 31], "1703": 27, "1708": [74, 84], "171": 22, "171kib": 43, "172": 22, "1720": 27, "1721": 27, "1723": 27, "1726": 36, "173": [22, 48, 84], "1731": 27, "1739057839": 73, "174": [22, 73, 81], "175": 22, "176": 22, "1768": 60, "177": 22, "1778": [14, 20], "178": [22, 81], "179": [10, 22, 31, 42, 68, 81], "1792": [3, 36], "1798": 36, "18": [14, 22, 24, 39, 42, 67, 95], "180": [18, 22], "1807": [14, 16], "181": 22, "1813": 84, "1814": 36, "1819": 60, "182": [22, 48, 68], "1820": [3, 27], "18250": 14, "18261": [73, 74, 84], "1829": 33, "18290": 84, "183": [22, 81], "1830": 27, "1836": 20, "18378": 84, "184": [8, 22, 42], "18410145": 42, "1847": 20, "185": [22, 81], "1853": 33, "1858": 36, "186": [22, 81], "1860082": 42, "1869": 27, "187": 22, "1871": 20, "1875": 61, "1878": 69, "1879": [27, 69], "188": [22, 68], "1880": 69, "1881": 69, "1882": 69, "1883": 69, "1884": 69, "1885": 69, "1886": [20, 69], "1887": 69, "1888": [27, 69], "1889": 69, "189": 22, "1890": 69, "1891": 69, "1896": 27, "19": [10, 14, 22, 39, 56, 64, 67, 73, 74], "190": [22, 31], "191": 22, "1911": 84, "192": [22, 54, 81], "1920": 60, "1921": 27, "193": [8, 22], "1935": 27, "194": [8, 22, 81, 84], "19430": [73, 74, 84], "1947": 60, "195": [22, 31], "19503": 84, "1958": 27, "196": [22, 31, 74, 81], "1966": 36, "197": [22, 81], "1971": 27, "1975": 27, "198": [22, 31, 73, 74, 81, 84], "1981": 27, "19815": 74, "1983": [27, 60], "1984": 27, "199": [22, 24, 68], "1994": 24, "19mib": 42, "1_000": [31, 42, 43], "1_forward": 31, "1b": [52, 55, 56], "1d": 3, "1e": [8, 36, 39, 46, 53, 54, 55, 56], "1e0d2e6638f6": 33, "1e12": 39, "1f": 58, "1k": 86, "1kib": 42, "1st": [3, 4, 5, 24, 37, 73, 79, 80], "2": [1, 2, 15, 20, 22, 33, 39, 40, 45, 46, 47, 62, 66, 67, 69, 72, 76, 77, 78, 79, 80, 81, 83, 86, 87, 92, 95, 97, 98], "20": [3, 4, 5, 8, 10, 14, 22, 24, 36, 37, 39, 42, 43, 45, 46, 47, 58, 60, 64, 66, 68, 73, 75, 79, 80, 83, 84], "200": [5, 22, 48, 64, 77, 80], "2000": [3, 64, 66], "2006": 27, "2009": [60, 73], "201": [8, 22, 42, 61], "2010": [14, 20], "2011": 60, "20159": 39, "2016": [14, 36], "2017": 53, "2018": 68, "2019": [53, 61, 68], "202": 22, "2020": 53, "2021": 53, "2023": [53, 54, 73], "2024": [1, 68, 81], "2025": [73, 91], "203": [22, 81], "2034": 36, "2036": 27, "2039": 20, "204": [22, 39, 68], "2041": 27, "20431": [73, 74, 84], "2046": [27, 36], "20479": [14, 42], "2048": [54, 55, 56, 73, 79], "205": [22, 81], "2050": 60, "206": [22, 81], "20694106817245483": 36, "207": [22, 64], "2074": [27, 36], "2079": 36, "2079609735": 4, "208": [22, 68, 81], "209": 22, "2096": 60, "2098": 27, "20gb": 97, "21": [14, 18, 22, 24, 27, 33, 39, 64], "210": [22, 73], "2100": 84, "21017": [73, 74, 84], "211": [22, 68, 81], "21106": [73, 74, 84], "2114": 14, "212": [22, 37], "2120": 33, "2124": 60, "2128": 27, "213": 22, "2133": [33, 36], "2138": [14, 16], "214": [22, 79, 83, 86], "215": [22, 79, 83, 86], "2150": 27, "2155": 36, "216": [22, 79, 83, 86], "2161": 33, "217": [22, 68], "2170": 36, "21717": 39, "2175": 27, "2177": 33, "218": 22, "2183": 27, "2184": 27, "219": 22, "2199": 27, "21k": [10, 42, 60], "22": [14, 22, 24, 27, 28, 33, 39, 61], "220": [14, 16, 22, 37, 60], "2204": 60, "221": [22, 81], "2213": 3, "2216": [27, 33, 36], "22169792652130127": 36, "222": [22, 64, 81], "223": 22, "224": [8, 22, 81], "2241": 14, "2246": 36, "2249": 27, "225": 22, "22536": 39, "226": [22, 42], "2260": 36, "2264": [20, 27], "2267": 60, "227": [22, 42, 43, 61], "2278": 27, "228": [10, 22, 64, 81], "229": [10, 22, 81], "2298": 60, "22e12": 39, "23": [14, 22, 27, 33, 64, 67, 73, 74, 86], "230": 22, "2305": 84, "2307": 36, "231": 22, "2318": 27, "2319": 27, "232": 22, "2326": 27, "233": [22, 42], "2330": 60, "2333": 27, "234": [8, 22], "23412": [73, 74, 84], "23491": 84, "235": [22, 54, 55, 56], "2354": 33, "236": [22, 81], "2360064": 28, "2369": 27, "237": 22, "2377": 33, "2379": 27, "238": [22, 42], "239": [22, 42, 43, 81], "2390": 36, "2392": 36, "2394": 36, "23g": 43, "23it": 77, "24": [10, 14, 22, 33, 36, 37, 39, 42, 43, 45, 46, 47, 55, 56, 58, 60, 64, 66, 73, 75, 84, 86], "240": 22, "2404": 73, "2405": [74, 81], "2406": 86, "2407": 36, "24086": 36, "241": 22, "2417": 60, "242": [42, 81], "2420": [24, 60], "2422": 61, "243": 81, "2430": 36, "2439": 36, "244": [43, 81], "2451": 60, "2460": 27, "2462": 27, "247": 81, "248": 54, "2483": 20, "24830": 74, "249": 48, "24993": 84, "25": [8, 10, 14, 22, 27, 28, 33, 36, 37, 39, 42, 43, 45, 46, 47, 48, 53, 54, 58, 60, 64, 66, 67, 68, 73, 74, 75, 83, 84, 86], "251": 81, "2516": 33, "252": 81, "254": [73, 79, 86], "25449": 84, "255": [24, 81], "2553": 36, "256": [8, 14, 15, 16, 22, 24, 29, 39, 42, 43, 54, 55, 56, 60, 73, 74, 81, 84], "2561": 42, "2564": 36, "2569": 33, "257": [14, 16, 18, 24, 36, 42, 60, 61, 64, 73, 74, 84], "2575": 27, "258": 24, "2581": [73, 74, 84], "2585": 60, "259": 61, "26": [14, 22, 53, 54, 61, 68, 80, 86, 95], "260": 39, "2601": 74, "2604": 33, "261": 54, "2615": 33, "2617": 61, "262": [14, 39, 73, 74, 81, 84], "262560224533081": 74, "263": [8, 39, 81], "2630": 36, "2639": 27, "264": [8, 84], "2642": 27, "2659": 36, "266": 81, "2661": 64, "2665732204914093": 36, "2693": 27, "2698": 36, "27": [8, 14, 18, 22, 24, 68, 74, 77, 80, 86, 87], "27018": 36, "2702": 36, "2705": 27, "271": [14, 16, 24, 43, 54], "2710": 36, "272": [68, 81], "27271": 14, "2745": 27, "2748": 36, "275": 84, "2753": [14, 20], "2757": 33, "2758": 27, "27648": 61, "277": 8, "2773": 60, "2775": 27, "278": 42, "279": 81, "27it": 74, "27mib": 43, "28": [14, 22, 24, 42, 43, 55, 56, 67, 74, 83, 86], "2800": [36, 42], "280539035797119": 74, "281": [73, 74, 81, 84], "282": 81, "2838": 36, "28399": 61, "284": [14, 81, 84], "28402": 61, "285": 68, "2856": 27, "286": [74, 84], "2867": 33, "287": [14, 24, 74, 81], "2873": 60, "2879": 27, "2882": [73, 74, 84], "2885": [14, 16], "2899": 27, "28it": 74, "29": [14, 22, 39, 67, 74, 79, 83], "290": [14, 60, 81], "2925": 84, "2927": 27, "293": [8, 42], "2935": 27, "2943": 27, "2948": 27, "2952": 3, "296": 24, "29669": 42, "297": [73, 81], "298": [24, 81], "299": [24, 81], "2990": 27, "2996": 27, "29e12": 39, "2_forward": 31, "2bcollect": [10, 64], "2bspam": [10, 64], "2d": [3, 27], "2e": 39, "2f": [3, 4, 8, 10, 36, 37, 39, 42, 52, 53, 54, 55, 56, 64, 69, 73, 79, 80, 84], "2kib": 43, "2mib": 43, "2nd": [3, 4, 5, 24, 27, 37, 64, 73, 78, 79, 80], "2t": 27, "2x": [8, 10, 39, 42, 58, 64], "3": [1, 15, 16, 18, 20, 22, 32, 33, 39, 40, 45, 46, 47, 62, 66, 67, 69, 72, 75, 76, 77, 78, 80, 82, 85, 87, 89, 94, 95], "30": [3, 4, 5, 14, 18, 22, 24, 39, 42, 45, 46, 47, 53, 54, 64, 67, 68, 74, 79, 83, 84], "300": [5, 24, 48, 61], "3000": [3, 54], "30003": 73, "3008": 60, "301": [73, 81], "3010": [14, 20], "302": 24, "303": 81, "3034": 20, "3035": 20, "304": [61, 81], "3046312861972384": 43, "3055": 36, "3058": 27, "306": 64, "3061": 27, "30642": 60, "3072": [10, 37, 55, 56, 64], "308": 74, "3080": 39, "3083": 36, "3084": 27, "309": 48, "3090": 39, "30961": 36, "3097": 27, "31": [14, 22, 24, 39, 64, 79, 83, 86], "310": 81, "3103": 27, "311": 24, "3113": 3, "312": 81, "31250": [54, 55], "313": [48, 73], "314": [36, 81], "315": 81, "3159": 20, "316": 61, "3166": 60, "3168": 36, "317": [24, 81], "3178": 36, "318": [24, 60, 68, 73, 74, 84], "319": [24, 81], "3190": 27, "31900": 48, "3197": 37, "31it": 74, "32": [3, 14, 22, 24, 42, 43, 53, 54, 55, 56, 61, 64, 67, 68, 73, 79, 83], "320": [24, 42, 64], "32000": [48, 53], "3201": 36, "3205": 36, "3208": 27, "321": 24, "32100": 48, "3219": 36, "322": [24, 64], "32200": 48, "3229": 60, "32300": 48, "324": [8, 24], "32543": [73, 74, 84], "3255": [14, 20], "3256": 39, "3257": [27, 33], "3258540630340576": 36, "326": [14, 24, 42, 73, 74, 81, 84], "3274": 20, "328": 73, "3285": 14, "3288": 36, "32896995544433594": 36, "329": [8, 84], "3297": 36, "32_000": 54, "32mib": [10, 42], "32x": 43, "33": [10, 14, 22, 27, 28, 33, 53, 60, 65, 67, 74, 79, 83], "330": 73, "331": 81, "3327": 27, "333": [64, 84], "3331": 27, "334": 73, "335": 81, "336": 74, "3368": 60, "337": 81, "3374": [14, 20], "3376": 84, "3388": 36, "339": 42, "33901": 16, "33m": 48, "33mib": 60, "34": [14, 22, 39, 54, 60, 74, 79], "340": [8, 14, 64, 84], "3400": 60, "3408": 27, "340m": 68, "341": 73, "3418": 36, "34248": 39, "3427e": 28, "3428": [27, 33], "343": [16, 42, 43], "345": [14, 36, 42, 64], "346": 10, "34680": 14, "3469": 36, "347": 73, "3470": 36, "3474": 27, "348": [42, 81], "3483": 36, "349": 81, "3493": [27, 33], "35": [14, 22, 39, 68, 73, 75, 84], "350": 81, "3503": 33, "351": [60, 81, 84], "352": 81, "353": [64, 84], "354": [37, 73, 81], "355": [14, 73], "355m": [10, 39, 42, 43, 45, 46, 47, 58, 60, 64, 66, 67, 68, 72, 73, 74, 75, 84], "3565": 27, "3567": 36, "358": 81, "3589": [27, 33], "359": 84, "3593": 27, "36": [10, 14, 22, 28, 36, 37, 39, 42, 43, 45, 46, 47, 58, 60, 64, 66, 73, 75, 80, 83, 84], "360": [37, 81], "361": 84, "3610": 42, "3613": 36, "3619": [14, 16], "362": [8, 37, 61, 84], "3626": [36, 42], "3629": 60, "3634": 20, "364": 10, "3648": 33, "365": [73, 81], "3653": 36, "366": 84, "3665": 20, "36674": 60, "367": [14, 16, 81], "3677": 42, "368": 37, "3681655883789063": 74, "3694": 36, "37": [14, 18, 22, 73, 83, 86], "370": 81, "37052": 74, "3717": 36, "372": 73, "373": 14, "3737": 27, "374": 81, "375": [74, 81], "376": 81, "377": 81, "3771": 60, "3775": 27, "378": 81, "3780": 33, "3791": 60, "3796": 42, "37it": 73, "37mib": 73, "38": [10, 14, 22, 48, 64, 68, 74, 79], "380": 37, "3800": 27, "381": 73, "3814": 20, "382": 81, "383": 73, "384": [39, 74, 81], "385": 81, "386": 81, "3860": [27, 33], "3873": 27, "388": [39, 81], "389": [39, 81], "3897": 27, "38mib": 73, "39": [14, 22, 31, 73, 83], "390": [8, 81], "3903": 60, "391": [42, 73, 81], "392": 64, "3926": 36, "3928": 27, "393": 42, "3932": 36, "3934": 3, "394": [31, 73], "395": 81, "3952": 84, "396": [10, 81], "3966": 27, "397": [64, 81], "3975": 20, "398": 81, "3_forward": 31, "3b": [52, 54, 55, 56], "3c1c2d3df5b3": 83, "3d": [3, 27], "3e": [8, 39], "3f": [8, 42, 64, 84], "3f8eb4da87fa": [73, 79, 86], "3h": 48, "3mobil": 64, "3rd": 73, "3x": 67, "4": [1, 4, 15, 16, 18, 20, 22, 28, 29, 33, 39, 45, 46, 47, 48, 66, 67, 69, 72, 75, 76, 77, 78, 79, 80, 83, 86, 89, 95, 97], "40": [14, 16, 22, 31, 39, 42, 68, 73, 79, 87], "400": [48, 54, 55, 56], "401": 81, "4015": [14, 20], "402": [14, 16, 81], "4020": 60, "4023": 36, "4028": 27, "404": 81, "405": 81, "406": [37, 73, 74], "4066": 27, "4076": 20, "408": 81, "4083": 33, "409": 64, "4096": [14, 20, 39, 53, 54, 55, 56], "41": [14, 22, 54, 86], "410": 81, "412": [36, 37, 73], "4122894287109373": 74, "4126": [33, 36], "4138": 64, "415": 53, "416": 84, "4164e": 28, "417": [42, 73], "41751": 42, "4177": 27, "418": 73, "4185": 84, "419": [61, 64, 73, 81], "419259": 61, "4196": 33, "42": [14, 22, 24, 42, 43, 48, 55, 56, 67, 83], "420": 81, "420934": 73, "421": 81, "4217": 60, "4220": 27, "4222": 36, "423": [31, 64, 84], "4232": 33, "42348": 36, "424": [24, 74], "4244": 20, "425": [5, 81], "4252": 14, "4260": 20, "42651": 74, "427": 42, "42758": 74, "428": 18, "42826": 42, "4288": 60, "42g": 73, "42it": 79, "43": [14, 22, 27, 28, 33, 43, 77, 79, 83], "430": 81, "4304": 27, "4306": 27, "4307": 64, "432": [37, 81], "4327": 36, "4327e": 28, "433": 81, "434": [48, 81], "4340": 27, "435": 48, "437": 81, "4373": 60, "438": [14, 16, 18], "439": 73, "4391": 27, "439459": 73, "44": [3, 4, 14, 22, 68, 79, 87], "440": 37, "4402": 73, "4404": 33, "441": [10, 84], "4419": 27, "44201": 74, "4421": 27, "4431": 27, "4432": 36, "4435": 54, "444444": 31, "4454": 36, "446": 81, "4461": 64, "4476": 36, "448": [73, 81], "4483": 27, "449": 81, "4494": 36, "45": [10, 14, 22, 31, 64, 83, 87], "450": 81, "45000": [83, 87], "4519": [27, 33], "452": [42, 68], "453": [64, 81], "4530": 36, "4539": 36, "454": [73, 81], "4541e": 42, "4545": 27, "4548": 64, "45486": 61, "4551": 27, "456": 84, "456k": [10, 42, 43, 60, 73], "4570": 27, "4576": 27, "457kit": 18, "458": [81, 84], "4590": 33, "4594": 27, "45mib": 60, "46": [10, 14, 22, 24, 42, 43, 64, 67], "460": 68, "4606": 27, "4608": 42, "461": [8, 24], "4611": 33, "462": [10, 81], "462005": 73, "463": 81, "464": [73, 84], "46486": [73, 74, 84], "465": 24, "4656": 27, "466": [14, 24, 81], "4661": 36, "4667": 36, "467": 84, "4671": 27, "468": 81, "468845": 73, "4690": 14, "4695": 36, "46m": 48, "47": [14, 22, 48, 74], "470": 81, "471k": [10, 42, 60], "473": 81, "474": 81, "4744e": 28, "4753": 27, "4754": 27, "476": 81, "477": [73, 74, 81], "4772": [27, 33], "478": 81, "47843": 36, "4787": 36, "4798": 42, "47mib": 42, "48": [10, 14, 22, 31, 36, 37, 39, 42, 43, 45, 46, 47, 53, 58, 60, 64, 66, 68, 73, 74, 75, 79, 84], "480": 81, "4800": 33, "4808e": 28, "4814": 3, "4816": 3, "482": [54, 55, 56], "4825": 64, "483": 81, "4831": 64, "4835": 36, "484": 81, "485": [73, 79, 86], "4850": 60, "4858": 27, "486": [81, 83], "4863": 60, "487": 81, "48725": 42, "4876": [73, 74, 84], "488": 8, "489": [64, 81], "48mib": 10, "49": [14, 18, 22, 39, 56, 68, 69, 83], "490": 81, "4900": 33, "4919": 33, "492": 81, "4920": 14, "4921": 27, "492335": 73, "4925": 27, "4929": 33, "493": 48, "4936": 73, "4937": 27, "494": [79, 83, 86], "495": [79, 81, 83, 86], "4950": 27, "496": [79, 81, 83, 86], "497": [79, 83, 86], "498": [54, 55, 56, 79, 83, 86], "4985": 84, "498m": [10, 42, 60], "499": 81, "49906": 42, "49e12": 39, "4e": 39, "4f": 39, "4fa551d4f938": [73, 79, 86], "4k": 73, "4kib": 60, "4o": [24, 87], "4th": [14, 64], "4x": 61, "5": [1, 4, 8, 10, 15, 20, 22, 24, 28, 29, 33, 39, 40, 48, 49, 51, 60, 66, 67, 69, 74, 75, 77, 79, 80, 81, 83, 86, 87, 93, 95, 99], "50": [3, 10, 14, 18, 22, 24, 27, 36, 42, 43, 48, 60, 61, 64, 67, 68, 69, 73, 74, 80, 84], "500": [48, 54, 81, 84], "5000": 3, "500_000": [54, 55, 56], "502": 14, "50256": [14, 60, 64, 66, 67, 73, 74, 75, 84], "50257": [8, 10, 14, 15, 16, 18, 24, 29, 36, 37, 39, 42, 43, 45, 46, 47, 53, 58, 60, 64, 66, 73, 75, 84], "50258": 60, "50259": 60, "5035": 36, "5042": 42, "505": [73, 81], "5069": 61, "507": [73, 81], "5077": [27, 33], "5078": 61, "508": [8, 81], "5088": 60, "509": 74, "50gb": 98, "50it": [74, 80], "50k": [1, 71, 86], "50x": [10, 74], "51": [22, 39, 42, 43, 72, 73], "5100": 42, "5102": 36, "511": [81, 84], "512": [42, 74], "5120": 42, "5145": [14, 18, 42], "515": 81, "5156": 73, "5159": 27, "516": [10, 14, 15, 16, 18, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 73, 75, 79, 83, 84, 86], "517": [10, 14, 15, 16, 18, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 73, 75, 81, 84], "5173": 36, "518": [10, 14, 15, 16, 18, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 73, 75, 79, 83, 84, 86], "5181": 36, "5188": 74, "519": [74, 79, 83, 86], "5198": 36, "52": [22, 31, 39, 53, 61, 67, 74], "5200": 74, "521": [79, 83, 84, 86], "5211": 64, "522": [73, 79, 83, 86], "523": [64, 81], "524": 81, "525": 61, "526": 81, "5266": 27, "528": [10, 81], "5285": 36, "5297": 28, "5299": [27, 28, 33], "52e12": 39, "52k": 74, "53": 22, "5307": 36, "531": 24, "5311": 28, "532": 31, "5321": [27, 33], "5323": [28, 33], "533": [14, 79, 83, 86], "5331": 27, "5337": 28, "534": 81, "535": [61, 79, 81, 83, 86], "5354": 36, "536": [36, 37, 73, 79, 83, 86], "537": [79, 83, 86], "538": [79, 83, 86], "5382": 3, "539": [79, 81, 83, 86], "53mit": 18, "54": [10, 22, 67], "541": 42, "543": [48, 81], "544": 81, "5440": [27, 64], "545": [43, 81], "547": 48, "5474": 36, "5478": [27, 33], "548": 68, "5483": 39, "55": [14, 22, 27, 28, 33, 48, 72, 73, 74, 84], "550": 81, "551": 81, "5510": 27, "5517": 27, "55256": 61, "5526": [27, 33], "553": 81, "5537": 64, "554": [14, 81], "5540": 64, "5547": 64, "5548": 36, "555": 81, "556": [68, 81], "5566": 64, "5567": 64, "5568": 64, "5569": 64, "557": [31, 37, 64, 74], "5570": 64, "5571": 64, "5572": [60, 64], "5573": 33, "5577": 27, "5581": 36, "559": 81, "559617757797241": 43, "5596182346343994": 43, "56": [14, 22, 37, 74, 80], "561": [64, 81], "563": 81, "564": [68, 73, 84], "5645": [27, 61], "5647": 36, "565": 81, "5651": 60, "566": 81, "5660": 36, "5663": 60, "567": 81, "5671": 27, "5674": 60, "5675": [27, 33], "568": 14, "5680": 36, "5683": 27, "569": [73, 81], "56bb8bd477a5": 83, "56it": 80, "57": [22, 24, 27, 28, 33, 72, 73, 74, 80], "570": 73, "57250": 48, "57292": 81, "573": 81, "574": 84, "575": 73, "576": [73, 74], "577073ffcc6c": [73, 79, 86], "5775": 42, "578": 81, "5786": 27, "579": 81, "5790": 27, "57it": 73, "58": [22, 27, 28, 33, 37, 39, 73, 87], "5810": [14, 20], "582": [42, 43, 48], "583": 64, "5835": 36, "584": 81, "585": 81, "5854": 64, "5856": 36, "586": 81, "5862": 60, "5872": 36, "5874": [27, 33], "588": [14, 42], "5880": 20, "5888": 36, "589": 37, "5891": [14, 27, 33], "58e12": 39, "59": [22, 42, 73], "590": [37, 81, 84], "5903": 27, "5905": 60, "5907": 39, "5910": 27, "592": [37, 81, 84], "5931": 27, "594": 81, "5948": 39, "595": 24, "596": 14, "597": 84, "5975": 14, "5983": 64, "59it": 79, "5b": 53, "5e": [8, 10, 39, 64, 84], "5kib": [10, 43], "5mib": 73, "5x": [14, 67], "6": [1, 4, 8, 10, 18, 22, 28, 33, 39, 42, 45, 47, 48, 54, 55, 56, 66, 67, 70, 75, 77, 79, 80, 86, 87, 99], "60": [8, 10, 22, 24, 42, 43, 48, 61, 64, 73, 79, 84, 87], "600": [42, 48, 84], "6000": 3, "602": 14, "603": 73, "6047": 36, "6049": 64, "605": [74, 81, 84], "6053": 36, "606": 68, "607": [8, 84], "6070": 36, "608": 37, "609": [61, 81], "60it": 74, "61": [22, 73, 79, 83, 86], "610": [81, 84], "6100": [36, 42], "6109": 36, "611": 72, "6130": 39, "614": [43, 81], "615": 81, "6159": 36, "616": [42, 53], "617": [8, 14, 24, 64, 69, 84], "618": [81, 84], "619": 69, "6194": 27, "62": [22, 24, 31, 42, 43, 60, 67, 68, 69, 73], "620": [69, 74, 84], "6202": [27, 33], "6206": 27, "621": [36, 37, 84], "622": [69, 74, 84], "623": [69, 81], "624": 84, "6247": 37, "62568": 39, "626": [73, 81], "627": [8, 84], "628": [14, 73], "629": 73, "6298": 27, "62it": 72, "63": [5, 22, 31, 42, 43, 67, 73, 84], "630": 84, "6300": [27, 33], "631": [73, 84], "6310": 27, "6315": [14, 20], "633": [73, 74, 84], "634": [8, 73], "635": 81, "636": [74, 84], "637": [37, 64, 73, 81], "639": 84, "6398": 36, "64": [10, 22, 27, 28, 33, 61, 67, 68, 73, 74, 79, 80, 83], "640": [8, 64], "642": [81, 84], "643": 81, "6437": 60, "6440994262695314": 74, "645": [14, 81], "646": 84, "647": [8, 73, 84], "648": [73, 84], "649": 60, "6496": 27, "65": [3, 4, 22, 39, 67, 68, 73, 77, 79, 80, 83, 84, 86, 87, 95], "650": [81, 84], "6503": 27, "6515": 27, "652": [74, 84], "6525": 73, "653": [73, 84], "654": 24, "655": 68, "656": [73, 84], "6565": 27, "657": 73, "658": 84, "6584": 27, "659": [73, 81, 84], "66": [18, 22, 27, 28, 33, 39, 67, 72, 73, 74, 84, 95], "660": [81, 84], "661": [42, 73], "662": [68, 81, 84], "6621": 36, "6622": 36, "663": [61, 73, 84], "664": 48, "665": 81, "6654": 27, "666": [10, 68, 73, 84], "668": [72, 73], "669": [81, 84], "66m": 68, "67": [14, 18, 22, 42, 54, 64, 65, 67, 73, 84], "670": 68, "671": 73, "672": [48, 73, 84], "6720": 36, "6726": 84, "673": 81, "674": 73, "6745": 60, "6747": 36, "6754": 36, "676": [73, 84], "6776e": 42, "6779": 39, "678": 73, "679": [73, 81], "6796e": 28, "68": [22, 31, 37, 54, 73, 84], "680": [73, 84], "6806": 36, "6808": 60, "681": 73, "682": [68, 84], "6827": [73, 84], "683": [24, 48, 73], "685": [73, 81, 84], "687": 84, "688": [54, 55, 56, 61, 68, 84], "689": 73, "69": [22, 67, 68, 73, 84], "690": [42, 68, 84], "691": 84, "692": 84, "693": [68, 81, 84], "6931": 84, "6931471824645996": 84, "6936": 36, "694": [48, 81], "695": 68, "6957": 20, "698": 68, "698406": 81, "699": [68, 73, 84], "69mib": [42, 73], "69mit": 18, "6a0746a1ec1a": [73, 79, 86], "6e": 39, "6mib": 73, "6x3": 14, "7": [1, 4, 5, 8, 10, 18, 22, 27, 39, 42, 43, 54, 55, 56, 66, 67, 75, 79, 81, 83, 84, 86, 87, 88, 95, 99], "70": [22, 64, 67, 73, 79, 83, 84, 86], "700": 48, "7000": 3, "7003": 27, "7026": [14, 16], "7058": 27, "706": 68, "7070": 27, "70b": [1, 73, 79, 81, 86], "71": [22, 42, 43, 55, 56, 73, 84], "7120": 36, "7121": 60, "7130": 36, "714": [8, 73], "7140e": 28, "7154": 27, "716": 36, "71630220413208": 74, "717": [42, 73, 81], "7176": 3, "7179": 27, "719": 73, "7195": 36, "72": [22, 64, 67, 68, 73, 84], "721": 81, "722": [14, 37], "7220": 74, "7235": 64, "7238": 61, "724": 48, "725": 42, "726": 42, "7267": 36, "727": 74, "729": 81, "73": [22, 42, 43, 73, 74, 84], "732": [42, 73], "733": 84, "734": [73, 84], "7342": 3, "7349": 36, "735": [8, 73], "736": 81, "738": 53, "7380": 60, "7388": 27, "739": 73, "73mib": 43, "74": [22, 68, 73, 80, 84], "7406": 39, "742": [24, 48], "7425": 36, "743": 73, "745": 24, "746": [8, 14], "747": 64, "749": 37, "75": [3, 4, 10, 22, 36, 42, 43, 64, 65, 67, 68, 72, 73, 84], "750": [8, 74], "7500": 42, "752": 5, "753": 81, "754": [14, 73], "754748503367106": 43, "7547486888037787": 43, "7548": 3, "7557": 36, "7559e": 42, "7599": 27, "75mib": 43, "76": [22, 73, 84], "760": 73, "761": 74, "7619192123413088": 72, "7619335651397705": 74, "761933708190918": 73, "762": 36, "763": 73, "768": [8, 10, 28, 31, 36, 37, 39, 42, 43, 45, 46, 47, 53, 58, 60, 64, 66, 73, 75, 84], "7684114456176756": 74, "77": [3, 10, 18, 22, 27, 28, 33, 39, 42, 43, 60, 73, 74, 84], "770": 81, "7705": 36, "772": 48, "7722": 42, "773": [37, 81], "774m": [10, 39, 42, 43, 45, 46, 47, 58, 60, 64, 66, 67, 73, 75, 84], "775": 68, "776": 73, "7764": 42, "77e12": 39, "78": [22, 65, 67, 73, 79, 84], "781": 42, "782": 73, "784": 81, "7849": [14, 20], "7860": 36, "7871": 61, "7873": 60, "788": 81, "789": [8, 27, 73], "7891": 27, "78mib": 60, "79": [22, 42, 43, 67, 68, 73, 84], "790": [31, 84], "792": 37, "793": [14, 81], "7936": 73, "7939": 27, "7940": 42, "7953": 20, "7957": 60, "7969": 61, "799": 84, "7b": [42, 52, 53, 54, 81, 86], "7e": 39, "7h": 48, "7k": 43, "7kib": [42, 60], "7mib": 60, "7th": 43, "8": [2, 4, 5, 8, 10, 15, 16, 18, 22, 24, 28, 29, 36, 39, 42, 43, 53, 54, 55, 56, 58, 66, 67, 74, 75, 79, 80, 83, 84, 86], "80": [22, 27, 28, 33, 39, 67, 68, 69, 72, 73, 74, 84], "800": [31, 37, 48, 73, 81], "8000": [51, 70, 88], "8040": 27, "80489": 81, "8053": 27, "806": 73, "8061": [20, 36], "809": 73, "81": [22, 31, 67, 73, 83, 84], "8111": 27, "8113": 39, "8138": 36, "814": [54, 55, 56], "818": [8, 61], "8192": [52, 54, 55, 56], "82": [22, 39, 64, 73, 84], "820": 10, "8203": [27, 42], "8210": 27, "8218": 60, "824": 37, "8243": 36, "8259087562561036": 73, "827": 8, "828": 8, "8296": 27, "83": [18, 22, 36, 37, 67, 73], "8310": 73, "832": [79, 83, 86], "833": [79, 83, 86], "834": [79, 83, 86], "836": 73, "838": 37, "8380e": 28, "8383": 60, "839039182662964": 72, "84": [18, 22, 24, 67, 84], "840": 37, "8400": [14, 20], "841": 24, "8415": 60, "8421e": 28, "8422": 36, "8424": 27, "8434": 27, "8435": 60, "844": [79, 83, 86], "845": [73, 79, 83, 86], "846": [79, 83, 86], "847": [79, 81, 83, 86], "8477": [73, 74, 84], "8479": 60, "848": 68, "84m": 43, "85": [22, 27, 28, 33, 64, 67, 68, 69, 73, 84], "850": [14, 42], "851": [8, 14], "8524": 27, "853": 68, "8557": 36, "8569": 3, "857": 73, "8573": 27, "8574": 60, "859": 73, "86": [16, 22, 73, 84], "860": 81, "864": 54, "868": 69, "869": 69, "86mib": 10, "87": [22, 27, 28, 33, 67, 73, 74, 84], "870": [69, 84], "871": 69, "8713": 60, "8719": 36, "872": [69, 73], "873": 69, "874": 69, "875": 69, "876": [8, 69, 81], "877": 69, "878": [69, 81], "879": 69, "88": [22, 68, 69, 73, 84], "880": [22, 69], "881": [22, 69, 73], "8812": 14, "882": [22, 42, 54, 69], "883": 22, "884": [8, 22], "8843": 36, "8887": 14, "89": [22, 27, 28, 33, 42, 43, 53, 68, 73, 84], "8901": 60, "8906": 61, "8909e": 28, "891": 37, "893": [81, 84], "895": 61, "8961": 60, "898": [74, 81], "8993": 27, "8ab4849b038c": [73, 79, 86], "8b": [73, 79, 86], "8cf247399e57": 83, "8e": 39, "8x7b": 81, "8xa100": 42, "9": [2, 8, 10, 14, 18, 22, 28, 39, 42, 43, 48, 53, 54, 55, 56, 58, 64, 67, 68, 72, 74, 76, 83, 86], "90": [8, 10, 22, 42, 43, 60, 64, 68, 84], "900": 48, "9000": 3, "9004": 61, "901": 18, "906": 73, "907": 84, "909": 81, "91": [22, 43, 52, 67, 68, 69, 73, 74, 76, 84], "912": 81, "913": [10, 14, 15, 16, 18, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 73, 75, 84], "9132": 60, "914": [10, 14, 15, 16, 18, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 61, 64, 66, 73, 75, 84], "915": [10, 14, 15, 16, 18, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 73, 75, 84], "9156": 27, "916": [10, 14, 15, 16, 18, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 73, 75, 84], "917": [8, 10, 14, 15, 16, 18, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 73, 75, 84], "9178": 14, "918": [10, 14, 15, 16, 18, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 73, 75, 84], "919": [10, 14, 15, 16, 18, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 73, 75, 84], "91901": 61, "9192": 60, "92": [22, 39, 67, 68, 84], "920": 48, "922": 14, "9268": 27, "927k": 73, "928": 74, "9289": 36, "93": [22, 31, 39, 67, 68, 73, 84], "9325": 74, "933": 42, "934": 8, "935": [48, 72, 73, 74, 84], "936": 8, "937": 8, "938": 8, "939": [8, 10, 14, 15, 16, 18, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 48, 53, 54, 55, 56, 58, 64, 66, 72, 73, 75, 79, 83, 84, 86], "94": [18, 22, 68, 76, 84], "940": [10, 14, 15, 16, 18, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 48, 53, 54, 55, 56, 58, 64, 66, 73, 75, 79, 83, 84, 86], "941": [10, 14, 15, 16, 18, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 73, 75, 79, 83, 84, 86], "942": [10, 14, 15, 16, 18, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 73, 75, 79, 83, 84, 86], "9422": 27, "943": [10, 14, 15, 16, 18, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 73, 75, 84], "944": [10, 14, 15, 16, 18, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 73, 75, 84], "945": [10, 14, 15, 16, 18, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 73, 75, 84], "9450": 27, "946": [10, 14, 15, 16, 18, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 48, 53, 54, 55, 56, 58, 64, 66, 73, 75, 84], "948": 84, "9484": 36, "9485": 36, "949": 8, "95": [10, 22, 64, 65, 67, 68, 73, 84], "9509": 3, "9544": 27, "9546": 84, "956": 14, "959": 16, "96": [10, 22, 65, 67, 68, 83], "9605e": 36, "961": [42, 48], "962": 84, "964": 84, "9660": 60, "9666": 14, "968": [79, 83, 86], "969": [79, 83, 86], "9693": 3, "97": [10, 22, 24, 39, 64, 67, 68, 84], "970": [79, 83, 86], "972": 24, "973": 72, "9737": 27, "975": [14, 48], "976": 84, "978": 1, "9781633437166": 1, "97e12": 39, "98": [22, 67, 79, 84], "98110580444336": 42, "982": [48, 84], "984": [14, 48], "985": [42, 43, 73], "9868e": 36, "98758347829183": 42, "988": 14, "99": [14, 22, 31, 42, 67, 68, 69, 79, 84], "9902": 64, "9904": 64, "9906": 54, "9912": 27, "992": 22, "993": [22, 84], "994": 22, "9949": 3, "995": [18, 22], "996": 22, "997": [22, 24], "998": 22, "9982": 3, "999": [3, 22, 73, 84], "9991": 3, "9995": 27, "99mib": 10, "9e": 39, "9e12": 39, "A": [1, 8, 10, 14, 15, 16, 33, 36, 37, 39, 42, 43, 45, 46, 47, 48, 53, 54, 58, 60, 61, 64, 66, 68, 73, 75, 79, 82, 85, 86, 87, 91, 94, 97], "And": [8, 14, 20, 27, 77, 92, 93], "As": [10, 12, 14, 20, 24, 27, 33, 36, 42, 48, 53, 54, 58, 60, 61, 64, 67, 73, 74, 84, 86, 87, 99], "At": [14, 42, 53, 58], "Be": [14, 86], "But": [14, 33, 58, 73, 93], "By": [8, 14, 27, 73, 91, 92], "For": [8, 10, 14, 24, 27, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 48, 52, 53, 54, 55, 56, 58, 60, 61, 64, 65, 67, 73, 74, 76, 79, 81, 83, 84, 86, 87, 90, 91, 92, 93, 94, 97, 99], "IF": 64, "IN": 64, "If": [0, 1, 2, 8, 10, 14, 20, 24, 27, 28, 31, 36, 39, 42, 43, 51, 53, 54, 55, 56, 58, 60, 64, 67, 70, 73, 74, 77, 80, 83, 84, 86, 87, 88, 90, 91, 92, 93, 94, 97, 99], "In": [1, 3, 4, 5, 8, 10, 12, 14, 15, 16, 18, 20, 22, 24, 25, 27, 28, 29, 31, 33, 34, 36, 37, 39, 40, 42, 43, 45, 46, 47, 48, 53, 54, 55, 56, 58, 60, 64, 66, 67, 69, 73, 74, 75, 77, 79, 80, 81, 83, 84, 86, 87, 91, 92, 93, 96, 97, 98, 99], "It": [10, 14, 27, 31, 33, 42, 43, 52, 53, 55, 56, 64, 68, 73, 78, 79, 80, 83, 86, 87, 91, 94, 98], "Its": [64, 87], "NOT": 27, "Near": 1, "No": [1, 3, 4, 5, 8, 10, 14, 15, 16, 18, 20, 22, 24, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 60, 64, 66, 69, 73, 74, 75, 76, 81, 84], "Not": 42, "Of": [53, 54], "On": [56, 91, 93], "One": [14, 27, 36, 58, 76, 77, 87], "Or": 39, "Such": 87, "That": [10, 24, 33, 42, 60, 84, 91], "The": [1, 2, 5, 10, 14, 16, 18, 20, 28, 29, 30, 33, 36, 37, 39, 42, 43, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 56, 58, 60, 61, 64, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 83, 84, 86, 87, 88, 91, 93, 97, 98, 99], "Their": [52, 55, 56, 73], "Then": [10, 27, 33, 42, 53, 54, 55, 56, 58, 64, 66, 74, 87, 93, 94, 99], "There": [8, 12, 14, 24, 42, 43, 54, 73, 74, 83, 87, 91, 99], "These": [14, 27, 42, 54, 73, 84, 99], "To": [1, 10, 14, 22, 24, 27, 31, 42, 43, 51, 53, 54, 58, 61, 64, 70, 74, 79, 84, 86, 87, 88, 91, 92, 93, 94, 99], "Will": 64, "With": [1, 42, 73, 74, 79, 86, 90], "_": [14, 27, 29, 31, 36, 37, 42, 53, 54, 55, 56, 60, 81], "_1": 53, "_2": [8, 27, 53], "_3": 53, "__dict__": [18, 54], "__getitem__": [3, 4, 14, 15, 16, 22, 29, 64, 73, 74, 84], "__init__": [3, 4, 5, 10, 14, 15, 16, 18, 22, 24, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 69, 73, 74, 75, 77, 80, 84, 87], "__len__": [3, 4, 14, 15, 16, 22, 29, 64, 73, 74, 84], "__version__": [3, 4, 14, 18, 20, 31], "_buffer": [54, 55, 79, 83, 86], "_calculate_fan_in_and_fan_out": 31, "_call_chain": [79, 83, 86], "_call_impl": 33, "_compiled_call_impl": 33, "_compute_llama3_paramet": [55, 56], "_create_connect": [79, 83, 86], "_dynamo": 31, "_encod": [79, 83, 86], "_engin": 69, "_global_backward_hook": 33, "_global_backward_pre_hook": 33, "_global_forward_hook": 33, "_global_forward_pre_hook": 33, "_longest_encoded_length": 64, "_make_engin": 69, "_mask": 84, "_mergeable_rank": 60, "_modified_open": 24, "_norm": 8, "_open": [79, 83, 86], "_pat_str": 60, "_read": 69, "_refine_defaults_read": 69, "_respons": [79, 83, 86], "_send_output": [79, 83, 86], "_send_request": [79, 83, 86], "_special_token": 60, "_ssl": 24, "_token": 53, "_validate_nam": 69, "_wrapped_call_impl": 33, "a100": [10, 39, 42, 56, 61, 64, 67, 73, 84], "a10g": 39, "aa81b541aae6": 83, "ab": [10, 36, 73, 74, 81, 86], "abbrevi": 77, "abil": 84, "abl": [8, 10, 24, 42, 53, 74, 97], "about": [10, 14, 24, 27, 31, 36, 42, 48, 54, 55, 56, 60, 61, 64, 67, 73, 74, 77, 79, 80, 83, 84, 86, 87, 99], "abov": [8, 10, 14, 20, 24, 27, 31, 33, 36, 37, 42, 51, 52, 53, 54, 58, 60, 61, 64, 68, 70, 73, 74, 77, 78, 79, 80, 81, 83, 84, 86, 87, 88, 91, 93, 94, 99], "absl": 73, "absolut": [14, 36, 53, 69, 73, 80], "abstract": 10, "abstracthttphandl": [79, 83, 86], "ac": 24, "acc": 67, "acceler": [8, 33, 73, 99], "accept": [1, 10, 43, 53, 54, 55, 56, 86, 87, 98], "access": [1, 27, 43, 48, 53, 54, 55, 56, 58, 67, 73], "access_token": [53, 54], "accident": 8, "accomplish": [20, 73], "accord": [31, 74, 84], "accordingli": [14, 27, 53, 54, 59], "account": [27, 39, 53, 54, 55, 56, 77, 80, 86, 87], "accumul": [39, 67], "accumulation_step": 67, "accur": [53, 54, 73, 79, 80, 87], "accuraci": [10, 65, 67, 68, 69, 73, 80, 87], "accuracy_scor": 69, "accuracy_test": 69, "accuracy_train": 69, "accuracy_v": 69, "accusatori": 86, "accustom": 87, "achiev": [27, 31, 36, 42, 48, 64, 67, 68, 73, 74, 79], "acknowledg": 54, "across": [8, 36, 42, 43, 54, 61, 73, 79, 83, 86], "act": [46, 64, 86], "action": [8, 54, 58, 64, 84, 91], "activ": [3, 54, 73, 74, 75, 76, 79, 80, 84, 86, 91, 93], "actual": [24, 42, 43, 54, 58, 77, 80, 84, 87], "ad": [1, 10, 24, 27, 29, 31, 48, 50, 53, 54, 61, 67, 73, 77, 79, 83, 84, 86, 97], "adam": 42, "adamw": [8, 10, 42, 43, 48, 64, 73, 84], "adapt": [10, 42, 54, 64, 67, 86], "add": [8, 10, 14, 24, 27, 31, 36, 37, 42, 48, 49, 54, 55, 56, 59, 60, 61, 64, 66, 67, 73, 74, 77, 80, 83, 87, 90, 91, 92, 93, 97], "add_bias_kv": 31, "add_head": [73, 79, 83, 86], "add_to_git_credenti": [53, 54, 55], "addit": [1, 5, 10, 14, 18, 31, 36, 38, 42, 47, 48, 53, 54, 55, 56, 58, 64, 65, 71, 73, 74, 76, 80, 84, 87, 91, 94, 98], "addition": [1, 24, 53, 54, 58, 67, 87, 99], "additional_experi": 67, "addmmbackward0": 3, "address": [51, 58, 70, 73, 79, 80, 83, 86, 87, 88], "adequ": 80, "adher": [73, 80, 86], "adjac": 24, "adjust": [8, 10, 14, 31, 36, 42, 43, 48, 53, 54, 55, 56, 64, 74, 84], "admir": 80, "adopt": 36, "adult": 86, "adulthood": 86, "advanc": [8, 36, 42, 48, 58, 61], "advantag": [2, 27, 33, 74, 87, 91, 99], "adventur": 86, "advers": 86, "advic": 61, "advis": 91, "aero": 54, "aesthet": 31, "affair": 61, "affect": [10, 27, 84], "aforement": 74, "afraid": 86, "african": 86, "after": [8, 10, 14, 24, 27, 36, 40, 42, 43, 52, 53, 54, 55, 56, 60, 61, 64, 73, 74, 77, 79, 83, 84, 86, 91, 94], "again": [8, 27, 33, 42, 43, 54, 58, 60, 64, 84, 87, 97], "against": [27, 42, 64, 79], "aggreg": 31, "ago": 61, "agre": 86, "agreement": 86, "ah": 14, "ai": [40, 42, 52, 53, 54, 55, 56, 62, 73, 86, 99], "aid": [61, 87], "aim": 84, "air": [8, 10, 42, 54, 55, 64, 67, 73, 79, 83, 84, 86, 99], "ak": 16, "aka": [27, 29, 31, 53, 54, 55, 56], "akin": 36, "aksionov": 31, "akwirw": 16, "al": [10, 36, 81], "alder": 73, "alfalfa": [52, 54, 55, 56, 73], "algorithm": 14, "alia": 98, "align": [1, 42, 53, 54, 64, 73, 80, 82, 83, 86], "all": [1, 2, 3, 4, 8, 10, 13, 14, 20, 24, 26, 31, 33, 35, 37, 41, 42, 53, 55, 56, 61, 63, 64, 65, 67, 68, 72, 73, 74, 80, 81, 84, 86, 87, 91, 92, 93, 94, 97, 99], "all_context_vec": 27, "all_token": 14, "all_word": 14, "allclos": 53, "alloc": [52, 56, 58, 61], "allow": [8, 10, 14, 24, 27, 36, 42, 46, 48, 53, 54, 58, 64, 73, 79, 83, 86, 87, 97, 99], "allowed_max_length": [73, 74, 84], "allowed_speci": [14, 15, 16, 18, 22, 24, 29, 42, 54, 55, 56, 60, 64, 73, 74], "almost": [10, 14, 36, 42, 53, 54, 67, 84], "alon": [27, 42], "along": [10, 25, 34, 40, 54, 55, 56, 84], "alongsid": [33, 63, 72], "alpa": 74, "alpaca": [53, 73, 84, 86, 87], "alpaca52k": 74, "alpaca_52k": 74, "alpaca_data": 74, "alpaca_ev": 73, "alpacaev": 73, "alpha": [10, 42, 64, 74], "alphabet": 74, "alreadi": [14, 24, 27, 36, 37, 42, 43, 53, 64, 72, 73, 74, 84, 99], "also": [8, 10, 14, 20, 24, 27, 31, 36, 39, 40, 42, 43, 52, 53, 54, 55, 56, 58, 60, 64, 67, 73, 74, 77, 79, 80, 83, 84, 86, 87, 91, 92, 93, 94, 97, 98, 99], "alter": 80, "altern": [1, 8, 10, 36, 40, 42, 48, 51, 53, 54, 56, 58, 62, 64, 67, 70, 73, 78, 79, 83, 84, 86, 88, 91, 92, 93, 94, 97, 99], "although": [24, 83], "altogeth": [79, 86], "alwai": [14, 42, 61, 91, 92, 93], "am": [36, 61, 86, 91, 92, 93], "amaz": [79, 86], "amazon": [1, 54, 98], "ambigu": [73, 87], "among": [14, 42, 54, 64], "amount": [8, 36, 42, 58], "amp": 64, "amper": 61, "amus": 8, "an": [2, 8, 10, 12, 14, 22, 24, 27, 28, 43, 48, 52, 53, 54, 55, 56, 58, 60, 61, 62, 64, 68, 71, 72, 74, 75, 76, 78, 79, 80, 83, 84, 85, 87, 89, 91, 92, 93, 96, 97, 98, 99], "analog": 14, "analys": [38, 43, 81, 87], "analysi": [1, 38, 40, 58, 87], "analyz": [40, 73], "ancient": 86, "andrei": 31, "andrej": 61, "angel": 86, "angl": [53, 54, 55, 56, 86], "ani": [1, 14, 24, 27, 36, 42, 53, 54, 58, 64, 67, 73, 77, 79, 83, 86, 87, 90, 91, 92, 93, 94, 99], "anim": [73, 79, 80, 83, 86], "anneal": [8, 42], "annot": 31, "announc": 54, "anoth": [8, 14, 27, 33, 42, 54, 61, 73, 79, 83, 86], "answer": [43, 64, 67, 73, 79, 80, 83, 84, 87], "anticip": 67, "antonym": [73, 74, 84], "anxieti": 86, "anymor": 47, "anyon": [77, 80, 87], "anyth": 86, "anywai": [14, 64], "apach": [55, 56], "api": [1, 73, 89], "api_kei": [77, 80, 87], "apolog": 54, "app": 24, "app_orig": 51, "app_own": 51, "appeal": [45, 47], "appear": [2, 13, 24, 26, 35, 41, 63, 72, 86, 87], "append": [8, 14, 15, 16, 22, 24, 29, 31, 36, 42, 54, 55, 56, 58, 64, 73, 74, 79, 80, 84, 86, 87], "appendix": [1, 14, 42, 48, 49, 50, 64, 67, 74], "appendix_": 74, "appendix_a": 90, "appendix_d": 90, "appl": [8, 10, 42, 53, 54, 64, 73], "appli": [8, 10, 14, 19, 24, 25, 31, 36, 42, 53, 54, 55, 56, 58, 64, 67, 73, 74, 79, 80, 83, 84, 86, 87, 91, 97], "applic": [8, 10, 42, 43, 53, 54, 64, 73, 79, 80, 83, 86], "apply_rop": 56, "appoint": [42, 43], "approach": [1, 10, 12, 14, 22, 24, 27, 42, 58, 73, 83, 86, 87, 91, 92, 93, 97, 99], "appropri": [36, 53, 73, 74, 75, 79, 80, 83, 84], "approx": [10, 36], "approxim": [8, 10, 36, 42, 43, 48, 52, 53, 64, 74, 87], "apt": [48, 91], "ar": [1, 8, 10, 14, 15, 16, 19, 20, 24, 25, 27, 28, 29, 31, 32, 33, 34, 36, 37, 39, 42, 43, 48, 52, 53, 54, 55, 56, 58, 60, 61, 64, 66, 67, 68, 69, 73, 74, 77, 79, 80, 81, 83, 84, 86, 87, 91, 92, 93, 94, 96, 97, 99], "arang": [14, 15, 29, 36, 37, 42, 53, 54, 55, 56], "arbitrari": [24, 27, 86], "architectur": [10, 27, 40, 42, 52, 54, 58, 62, 64, 68, 73, 74], "archiv": [10, 64], "area": 86, "aren": [14, 86], "arena": 73, "arg": [24, 33, 58, 79, 83, 84, 86], "argmax": [3, 4, 36, 42, 43, 55, 56, 64, 66], "argu": 36, "argument": [42, 45, 47, 48, 73, 74, 84, 86], "arm": 42, "arm64": 91, "aro": 64, "around": [10, 36, 54, 73, 74, 79, 83, 86], "arrai": [3, 24, 36], "arrang": [74, 86], "arrt": 14, "art": [24, 42], "articl": [14, 61, 84], "artifact": 97, "arxiv": [73, 74, 81, 84, 86], "ary3d": 3, "as_vers": [18, 54], "ascertain": [79, 80], "ascii": 24, "ask": [0, 58, 64, 79, 80, 84, 86, 87, 97], "aspect": [73, 80, 86], "aspen": 73, "assembl": 84, "assert": [14, 27, 29, 31, 53, 54, 55, 56, 64, 69, 84], "assess": 73, "assign": [8, 10, 24, 42, 45, 53, 54, 55, 56, 58, 64, 84, 90], "assign_check": 46, "assist": [54, 55, 56, 73, 74, 86, 87], "associ": [33, 54, 73, 74, 84, 86, 97], "assum": [8, 33, 36, 37, 39, 42, 53, 54, 55, 56, 58, 73, 79, 80, 84, 87, 91, 92, 93], "astral": 93, "astrologi": 86, "async": 31, "ate": 77, "athen": 83, "atmospher": 86, "atom": [78, 79, 80, 86], "att": [10, 36, 37, 42, 45, 46, 53, 54, 55, 56, 64], "attempt": [33, 73, 79], "attend": [14, 31, 54, 64, 67], "attent": [1, 8, 10, 14, 28, 32, 33, 42, 43, 48, 53, 55, 56, 60, 61, 64, 67, 73, 86], "attention_norm": 53, "attn": [42, 45, 46], "attn_dropout": 46, "attn_mask": 31, "attn_output": 31, "attn_output_weight": 31, "attn_scor": [27, 28, 29, 31, 33, 53, 54, 55, 56], "attn_score_22": 27, "attn_scores_2": 27, "attn_weight": [27, 28, 29, 31, 33, 53, 54, 55, 56], "attn_weights_2": 27, "attn_weights_2_na": 27, "attn_weights_2_tmp": 27, "attract": 10, "attribut": [8, 33], "audienc": 1, "audit": [79, 83, 86], "augment": [36, 42], "austen": [73, 74, 84], "authent": 53, "author": [1, 73, 74, 84, 87], "auto": [18, 81], "auto_open": [79, 83, 86], "autograd": 3, "autom": [73, 97], "automat": [1, 2, 36, 42, 51, 52, 70, 73, 79, 86, 88, 97, 99], "autonotebook": 18, "avail": [1, 8, 10, 24, 36, 42, 53, 54, 58, 64, 73, 84, 87, 93, 97, 98], "averag": [42, 61, 67, 72, 73, 74, 79, 80, 83, 84], "average_embed": 67, "avg": 61, "avg_log_prob": 84, "avg_log_proba": 42, "avoid": [8, 36, 39, 45, 47, 53, 84, 86, 87, 91], "avx2": 73, "avx512_vnni": 73, "avx512f": 73, "aw": 42, "awai": [42, 86], "awaken": 86, "awar": 87, "award": [64, 66, 80], "awww": 64, "ax": [31, 42], "ax1": [42, 64], "ax2": [42, 64], "axi": [42, 45, 46, 64], "axiom": [42, 43], "axolotl": 73, "ayat": 86, "ayoosh": 73, "azur": 54, "b": [1, 3, 5, 10, 24, 27, 31, 33, 42, 45, 46, 53, 54, 55, 56, 64, 69, 73, 79, 80, 83, 84, 86], "ba": 36, "back": [8, 14, 24, 36, 37, 42, 43, 48, 53, 54, 55, 56, 60, 84, 86, 91, 92, 93], "backblazeb2": [10, 64], "backdrop": 86, "backend": [8, 10, 42, 52, 53, 54, 55, 56, 64, 73], "background": [64, 73], "backpropag": [8, 10, 14, 67, 74], "backup": [10, 64], "backward": [3, 4, 8, 36, 39, 42, 64, 84], "bad": 87, "bag": 86, "bait": 86, "bake": 86, "balanc": [10, 64, 68, 69, 73], "balanced_accuracy_scor": 69, "balanced_accuracy_test": 69, "balanced_accuracy_train": 69, "balanced_accuracy_v": 69, "balanced_df": [10, 64], "balloon": 86, "ballpark": [42, 43, 74, 81], "bar": [31, 36, 42, 51, 70, 73, 77, 79, 80, 83, 86, 87, 88], "bar_width": 42, "bark": 73, "barlei": [54, 73, 79, 83, 86], "barrier": 91, "base": [1, 8, 10, 14, 24, 31, 33, 36, 37, 42, 43, 50, 52, 53, 54, 55, 56, 60, 61, 64, 66, 67, 73, 74, 79, 80, 83, 84, 85, 86, 87, 88], "base_config": [10, 37, 39, 42, 45, 46, 47, 58, 64, 65, 66, 73, 74, 75, 84], "base_path": 65, "base_token": 60, "baselin": [58, 74, 93], "baseline_mem": 58, "basi": 84, "basic": [27, 42, 52, 84, 87], "bass": 53, "batch": [3, 4, 8, 10, 14, 15, 16, 22, 27, 28, 29, 31, 33, 36, 42, 55, 56, 64, 66, 67, 74], "batch_data": 84, "batch_exampl": 36, "batch_first": 31, "batch_idx": [3, 4, 84], "batch_max_length": [73, 74], "batch_siz": [3, 4, 8, 10, 14, 15, 16, 22, 27, 29, 31, 36, 37, 39, 42, 43, 48, 53, 54, 55, 56, 64, 67, 73, 74, 84], "ba\u015f\u0131nda": 54, "beam": 86, "beard": [8, 42], "beauti": 24, "becaus": [3, 8, 10, 14, 20, 24, 27, 33, 39, 42, 43, 54, 58, 64, 67, 73, 80, 84, 86, 87, 97], "becom": [27, 36, 43, 44, 58, 62, 86, 94], "bed": 86, "been": [8, 10, 24, 27, 33, 42, 43, 53, 54, 55, 61, 64, 66, 73, 74, 77, 79, 83, 86, 87, 91, 97], "befor": [10, 14, 24, 27, 36, 42, 53, 54, 55, 56, 58, 61, 64, 73, 74, 79, 83, 84, 86, 91, 97], "begin": [8, 10, 14, 24, 36, 42, 58, 73, 84, 87, 97], "begin_of_text": [54, 55, 56, 86], "beginn": 91, "behav": 8, "behavior": [36, 42, 54, 60, 64, 86, 87], "behind": [1, 8, 27, 32, 34, 36, 42, 54], "being": [10, 14, 15, 16, 24, 27, 42, 48, 53, 54, 55, 56, 64, 73, 79, 86], "bell": [1, 48, 50], "below": [1, 8, 10, 12, 14, 20, 24, 25, 27, 30, 31, 32, 33, 34, 36, 37, 40, 42, 43, 47, 48, 51, 52, 53, 54, 55, 58, 60, 61, 64, 67, 70, 73, 74, 75, 76, 77, 79, 80, 81, 83, 84, 86, 87, 88, 91, 93, 94, 97, 99], "bench": 81, "benchmark": [17, 25, 30, 31, 73], "benefici": 87, "benefit": [86, 97], "bertrand": 61, "besid": [24, 61], "bessel": 36, "best": [1, 24, 31, 36, 39, 61, 73, 79, 80, 84, 86, 99], "best_practic": 58, "beta": 84, "better": [10, 24, 27, 30, 36, 60, 61, 67, 68, 73, 74, 79, 80, 84, 87], "between": [1, 8, 10, 14, 24, 27, 31, 33, 36, 42, 54, 55, 56, 60, 67, 73, 76, 81, 84, 99], "beyond": 36, "bfloat16": [39, 53, 54, 55, 56], "bhmd": 31, "bhnd": 31, "bhnm": 31, "bia": [3, 8, 10, 20, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 60, 64, 66, 73, 75, 84], "bias": [31, 36, 48, 60], "bias_k": 31, "bias_q": 31, "bias_v": 31, "biblic": 86, "bibtex": 1, "big": 27, "bigger": [24, 60], "bigram": 24, "bill": [77, 80, 87], "billion": [53, 54, 61, 73, 79, 83, 86], "bin": [31, 69, 91, 93], "binari": [64, 67, 68, 69, 73], "binary_cross_entropi": 3, "bincount": [42, 43], "bind": [79, 83, 86], "biolog": 87, "birch": 73, "birthdai": 86, "bit": [27, 53, 61, 73, 84, 91], "bitch": 64, "blanket": 86, "bloat": [42, 53, 54, 84, 87], "blob": [24, 31, 54], "blobfil": [52, 54, 55, 56], "block": [37, 42, 47, 53, 54, 55, 56, 64, 65, 67, 68, 86], "block_mask": 31, "blog": [54, 67], "blue": 77, "bmatrix": 8, "bnd": 31, "bni": 31, "bo": [14, 54, 55, 56], "bodhisattva": 86, "bodi": [79, 83, 86], "bog": 86, "boilerpl": [42, 48], "bonu": [14, 18, 19, 21, 23, 24, 37, 42, 48, 51, 59, 60, 70, 73, 74, 88, 90], "book": [1, 3, 4, 5, 8, 10, 12, 14, 15, 16, 18, 20, 22, 24, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 48, 53, 54, 55, 56, 58, 60, 61, 62, 64, 65, 66, 69, 73, 74, 75, 77, 79, 80, 81, 83, 84, 86, 87, 90, 95, 97, 99], "bool": [27, 29, 31, 33, 53, 54, 55, 56, 84], "boolean": [27, 29, 31, 53, 54, 55, 84], "boost": [67, 68], "booth": 86, "borrow": 84, "both": [27, 36, 43, 48, 53, 58, 64, 74, 84, 92, 98, 99], "bottl": 61, "bottom": [31, 48, 99], "bounc": [1, 86], "bound": 31, "boundari": 86, "box": 86, "bpe": [1, 10, 14, 17, 25, 36, 42, 43, 54, 62, 64, 72, 73, 74], "bpe_merg": 24, "bpe_merges_path": [18, 24], "bpe_openai_gpt2": [17, 18], "bpe_rank": 24, "bpetokenizersimpl": [18, 24], "br": 24, "bracket": 84, "brain": 61, "brainstorm": 43, "branch": 86, "break": [14, 15, 16, 24, 27, 29, 42, 55, 56, 60, 64, 73, 79, 83, 84, 86], "breakdown": [14, 37, 80], "breakfast": 77, "breath": 86, "breed": 52, "breeder": 73, "brew": 91, "bridg": 86, "brief": [24, 42, 58, 64, 73, 87], "briefli": [27, 33, 47, 58], "bring": [42, 86], "broadcast": 31, "broken": 60, "brotha": 64, "brows": 99, "browser": [1, 51, 70, 88], "brute": 58, "buddhism": 86, "buddhist": 86, "buf": [53, 54, 55, 56], "buffer": [1, 31, 34, 53, 54, 55, 56, 58], "bug": 0, "bui": 84, "build": [3, 4, 5, 8, 10, 14, 15, 16, 18, 20, 22, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 60, 64, 65, 66, 69, 73, 74, 75, 77, 79, 80, 81, 83, 84, 86, 87, 90, 95, 97], "builder": 97, "built": [53, 58, 91, 97], "builtin": 24, "bullet": 73, "bulstrod": 61, "bundl": 1, "bunni": 86, "buri": 86, "burlington": 14, "busi": 87, "button": [1, 73, 79, 83, 86, 99], "bye": [73, 79, 83, 86], "byeswickattribut": 36, "bypass": 92, "byte": [1, 14, 17, 25, 36, 37, 53, 54, 55, 56, 58, 73, 79, 83, 86], "byte_ari": 24, "bytearrai": 24, "bytepair": 17, "bytepair_encod": 14, "c": [1, 24, 64, 73, 79, 80, 83, 86, 92, 93], "c3316": 39, "c3621": 39, "c3622": 39, "c3623": 39, "c3798": 39, "c3899": 39, "c4091": 39, "c_attn": [42, 45, 46], "c_fc": [42, 45, 46], "c_proj": [42, 45, 46], "ca": 27, "ca_with_buff": 33, "ca_without_buff": 33, "cach": [53, 54, 55], "cache_d": 69, "cache_dir": 46, "cadefault": [79, 83, 86], "cafil": [79, 83, 86], "calc_accuracy_load": [10, 64, 90], "calc_closs_load": 64, "calc_loss_batch": [8, 42, 64, 90], "calc_loss_load": [42, 43, 64, 73, 90], "calcul": [8, 10, 14, 27, 31, 36, 37, 39, 43, 53, 54, 55, 56, 61, 69, 73, 79, 80, 84, 87], "calculate_s": 37, "california": 83, "call": [3, 4, 5, 8, 10, 14, 15, 16, 18, 20, 22, 24, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 48, 52, 53, 54, 55, 56, 58, 60, 64, 66, 69, 73, 74, 75, 77, 79, 80, 81, 83, 84, 86, 87, 91], "calm": 86, "calmli": 86, "calori": 73, "came": 61, "camp": 86, "can": [1, 2, 8, 10, 14, 20, 22, 24, 27, 28, 31, 33, 36, 37, 40, 41, 42, 43, 45, 47, 48, 49, 51, 52, 53, 54, 55, 56, 58, 59, 60, 61, 63, 64, 65, 67, 69, 70, 73, 74, 76, 77, 78, 79, 80, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 97, 99], "can_merg": 24, "cannot": [1, 54, 77, 80, 87], "cannotsendhead": [79, 83, 86], "capabl": [24, 27, 53, 54, 73, 86], "capac": 86, "capath": [79, 83, 86], "capit": [64, 73, 83], "capsiz": 31, "capsul": 86, "captiv": 73, "captur": [37, 42, 53, 73, 79, 80, 83, 86], "car": [73, 74, 84], "care": [14, 42, 74, 84, 86], "carefulli": 86, "carlo": [14, 42, 61], "carri": [20, 27, 31, 39, 42, 73], "carrot": 73, "case": [14, 27, 33, 36, 42, 43, 44, 47, 58, 60, 62, 64, 67, 73, 80, 84, 93, 94, 96, 97], "cash": [64, 66], "casual": 80, "cat": [24, 27, 29, 31, 36, 42, 53, 54, 55, 56, 76, 77, 86], "catbackward0": [27, 28], "caus": [64, 86, 91], "causal": [29, 31, 32, 33, 34, 36, 53, 54, 55, 56, 64, 67], "causalattent": [27, 90], "causalattentionwithbuff": 33, "causalattentionwithoutbuff": 33, "causalselfattent": 29, "cautiou": 86, "caveat": [31, 74], "cc": 73, "cd": [48, 90, 97, 99], "cdot": [36, 53], "celesti": 86, "cell": [3, 4, 5, 8, 10, 14, 15, 16, 18, 20, 22, 24, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 60, 64, 66, 69, 73, 74, 75, 77, 79, 80, 81, 83, 84, 86, 87, 91, 99], "cell_cod": [18, 54], "cell_typ": [18, 54], "cent": [77, 80, 87], "center": [31, 36, 86], "centerdot": 84, "ceremoni": 86, "certain": [27, 42, 61, 73, 83, 91, 93], "cf1dad0dd611": 33, "cfg": [36, 37, 53, 54, 55, 56], "ch": 1, "ch02": [1, 8, 13, 14, 15, 24, 42, 43, 90], "ch03": [1, 26, 29, 31, 36, 90], "ch03_mha": 31, "ch03_mha_wrapp": 31, "ch04": [1, 8, 10, 35, 39, 42, 45, 46, 47, 58, 60, 64, 73, 84, 90], "ch05": [1, 8, 10, 40, 41, 42, 45, 46, 47, 48, 51, 52, 53, 54, 55, 56, 60, 64, 73, 84, 90], "ch06": [1, 10, 63, 66, 69, 70, 90], "ch07": [1, 72, 73, 74, 75, 84, 88, 90], "chain": [79, 83, 86, 87], "chainlit": [51, 70, 88], "chakra": 86, "challeng": [60, 67, 74, 86], "chamber": [73, 79, 83, 86], "chanc": 42, "chang": [3, 14, 27, 28, 29, 31, 33, 42, 43, 53, 54, 61, 64, 65, 73, 76, 80, 84, 86, 87, 90, 97, 99], "chapter": [1, 8, 9, 10, 11, 15, 18, 24, 32, 33, 48, 49, 51, 52, 53, 54, 58, 60, 61, 66, 67, 68, 70, 75, 83, 84, 87, 88, 90, 99], "char": 24, "charact": [14, 24, 42], "chat": [53, 54, 73, 77, 79, 80, 81, 83, 86, 87], "chat_token": [54, 55, 56], "chatbot": 73, "chatformat": [52, 54, 55, 56, 90], "chatgpt": [1, 51, 70, 84, 88], "chatlin": 64, "cheap": [14, 42], "cheaper": 36, "check": [5, 8, 10, 18, 24, 27, 33, 36, 42, 43, 48, 53, 54, 55, 56, 58, 60, 64, 66, 69, 73, 84, 87, 91, 92, 93, 94, 99], "check_if_run": [73, 90], "check_packag": 95, "checkmat": 64, "checkpoint": [10, 42, 43, 46, 48, 60, 64, 72, 73, 74, 84], "cheetah": [74, 84], "chef": [73, 74, 75, 84], "chemistri": 87, "chess": 64, "chicago": [1, 14], "child": [10, 86], "children": 86, "chines": 86, "chip": [8, 10, 42, 64, 73], "chlorid": 87, "chocol": 42, "choic": [52, 67, 73, 77, 80, 83, 87, 99], "choos": [24, 27, 36, 48, 64, 92, 93], "choose_model": [10, 45, 46, 47, 58, 64, 66, 73, 75, 84], "chosen": [61, 67, 83, 84], "chosen_full_text": 84, "chosen_full_token": 84, "chosen_mask": 84, "chosen_respons": 84, "chosen_reward": 84, "chr": 24, "christian": 86, "chunk": [14, 15, 16, 22, 29, 45], "chunksiz": 69, "cite": 1, "ckpt": [10, 42, 43, 60, 64, 72, 73, 74], "cl": [10, 14, 15, 16, 18, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 73, 75, 84], "cl100k_base": 24, "clangmiss": 54, "clariti": 73, "class": [3, 4, 5, 10, 14, 15, 16, 18, 22, 24, 28, 29, 33, 36, 37, 41, 42, 48, 55, 56, 64, 69, 73, 74], "classic": 86, "classif": [1, 10, 68, 69, 73], "classifi": [1, 66, 69, 84], "classify_review": [64, 66, 90], "claud": [14, 81], "claus": 73, "clean": [39, 84, 97], "clean_text": [52, 54, 55, 56, 90], "cleaner": 39, "cleanup": 58, "clear": [1, 33, 54, 58, 73, 80, 86, 87], "clearli": [73, 80], "clever": 79, "cli": [53, 54, 55], "click": [1, 53, 54, 55, 56, 73, 79, 83, 86, 97, 99], "client": [77, 79, 80, 83, 86, 87], "climat": 55, "clip": [42, 48, 49, 62], "clip_grad_norm_": 8, "clone": [1, 46, 48, 53, 54, 55, 56, 84, 90, 97, 99], "close": [8, 10, 36, 42, 64, 73, 74, 84], "closer": [42, 43, 84], "closet": 86, "cloud": [42, 54, 73, 74, 84], "clover": [52, 56], "clue": 86, "clutter": [27, 42, 86], "cm": [79, 80], "co": [8, 42, 45, 47, 52, 53, 54, 55, 56], "coax": 86, "cobra": [79, 80], "code": [1, 3, 4, 5, 8, 9, 10, 11, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 28, 29, 31, 32, 33, 37, 39, 42, 43, 44, 49, 52, 53, 54, 57, 58, 59, 60, 64, 65, 66, 67, 68, 69, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 85, 86, 87, 90, 91, 94, 95, 98], "coher": [8, 10, 42, 64], "colab": [53, 84, 91], "colab_gpu": 84, "colab_tpu_addr": 84, "collaps": 84, "collat": [73, 74, 84], "collate_fn": [73, 74, 84], "colleagu": 61, "collect": [10, 24, 54, 58, 64, 73, 74], "collector": 54, "color": [31, 77, 86], "column": [27, 36, 64], "com": [1, 3, 4, 5, 8, 10, 14, 15, 16, 18, 20, 22, 24, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 48, 53, 54, 55, 56, 58, 60, 64, 65, 66, 69, 73, 74, 75, 77, 79, 80, 81, 83, 84, 86, 87, 90, 91, 93, 94, 95, 97, 99], "combin": [14, 27, 29, 36, 42, 53, 54, 55, 56, 64, 73, 86], "combined_1": 48, "combined_2": 48, "combined_weight": [54, 55, 56], "come": [14, 27, 64, 86, 91], "comfort": 86, "comma": 14, "command": [1, 2, 48, 51, 54, 70, 73, 79, 83, 86, 88, 91, 92, 93, 94, 97, 99], "comment": [54, 55, 58, 69, 84], "commerc": 87, "common": [27, 36, 42, 48, 64, 69, 73, 79, 84, 86, 87], "commonli": [24, 27, 36, 52, 53, 55, 56, 61, 84, 97], "commun": [45, 46, 86, 87], "comp": 64, "compact": [42, 43, 54, 60], "compani": [73, 87], "companion": [55, 56], "compar": [1, 3, 4, 10, 14, 17, 20, 24, 27, 30, 34, 36, 42, 54, 58, 61, 64, 67, 68, 71, 73, 79, 80, 84], "comparison": [24, 39, 54, 58, 73, 74, 79, 80, 81, 93], "compat": [31, 36, 42, 48, 61, 73, 87, 91, 92, 93, 94, 97], "compet": 87, "competit": 86, "compil": [36, 42, 48, 52, 73], "complet": [8, 10, 15, 24, 29, 40, 42, 48, 53, 54, 64, 68, 72, 73, 74, 75, 77, 79, 80, 83, 84, 86, 87, 91, 94, 97, 98], "complex": [8, 14, 27, 36, 39, 84, 86, 87], "complic": [24, 27, 73, 74, 84], "compon": [8, 55, 56], "comprehens": [55, 56, 91, 94], "compress": [24, 52, 55, 56, 69], "comput": [1, 8, 10, 12, 14, 20, 24, 29, 31, 33, 36, 39, 42, 48, 53, 54, 55, 56, 64, 73, 74, 79, 80, 83, 84, 86, 91, 92, 93, 99], "computation": [36, 42], "compute_accuraci": [3, 4], "compute_dpo_loss": 84, "compute_dpo_loss_batch": 84, "compute_dpo_loss_load": 84, "compute_logprob": 84, "compute_rop": [53, 54, 55], "compute_rope_param": 56, "concat": 64, "concaten": [14, 27, 48], "concentr": 87, "concept": [8, 10, 20, 33, 36, 42, 54, 58, 64, 79, 84], "conceptu": [27, 45, 47, 73], "concern": [36, 42, 47], "concis": [14, 27, 73, 74], "conclud": 54, "conclus": 87, "concret": [27, 73, 84], "conda": [42, 92, 93, 94, 97], "condens": 27, "condit": 42, "conduct": [74, 84, 86], "confid": [42, 86], "config": [31, 37, 39, 53, 54, 77, 80, 87, 97], "config_fil": [53, 54, 77, 80, 87], "configur": [36, 37, 42, 43, 50, 54, 55, 56, 60, 67, 84, 91, 97, 98, 99], "confirm": [24, 54, 55, 56, 91], "conflict": 91, "confus": [73, 87], "congrat": 73, "connect": [14, 19, 25, 37, 53, 54, 55, 56, 64, 68, 79, 83, 86, 94, 97, 99], "connectionrefusederror": [79, 83, 86], "conquest": 61, "consequ": 64, "conserv": 84, "consid": [0, 1, 22, 24, 27, 33, 36, 37, 42, 54, 60, 67, 73, 77, 80, 83, 84, 86, 87, 91], "consider": 74, "consist": [1, 10, 14, 22, 24, 27, 42, 52, 55, 56, 64, 73, 74, 79, 83, 84, 86, 87, 97, 99], "consol": 98, "consolid": 53, "constant": 53, "construct": [54, 86, 87], "consult": [73, 91, 94], "consumpt": 39, "contact": 64, "contain": [1, 2, 3, 5, 7, 8, 9, 10, 11, 13, 14, 15, 19, 20, 21, 23, 25, 26, 27, 29, 30, 34, 35, 36, 40, 41, 42, 43, 44, 47, 48, 51, 52, 53, 54, 57, 59, 60, 61, 62, 63, 64, 66, 68, 70, 71, 72, 73, 74, 75, 76, 78, 84, 85, 87, 88, 89, 97], "content": [14, 24, 25, 34, 40, 42, 43, 54, 55, 56, 73, 74, 77, 79, 80, 83, 84, 86, 87], "contest": 86, "context": [8, 10, 20, 22, 27, 31, 36, 39, 42, 43, 45, 46, 47, 52, 53, 54, 55, 56, 58, 60, 61, 64, 66, 67, 73, 74, 75, 79, 80, 83, 84, 86, 87, 97], "context_len": [31, 42, 53, 54, 55, 56], "context_length": [8, 10, 14, 15, 16, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 52, 53, 54, 55, 56, 58, 60, 64, 65, 66, 67, 73, 74, 75, 84], "context_length_new": [54, 55, 56], "context_length_old": [54, 55, 56], "context_s": [10, 14, 27, 36, 42, 43, 45, 46, 47, 52, 53, 54, 55, 56, 64, 73, 74, 75, 84], "context_vec": [27, 28, 29, 31, 33, 53, 54, 55, 56], "context_vec_2": 27, "contigu": [27, 29, 31], "continu": [14, 24, 27, 42, 48, 58, 60, 73, 79, 80, 84, 91], "contract": 64, "contrast": [24, 99], "contribut": [27, 54], "contributor": 61, "control": [10, 84], "conundrum": 86, "conv1d": 46, "convei": 73, "conveni": [27, 33, 42, 54, 58, 65, 73, 74, 84, 90, 93, 94], "convent": [1, 27, 36, 42, 99], "converg": [36, 49], "convers": [42, 54, 73, 77, 86, 87], "convert": [1, 20, 24, 27, 29, 31, 36, 37, 40, 42, 44, 55, 56, 58, 60, 62, 64, 66, 69, 73, 74, 75, 77, 79, 80, 83, 84, 86, 87], "convolut": 64, "cook": [73, 74, 75, 84, 86], "cool": 86, "coordin": 2, "cope": 86, "copi": [1, 3, 37, 39, 42, 43, 51, 53, 54, 55, 56, 58, 60, 70, 73, 74, 84, 88], "copy_": 58, "copyright": 54, "core": [14, 24, 31, 73, 84], "corn": [54, 73], "corner": 97, "corpu": [42, 62], "corrcoef": 81, "correct": [3, 4, 8, 10, 24, 33, 36, 42, 53, 54, 64, 73, 74, 79, 80, 83, 84, 87], "correct_predict": 64, "correctli": [5, 27, 33, 42, 64, 73, 74, 77, 79, 80, 84, 87], "correlation_t": 81, "correspond": [1, 10, 24, 36, 42, 43, 60, 64, 73, 74, 84, 86, 87], "correspondingli": 36, "cosin": [42, 48, 49, 53, 54, 55, 56], "cosmologi": 86, "cost": [10, 42, 53, 54, 77, 80, 87], "costum": 86, "cottonwood": 73, "couch": [76, 77], "could": [10, 14, 24, 42, 48, 53, 64, 66, 73, 74, 75, 80, 84, 86, 87, 91], "couldn": 43, "count": [36, 37, 39, 64, 67, 87], "count_paramet": 28, "counter": 24, "countri": 87, "countvector": 69, "coupl": 43, "cours": [53, 54, 61], "cover": [1, 8, 12, 14, 25, 27, 34, 36, 40, 42, 64, 73, 84], "cozi": 86, "cpp": [73, 79, 83, 86], "cpu": [4, 8, 10, 33, 39, 42, 43, 45, 46, 47, 52, 53, 54, 55, 56, 61, 64, 66, 67, 72, 73, 74, 75, 84, 91, 94, 99], "cpu_feature_guard": 73, "craft": 86, "crash": 24, "crazi": 64, "creat": [1, 3, 8, 10, 15, 16, 18, 22, 24, 27, 29, 31, 35, 36, 42, 43, 44, 47, 53, 54, 55, 56, 58, 60, 63, 66, 68, 69, 72, 79, 80, 82, 83, 85, 86, 92, 93, 97, 98], "create_balanced_dataset": [10, 64, 90], "create_block_mask": 31, "create_connect": [79, 83, 86], "create_dataload": [16, 29], "create_dataloader_v1": [8, 14, 15, 22, 42, 43, 90], "creation": [27, 86, 98], "creativ": [43, 86], "credenti": [53, 54, 55], "crepuscular": [79, 80], "crfm": 73, "criteria": [73, 80, 83, 87], "critic": [73, 80, 87], "crocodil": 74, "croft": 14, "crop": 36, "cross": [64, 73, 84], "cross_entropi": [3, 4, 42, 64, 67, 73, 84], "cross_entropy_loss": 84, "crown": 86, "crucial": [8, 27, 39, 42, 54, 80, 87], "crystal": 86, "csv": [10, 64, 65, 68, 69], "csv_file": 64, "ctrl": 97, "ctx\u59cb": 53, "cu121": 74, "cu124": [31, 61], "cubla": 73, "cuda": [3, 4, 5, 8, 10, 31, 33, 39, 42, 43, 45, 46, 47, 52, 53, 54, 55, 56, 58, 61, 64, 66, 73, 74, 84, 97, 98], "cuda_bla": 73, "cuda_dnn": 73, "cuda_fft": 73, "cuda_visible_dev": 2, "cudnn": 73, "cue": 86, "cufft": 73, "cultur": 86, "cumbersom": 93, "cumul": 36, "cumulonimbu": [73, 74, 84], "cumulu": [73, 74, 84], "cup": 64, "cupcak": 86, "curios": 86, "curiou": [61, 74, 86], "curl": [92, 93], "current": [1, 8, 24, 27, 31, 36, 42, 43, 64, 84, 87, 91, 97], "current_dir": [18, 54], "current_max": 84, "current_vers": 31, "current_weight": [54, 55, 56], "curtain": 42, "curv": [8, 48, 73, 84], "custom": [31, 54, 60, 73, 84, 86, 98], "custom_collate_draft_1": 73, "custom_collate_draft_2": 73, "custom_collate_fn": [73, 74, 84, 90], "custom_token": 60, "custom_token_id": 60, "customized_collate_fn": [73, 84], "cut": 24, "cycl": [8, 73, 79, 83, 86], "cyclic": 86, "d": [1, 27, 36, 42, 43, 46, 48, 49, 50, 54, 55, 56, 64, 73, 77, 79, 83, 86, 90, 95], "d_in": [27, 28, 29, 31, 33, 36, 37, 53, 54, 55, 56], "d_k": 27, "d_out": [27, 28, 29, 31, 33, 36, 37, 53, 54, 55, 56], "daemon": 97, "dai": [8, 36, 42, 43, 64, 73, 74, 75, 79, 80, 83, 84, 86, 87, 91], "daili": [73, 83], "danc": 84, "dandelion": [52, 56], "dark": [31, 86], "dat": 64, "data": [1, 4, 8, 10, 16, 24, 26, 33, 36, 43, 45, 47, 48, 58, 60, 72, 74, 79, 80, 82, 83, 86], "data_dir": 48, "data_file_path": [10, 64], "data_it": [14, 22], "data_load": [42, 64, 84], "data_smal": 48, "data_to_process": 87, "data_typ": 39, "datafram": [64, 81], "dataload": [1, 3, 4, 10, 13, 14, 15, 16, 21, 22, 29, 64, 73, 74, 84], "dataset": [1, 3, 4, 8, 14, 15, 16, 22, 24, 29, 42, 43, 53, 54, 67, 69, 71, 76, 79, 80, 82, 89, 99], "dataset_s": 86, "date": [43, 53, 54, 64, 72, 74, 86, 94], "date_format": 69, "date_pars": 69, "dayfirst": 69, "ddp": [1, 2, 3, 4, 48, 61], "de": [53, 86], "deactiv": 91, "deadli": 86, "deal": [14, 33, 64, 86], "debat": 86, "debian": 91, "debug": 97, "decai": [48, 49, 54], "decid": [36, 83, 97, 99], "decim": [27, 42, 69, 87], "declar": 92, "decod": [8, 14, 16, 18, 27, 31, 36, 53, 54, 55, 56, 60, 68, 73, 74, 79, 83, 84, 86], "decode_piec": 53, "decode_tokens_from_batch": 84, "decoded_str": 24, "decoded_text": [36, 42], "decompress": [24, 54], "decreas": [8, 42, 73, 76], "dedic": 99, "deep": [8, 27, 36, 42, 43, 64, 66, 73, 75, 84, 86], "deeper": [55, 56], "def": [3, 4, 5, 8, 10, 14, 15, 16, 18, 22, 24, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 69, 73, 74, 75, 77, 79, 80, 83, 84, 86, 87], "default": [24, 31, 39, 45, 47, 48, 53, 54, 55, 56, 64, 67, 73, 76, 91, 92, 98], "defaultkeepstorag": 97, "defens": 86, "defin": [8, 10, 14, 15, 16, 18, 20, 24, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 48, 52, 53, 55, 56, 58, 60, 64, 65, 66, 73, 75, 84, 87], "definit": [18, 54, 58], "degre": [42, 43, 64], "deiti": 86, "del": [39, 53, 54, 55, 56, 58, 79, 83, 86], "delai": 87, "delet": [53, 58, 93], "deletingpretcc\u5426": 53, "delight": 86, "delim_whitespac": 69, "delimit": 69, "delta": 10, "demand": 42, "demeanor": 86, "demo": 87, "demonstr": [2, 20, 27, 33, 61], "denomin": 36, "denot": [14, 53, 84], "depend": [14, 36, 42, 48, 52, 54, 55, 56, 64, 73, 91, 97, 99], "depict": [10, 27, 36, 64], "depth": [1, 87], "dequ": 24, "deriv": [27, 87], "desc": [73, 79, 80, 83], "descend": 74, "describ": [1, 14, 24, 27, 42, 45, 47, 61, 72, 73, 74, 75, 79, 80, 83, 84, 86, 91, 92, 93, 94, 97, 99], "descript": 73, "design": [1, 8, 20, 27, 45, 47, 50, 67, 69, 86, 92, 99], "desir": [14, 20, 24, 27, 29, 31, 42, 43, 53, 84], "desired_respons": [73, 84], "desktop": [91, 97, 99], "despit": 80, "destabil": 8, "destroi": 14, "detach": [45, 46, 53, 54, 55, 56, 84], "detail": [8, 10, 24, 36, 42, 53, 54, 58, 60, 61, 64, 67, 72, 73, 84, 86, 87], "detect": 97, "determin": [10, 20, 27, 36, 42, 54, 73, 84], "determinist": [73, 79, 80, 83, 86], "detract": 73, "dev": [5, 18, 31, 90, 91, 93, 97], "devcontain": 96, "develop": [1, 12, 14, 18, 27, 42, 48, 73, 87, 90, 97, 99], "deviat": [1, 36, 80], "devic": [8, 10, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 52, 53, 54, 55, 56, 58, 64, 66, 72, 73, 74, 75, 84], "device_nam": 39, "devonshir": 14, "df": [10, 64], "dhatu": 86, "di": 31, "diagon": [27, 29, 31, 33, 53, 54, 55, 56], "diagram": 1, "dialect": 69, "dialogu": 69, "dict": [24, 44, 45, 57], "dict_kei": [42, 84], "dictionari": [24, 39, 42, 43, 50, 60, 73, 79, 83, 86], "did": [64, 73, 79, 80, 84], "didn": [36, 64], "diet": [52, 54, 55, 56, 73], "differ": [1, 8, 10, 14, 24, 25, 30, 31, 34, 36, 42, 53, 54, 61, 67, 73, 79, 80, 81, 83, 84, 86, 87, 93, 99], "differencespadxsnu": 53, "differenti": 64, "difficult": [86, 87], "diffus": 86, "digest": [73, 79, 83, 86], "digit": [22, 27, 42, 64], "dii": 86, "dim": [3, 4, 27, 28, 29, 31, 33, 36, 42, 43, 53, 54, 55, 56, 64, 66, 84], "dimens": [8, 10, 14, 20, 27, 28, 29, 31, 36, 42, 43, 53, 54, 55, 56, 60, 61, 64, 66, 73], "dimension": [14, 27, 36, 42, 54, 64], "dinner": [64, 66], "direct": [1, 73, 80, 82, 89], "directli": [20, 22, 36, 43, 44, 45, 46, 47, 53, 54, 55, 58, 73, 80, 84, 86, 87, 91, 93], "directori": [1, 5, 6, 24, 48, 58, 69, 84, 91, 92, 93, 94, 97, 98, 99], "disabl": [8, 24, 27, 31, 36, 42, 64, 67], "disable_causal_mask": 67, "disallow": 24, "disallowed_speci": [54, 55, 56], "discard": 58, "disciplin": 87, "discov": 80, "discoveri": 80, "discrep": [36, 42, 67], "discuss": [0, 1, 8, 14, 24, 36, 42, 54, 60, 64, 67, 73, 84, 87, 91, 92, 93, 94, 99], "disengag": 86, "disk": [48, 58], "displeas": 74, "dispref": [83, 84], "disrupt": 27, "dissolv": 87, "dist": [10, 14, 15, 16, 18, 27, 29, 33, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 73, 75, 84], "distanc": 87, "distil": [68, 74], "distinct": 54, "distinguish": 10, "distribut": [2, 10, 14, 15, 16, 18, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 48, 53, 54, 55, 56, 58, 61, 64, 66, 73, 75, 84], "distribution_nam": [10, 14, 15, 16, 18, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 73, 75, 84], "diurnal": 80, "divbackward0": [27, 36], "diverg": 84, "divers": [42, 43, 74], "divid": [8, 27, 36, 42, 54, 64, 73, 84], "divin": 86, "divis": [27, 29, 31, 36, 39, 53, 54, 55, 56], "do": [8, 14, 24, 27, 33, 36, 42, 43, 45, 46, 47, 52, 53, 54, 55, 56, 58, 60, 64, 73, 79, 83, 84, 86, 87, 91, 93], "do_open": [79, 83, 86], "doc": 1, "docker": [1, 48], "doctor": 77, "document": [5, 31, 91, 92, 93, 94, 97, 99], "doe": [1, 8, 10, 14, 24, 27, 31, 36, 42, 48, 51, 53, 58, 64, 67, 69, 70, 74, 79, 80, 87, 88, 93, 97, 99], "doesn": [10, 14, 27, 31, 48, 53, 79, 83, 86, 93, 97], "dollar": [42, 53, 54], "domain": [14, 42, 48, 61], "don": [1, 10, 14, 27, 33, 36, 42, 43, 54, 60, 64, 67, 73, 77, 80, 84, 86, 87, 97, 99], "done": [24, 27, 53, 58, 86], "donkei": [8, 42, 43], "door": 86, "dosag": 87, "dot": [27, 29, 53, 54, 55, 56], "dotal": 87, "doubl": [10, 24, 33, 36, 64, 73, 94], "doublequot": 69, "down": [8, 14, 42, 43, 60, 68, 84, 86], "down_proj": [54, 55, 56], "download": [1, 10, 24, 31, 41, 42, 45, 53, 54, 55, 56, 63, 64, 69, 72, 73, 78, 93, 98, 99], "download_and_load_fil": [73, 84, 90], "download_and_load_gpt2": [10, 42, 43, 60, 64, 73, 90], "download_and_unzip_spam_data": [10, 64, 90], "download_file_if_abs": 24, "download_prepare_dataset": 68, "download_vocab": 18, "downsid": 24, "downward": [64, 84], "dpo": [1, 73, 82, 89], "dq": 24, "drag": 99, "dramat": 73, "draw": 86, "drawn": 86, "drive": [84, 87], "drivel": 69, "drop": [27, 36, 46, 64], "drop_emb": [10, 36, 37, 53, 64], "drop_last": [3, 4, 8, 10, 14, 15, 22, 42, 43, 64, 73, 84], "drop_rat": [8, 10, 36, 37, 39, 42, 43, 45, 46, 47, 53, 58, 60, 64, 66, 73, 75, 84], "drop_rate_attn": 37, "drop_rate_emb": 37, "drop_rate_shortcut": 37, "drop_resid": [10, 64], "drop_shortcut": [36, 37, 53], "dropout": [8, 10, 29, 31, 33, 36, 39, 42, 43, 45, 46, 47, 53, 58, 60, 64, 66, 73, 75, 84], "dropout_p": 31, "dropout_r": 27, "dry": [52, 55, 56], "dtype": [3, 39, 53, 54, 55, 56, 64, 69], "dtype_backend": 69, "dubarri": 14, "due": [10, 14, 24, 27, 36, 42, 43, 64, 67, 73, 74, 80, 84, 86, 87, 91, 93], "dummi": [39, 53, 54, 68, 69], "dummy_clf": 69, "dummyclassifi": 69, "dummygptmodel": 36, "dummylayernorm": 36, "dummytransformerblock": 36, "dump": [24, 73, 77, 79, 80, 83, 86, 87], "dun": 64, "duplic": [1, 27], "durat": [67, 86], "dure": [8, 10, 14, 27, 33, 36, 42, 45, 47, 54, 60, 64, 79, 80, 83, 84, 86, 90, 93, 97], "dust": 86, "duti": [61, 86], "dwarf": 76, "dynam": 74, "e": [1, 8, 14, 24, 27, 33, 36, 39, 42, 43, 45, 47, 53, 54, 58, 60, 64, 67, 71, 73, 74, 84, 86, 87, 90, 98], "e0000": 73, "each": [1, 5, 8, 10, 14, 18, 20, 24, 27, 29, 31, 36, 42, 43, 48, 53, 54, 55, 56, 58, 60, 61, 64, 73, 74, 79, 80, 83, 84, 86, 91], "eagl": [79, 80], "earli": [36, 42, 55, 56, 64, 80], "earlier": [10, 24, 27, 28, 31, 36, 42, 53, 54, 64, 73, 84, 87], "earth": 86, "earthli": 86, "eas": [36, 87], "easi": [52, 73], "easier": [33, 36, 42, 48, 64], "easiest": [61, 84, 97], "easili": [43, 84, 99], "eat": [52, 53, 54, 55, 56, 73, 79, 83, 86], "eaten": 77, "ebook": 48, "ecolor": 31, "ed": 24, "edgecolor": 31, "edit": [74, 76, 84, 87, 97], "edith": 14, "editor": [1, 97], "edu": [10, 64, 73], "educ": [1, 8, 24, 42, 43, 48, 61, 64, 73, 86], "eer": 53, "effect": [24, 27, 36, 42, 52, 58, 61, 67, 73, 84, 86, 87, 97, 99], "effici": [1, 14, 20, 27, 39, 42, 48, 53, 54, 56, 62, 64, 67, 68, 73, 79, 83, 84, 86, 87, 91], "effiic": 83, "effort": [8, 10, 36, 42, 43, 45, 46, 47, 53, 54, 61, 64, 74, 86], "effort_dead": 54, "einstein": 31, "either": [64, 67, 73, 79, 83, 84, 86, 91], "elabor": [61, 73], "elaps": 48, "elapsed_tim": 31, "elbow": 42, "element": [20, 24, 27, 36, 53, 54, 55, 56, 80, 86], "element_s": [53, 54, 55, 56], "elementwise_affin": 46, "eleph": 74, "elev": 80, "elif": [8, 10, 24, 37, 42, 52, 53, 54, 55, 56, 64, 69, 73, 84], "elimin": [97, 99], "els": [4, 8, 10, 14, 15, 16, 18, 24, 27, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 52, 53, 54, 55, 56, 58, 64, 66, 69, 73, 74, 75, 79, 80, 83, 84, 86, 87], "em": 24, "emb": [14, 54], "emb_dim": [8, 10, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 60, 64, 66, 73, 75, 84], "emb_siz": [36, 37, 53, 54, 55, 56, 60], "embed": [1, 8, 10, 15, 16, 19, 24, 25, 27, 29, 31, 36, 37, 42, 43, 46, 53, 54, 55, 56, 64, 67, 73], "embed_dim": [31, 53, 54], "embed_token": [54, 55, 56], "embedding_lay": 14, "embedding_vs_matmul": 14, "embeddingbackward0": [14, 20], "embrac": [24, 86], "emerg": 86, "emot": [79, 80, 86], "emotion": 86, "empathi": 86, "emperor": 14, "emphasi": 79, "emploi": [67, 73, 86], "empti": [10, 14, 27, 73, 86], "empty_cach": [39, 54, 58], "en": 18, "enabl": [14, 24, 27, 31, 36, 42, 45, 47, 53, 58, 67, 73, 97, 98], "enable_tim": 31, "enc_sampl": 14, "enc_text": 14, "encod": [1, 8, 10, 15, 16, 17, 19, 20, 22, 25, 27, 29, 36, 42, 43, 53, 54, 55, 56, 60, 64, 66, 68, 69, 72, 73, 74, 79, 83, 84, 86], "encode_as_id": 53, "encode_chunk": [79, 83, 86], "encode_head": [54, 55, 56], "encoded_length": 64, "encoded_tensor": [36, 42, 55, 56], "encoded_text": [16, 29, 64, 73, 74, 84], "encoding_error": 69, "encount": [14, 24, 42, 55, 56, 74, 91, 92, 93, 97], "encourag": [84, 86], "encrypt": 98, "end": [8, 14, 24, 27, 31, 39, 42, 55, 56, 61, 64, 73, 79, 80, 83, 86, 87], "end_header_id": [54, 55, 56, 86], "end_of_text": [54, 55, 56], "end_tim": [8, 10, 39, 42, 64, 73, 84], "endhead": [79, 83, 86], "endless": 86, "endoftext": [14, 15, 16, 18, 22, 24, 29, 42, 60, 64, 67, 73, 74, 84], "endswith": [24, 87], "energi": [39, 86], "engag": [1, 27, 86], "engin": [27, 69, 97], "english": [48, 79, 87], "enhanc": [27, 53, 84, 87, 99], "enjoi": [54, 73, 74, 86], "enlighten": 86, "enough": [14, 42, 45, 46, 47, 58, 83, 86], "ensur": [1, 8, 10, 24, 27, 31, 33, 36, 42, 53, 54, 61, 64, 73, 77, 86, 87, 91, 92, 93, 97, 99], "ensure_ascii": 24, "ent": 24, "entangl": 24, "enter": [24, 27, 91], "entertain": 24, "entic": 86, "entir": [14, 15, 16, 27, 29, 58, 61, 64, 65, 67, 73, 77, 86, 97], "entiti": 24, "entranc": [24, 86], "entri": [1, 14, 24, 36, 42, 43, 55, 56, 60, 64, 72, 73, 74, 84, 86, 91], "entropi": [64, 73, 84], "enumer": [3, 4, 14, 24, 27, 36, 42, 43, 60, 64, 73, 74, 77, 83, 84], "envelop": 42, "environ": [1, 5, 12, 33, 43, 48, 73, 84, 92, 94, 98, 99], "eo": [14, 54, 55, 56], "eof": 24, "eos_id": [42, 55, 56, 73, 74, 75, 84], "eot_id": [54, 55, 56], "ep": [8, 10, 36, 42, 43, 46, 48, 53, 54, 55, 56, 61, 64, 68, 72, 73, 74, 84], "epoch": [3, 4, 8, 42, 43, 48, 61, 64, 73, 84], "epochs_seen": [42, 64, 84], "epochs_tensor": [8, 10, 42, 64, 73, 84], "epsilon": 53, "eq": 84, "equal": [14, 39, 42, 54, 55, 56, 64, 73, 80, 84, 87], "equat": [54, 84], "equip": 33, "equival": [14, 19, 20, 25, 27, 54, 65, 73, 83, 84], "err": [79, 83, 86], "errno": [24, 69, 79, 83, 86], "erron": 87, "error": [10, 14, 18, 24, 27, 33, 36, 42, 43, 53, 60, 64, 69, 73, 74, 79, 80, 83, 86, 87, 97], "error_kw": 31, "escal": 86, "escap": 24, "escapechar": 69, "esoter": 86, "especi": [10, 54, 99], "esplanad": 64, "essenc": [33, 84], "essenti": [10, 14, 20, 27, 53, 54, 73], "establish": [14, 86], "estim": [36, 39, 43, 64], "et": [10, 36, 81], "eta": 48, "etc": [14, 24, 31, 71, 80], "euclidean": 8, "europ": 87, "eval": [3, 4, 8, 10, 36, 42, 43, 46, 47, 58, 60, 64, 66, 69, 73, 75, 78, 79, 80, 84], "eval_freq": [8, 10, 42, 43, 64, 73, 84], "eval_it": [8, 10, 42, 43, 64, 73, 84], "evalu": [1, 3, 4, 8, 10, 43, 64, 68, 72, 74, 81, 83, 84, 87, 89], "evaluate_dpo_loss_load": 84, "evaluate_model": [8, 42, 64, 90], "even": [14, 24, 36, 42, 53, 54, 55, 56, 60, 67, 73, 79, 80, 86, 91, 93, 99], "event": 31, "eventu": 58, "everi": [8, 10, 27, 36, 42, 43, 45, 46, 47, 53, 54, 61, 64, 73, 74, 75, 83, 84, 86, 87, 91, 99], "everyon": 1, "everyth": [33, 77, 91], "everywher": 65, "evoc": 73, "ex": [67, 91], "exact": [36, 43], "exactli": [3, 8, 20, 53, 64, 87], "exampl": [1, 8, 10, 14, 20, 27, 36, 42, 43, 48, 52, 53, 54, 55, 56, 58, 60, 64, 67, 73, 74, 76, 77, 79, 80, 83, 84, 86, 87, 91, 92, 93, 94, 97, 99], "example_batch": [53, 54], "example_data": 84, "example_dataload": 84, "example_dataset": 84, "exampledeepneuralnetwork": 36, "examples_seen": [10, 64], "examples_seen_tensor": [10, 64], "exce": [8, 27, 36, 42, 64, 80, 84], "excecut": [70, 88], "exceed": 8, "excel": 86, "except": [10, 24, 31, 36, 39, 42, 53, 54, 64, 73, 74, 79, 80, 83, 86], "excess": 73, "excit": [79, 86], "exclam": [79, 80], "exclamatori": [79, 80], "exclud": [33, 67, 74, 84], "exec": [18, 54], "execut": [1, 8, 14, 18, 24, 27, 31, 39, 42, 43, 45, 47, 51, 54, 58, 70, 73, 79, 83, 84, 86, 88, 91, 92, 93, 94, 97, 98, 99], "execution_mean": 31, "execution_stat": 31, "execution_std": 31, "execution_time_minut": [8, 10, 42, 64, 73, 84], "executionpolici": 92, "exercis": [1, 2, 14, 25, 27, 36, 41, 42, 63, 64, 72, 73, 89], "exercise_experi": [72, 74], "exercise_solut": 74, "exert": 74, "exist": [8, 10, 14, 18, 24, 42, 43, 45, 47, 48, 52, 53, 54, 58, 60, 64, 66, 72, 73, 74, 75, 84, 86, 87], "exist_ok": 58, "exp": [27, 42], "expand": [31, 53, 54, 55, 56, 61], "expect": [4, 10, 31, 33, 42, 43, 54, 55, 56, 60, 64, 67, 73, 83, 84, 86, 87], "expens": [8, 42, 73], "experi": [1, 43, 61, 64, 65, 71, 73, 74, 80, 86, 87, 94, 99], "experienc": 73, "experiment": [87, 97], "explain": [1, 19, 21, 23, 24, 25, 27, 32, 33, 34, 42, 55, 56, 58, 59, 60, 61, 64, 73, 84, 87, 97], "explan": [33, 39, 43, 53, 54, 55, 56, 73, 77, 87, 91, 94], "explanatori": 84, "explicit": [31, 64], "explicitli": [36, 43, 73, 79, 83, 86], "explod": 8, "explor": [50, 86], "exponenti": 42, "express": [14, 79, 80, 84, 86], "exquisit": [42, 43], "extend": [1, 14, 24, 50, 54, 55, 56, 71, 74, 77, 84], "extended_token": 60, "extens": [64, 97], "extension\u00e9gor": 54, "extent": 10, "extermin": 42, "extern": 73, "extra": [18, 38, 39, 51, 54, 55, 56, 68, 70, 73, 76, 77, 80, 87, 88], "extract": [14, 64, 74, 87], "extract_in": 87, "extract_instruct": [86, 87], "extract_oup": 87, "extract_respons": [75, 84, 87], "extractal": 64, "extracted_path": [10, 64], "extrem": 27, "ey": 86, "eyesight": 86, "f": [3, 4, 8, 10, 14, 15, 16, 18, 22, 24, 29, 31, 36, 37, 39, 42, 43, 45, 46, 47, 52, 53, 54, 55, 56, 58, 60, 64, 66, 69, 73, 74, 75, 77, 79, 80, 83, 84, 86, 87], "f001": [10, 64], "f1cd752815fc": 83, "fa": 64, "face": [24, 42, 44, 47, 53, 54, 55, 56, 86], "facecolor": 31, "facilit": [84, 87], "fact": [10, 42, 53, 61, 64, 87], "factor": [8, 10, 27, 52, 54, 55, 56, 87], "factori": 73, "fail": [10, 24, 64, 73, 79, 80, 95], "fair": [61, 73], "fall": [91, 92, 93], "fallback": [24, 94], "fallback_token": 24, "fals": [3, 4, 8, 10, 14, 20, 22, 24, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 46, 53, 54, 55, 56, 58, 60, 64, 65, 69, 73, 74, 84, 97], "false_valu": 69, "famili": [54, 86], "familiar": [14, 53, 64, 84, 86, 91], "fan_in": 31, "fanci": 42, "far": [10, 33, 35, 42, 54], "fashion": [10, 86, 87], "fast": [36, 42, 74, 83, 84, 92], "faster": [1, 8, 10, 14, 36, 42, 64, 67, 68, 73, 74, 90, 91, 93], "fastest": 61, "favor": 86, "favorit": [73, 86], "fc1": [53, 54, 55, 56], "fc2": [53, 54, 55, 56], "fc3": [53, 54, 55, 56], "fd": 24, "fe": 24, "fear": 86, "feasibl": [27, 86], "featur": [3, 4, 8, 27, 36, 48, 60, 61, 91, 94], "feature_extract": 69, "featureiman": 36, "fed": [54, 55, 64], "feed": [10, 42, 52, 53, 54, 55, 56, 60, 64, 73, 79, 83, 84, 86], "feed_forward": 53, "feedback": [73, 81, 84], "feedforward": [10, 36, 37, 54, 55, 56, 64, 90], "feel": [24, 42, 54, 55, 86, 91, 92, 93, 94], "felin": 86, "fellow": [14, 42], "felt": [8, 42], "feminin": 86, "fetch": [18, 54, 55], "few": [8, 42, 53, 54, 64, 73, 74, 84, 86, 97], "fewer": [48, 67, 74], "ff": [10, 36, 37, 42, 45, 46, 53, 54, 55, 56, 64], "ffn": 36, "ffn_norm": 53, "fiber": [55, 73, 79, 83, 86], "field": [74, 86, 87, 99], "fig": [31, 42, 64], "fight": 86, "figsiz": [8, 36, 42, 64], "figur": [8, 10, 14, 27, 30, 31, 36, 42, 54, 73, 74, 84, 99], "file": [1, 8, 10, 14, 15, 16, 18, 22, 24, 27, 29, 35, 36, 37, 39, 41, 42, 43, 45, 46, 48, 51, 52, 53, 54, 55, 56, 58, 61, 63, 64, 66, 69, 70, 72, 73, 74, 75, 77, 78, 79, 80, 81, 83, 84, 86, 87, 88, 91, 92, 93, 94, 97, 98, 99], "file_nam": [42, 47, 73], "file_path": [8, 14, 24, 42, 43, 72, 73, 74, 84], "filenam": [24, 31, 53, 54, 55, 56, 69], "filenotfounderror": [18, 24, 54, 69], "filepath_or_buff": 69, "files_to_download": 24, "fill": [27, 29, 31, 53, 54, 55, 56, 73, 86], "film": [69, 86], "filter": [14, 42, 55, 56, 84, 86], "fina": 64, "final": [8, 10, 22, 24, 36, 42, 58, 64, 65, 74, 77, 79, 83, 84, 86, 87, 91, 93], "final_norm": [10, 36, 37, 42, 45, 46, 53, 54, 55, 56, 64], "financi": 87, "find": [1, 8, 14, 24, 27, 36, 37, 42, 43, 49, 50, 54, 55, 56, 60, 64, 66, 73, 74, 75, 80, 83, 84, 86, 87, 92, 93, 97, 99], "find_freq_pair": 24, "find_highest_gradi": [8, 90], "findal": 87, "findit": 24, "fine": [33, 53, 54, 55], "finetun": [1, 8, 42, 58, 60, 68, 70, 79, 80, 81, 83, 86, 87, 99], "finetuned_model_path": [66, 75, 84], "finish": [42, 43, 77, 79, 80, 97], "firm": 86, "first": [8, 10, 14, 20, 24, 27, 31, 36, 42, 43, 51, 52, 53, 54, 55, 56, 58, 60, 61, 64, 67, 70, 71, 73, 74, 77, 79, 80, 83, 84, 86, 87, 88], "first_batch": [14, 22], "first_head": 27, "first_r": 27, "fit": [27, 36, 49, 69, 79, 80, 86], "fit_transform": 69, "fix": 60, "flag": [73, 94], "flash": 48, "flashlight": 86, "flask": 81, "flat": [42, 55, 56, 69, 83, 84], "flatten": [8, 42, 84], "fledg": 97, "flex_attent": 31, "flexibl": [31, 67], "float": [20, 24, 38, 39, 42, 53, 54, 55, 56, 61, 64, 73, 84], "float16": 39, "float32": [3, 36, 37, 39, 53, 54, 55, 56], "float_precis": 69, "floatvec": 3, "flop": [1, 38], "flops_backward": 39, "flops_forward": 39, "flops_per_second": 39, "flops_per_second_dict": 39, "flops_per_token": 39, "florenc": 14, "flow": [36, 84], "fly": 10, "fma": 73, "fn": 31, "focu": [14, 36, 42, 53, 54, 55, 56, 58, 60], "focus": [24, 48, 55, 56, 58, 72, 84, 86, 93], "folder": [1, 42, 44, 48, 51, 52, 57, 61, 65, 68, 70, 73, 76, 77, 78, 80, 84, 85, 87, 88, 91, 93, 94, 97, 99], "follow": [1, 8, 10, 12, 14, 18, 20, 24, 27, 28, 31, 33, 36, 42, 43, 48, 51, 52, 53, 54, 55, 56, 60, 61, 64, 65, 67, 68, 70, 74, 77, 79, 80, 83, 84, 86, 87, 88, 91, 92, 93, 94, 97, 99], "food": [54, 56, 73, 86], "foodi": 86, "footprint": 61, "forc": [43, 58, 86], "forget": 43, "form": [14, 24, 42, 48, 52, 55, 56, 73, 77, 87], "formal": [76, 79, 80], "format": [24, 42, 48, 53, 73, 74, 78, 79, 80, 83, 84, 86, 87], "format_input": [73, 74, 79, 80, 83, 84, 90], "format_input_phi": 74, "former": 61, "formula": [10, 27, 36, 53], "forth": [24, 27, 43, 67, 84], "fortun": [42, 54], "forum": [0, 1, 91, 92, 93, 94, 99], "forwar": 27, "forward": [3, 4, 5, 10, 27, 28, 29, 33, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 64, 86], "forward_backward": 31, "forward_cal": 33, "found": [4, 10, 14, 15, 16, 18, 24, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 48, 53, 54, 55, 56, 58, 64, 66, 73, 75, 76, 77, 80, 84, 86, 87, 91, 99], "foundat": [1, 53, 54], "four": [14, 73, 79, 83, 86], "fp16": 39, "fp32": 39, "fr": 64, "frac": [8, 36, 39, 53, 54, 64, 84], "fraction": 64, "frame": [8, 86], "framework": [10, 14, 15, 16, 18, 24, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 69, 73, 75, 77, 79, 80, 83, 84, 86, 87], "free": [1, 24, 42, 48, 53, 54, 55, 56, 58, 64, 77, 80, 87, 91, 92, 93, 94], "freeli": 64, "freez": [10, 64], "freind": [83, 84, 87], "freq": [42, 43], "freq_config": [54, 55, 56], "frequenc": [43, 53, 54, 55, 56], "frequent": [24, 64, 69], "fresh": [73, 86], "friend": [83, 84, 86, 87], "friendli": [84, 86, 91], "from": [3, 4, 5, 8, 10, 14, 15, 16, 20, 22, 25, 27, 28, 33, 37, 39, 41, 43, 44, 48, 51, 58, 60, 62, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 86, 87, 88, 91, 92, 93, 94, 95, 97, 99], "from_nam": [10, 14, 15, 16, 18, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 73, 75, 84], "from_numpi": 3, "from_pretrain": [18, 46], "frozen": 67, "fruit": [54, 73], "fssl": 92, "fulfil": 80, "full": [10, 67, 68, 91, 93, 97], "full_lik": 42, "full_text": [73, 74], "full_url": [79, 83, 86], "fulli": [14, 19, 25, 27, 73, 79, 80, 83, 97], "fullnam": [18, 54], "fullurl": [79, 83, 86], "fun": [84, 86], "func": [31, 58, 79, 83, 86], "function": [1, 3, 4, 10, 14, 18, 20, 24, 27, 31, 36, 41, 48, 49, 50, 53, 54, 55, 56, 58, 60, 61, 62, 63, 64, 66, 67, 72, 73, 74, 76, 79, 80, 83, 86, 87, 99], "functool": [24, 73, 84], "fund": [77, 80, 87], "fundament": [79, 87], "further": [1, 24, 27, 31, 48, 64, 67, 73, 74, 84, 86, 91], "furthermor": [27, 61, 64, 73, 74], "futur": [53, 61, 73, 84, 86], "g": [8, 10, 14, 24, 33, 36, 42, 43, 45, 47, 53, 54, 58, 60, 64, 71, 73, 84, 86, 98], "g4dn": 98, "gage": 24, "gain": [24, 55, 56, 73, 91], "galleri": 14, "galor": 48, "galoreadamw": 48, "game": [77, 86], "gamma": 53, "gamma_i": 53, "gap": [64, 67], "garbag": 54, "garland": 8, "gate": [36, 53, 68], "gate_proj": [54, 55, 56], "gather": 84, "gaussian": 36, "gave": 61, "gb": [39, 48, 52, 53, 54, 55, 56, 58, 61, 73, 79, 83, 86], "gc": [54, 58, 97], "gees": 77, "geforc": 39, "gelu": [10, 37, 61, 64, 90], "gener": [1, 8, 10, 14, 43, 45, 46, 51, 53, 54, 58, 60, 61, 64, 70, 72, 73, 74, 75, 77, 78, 79, 80, 84, 87, 88, 89, 90], "generalist": 64, "generate_and_print_sampl": [8, 42, 84, 90], "generate_and_print_sample": 8, "generate_model_respons": 83, "generate_model_scor": [73, 79, 80, 90], "generate_simpl": 42, "generate_text": 36, "generate_text_simpl": [10, 36, 42, 43, 64, 73, 84, 90], "generated_text": [73, 74, 84], "geniu": [14, 42], "gentl": 86, "gentli": 86, "geometri": 79, "get": [10, 14, 15, 16, 18, 24, 27, 29, 33, 36, 37, 39, 42, 43, 45, 46, 47, 48, 53, 54, 55, 56, 58, 64, 66, 69, 73, 75, 79, 83, 84, 86, 97, 99], "get_buff": [54, 55], "get_config": 37, "get_data": 48, "get_device_nam": 39, "get_encod": [8, 10, 14, 15, 16, 18, 22, 24, 29, 36, 42, 43, 45, 46, 47, 60, 64, 66, 73, 74, 75, 84], "get_gpu_model": 39, "get_handl": 69, "get_height": 31, "get_method": [79, 83, 86], "get_requirements_dict": 95, "get_special_token_id": 24, "get_width": 31, "get_x": 31, "getattr": [18, 54, 79, 83, 86], "getcwd": [18, 54], "getpid": 58, "getrespons": [79, 83, 86], "ggerganov": 73, "gi": 42, "gideon": 14, "gigabyt": [53, 54, 55, 56], "gimpel": 36, "gisburn": [8, 14, 42], "git": [1, 48, 53, 54, 55, 90, 97, 99], "github": [0, 1, 3, 4, 5, 8, 10, 14, 15, 16, 18, 20, 22, 24, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 48, 53, 54, 55, 56, 58, 60, 64, 65, 66, 69, 73, 74, 75, 77, 79, 80, 81, 83, 84, 86, 87, 91, 92, 93, 94, 95, 97, 98, 99], "githubusercont": [8, 14, 24, 42, 43, 55, 56, 73, 74, 84, 91, 93, 94, 99], "give": [24, 27, 60, 73, 84, 86, 94], "given": [10, 27, 36, 42, 64, 73, 74, 78, 79, 80, 83, 84, 87], "glanc": [27, 58], "global": [56, 87], "global_step": [8, 42, 64, 84], "globalslet": 54, "glu": [53, 68], "gnostic": 86, "go": [18, 45, 46, 47, 48, 53, 54, 58, 64, 83, 84, 86, 87], "goal": [14, 24, 27, 42, 64, 73, 84], "god": 86, "goe": [64, 83, 84, 87], "gone": 42, "good": [1, 14, 27, 42, 61, 64, 73, 84, 86, 87, 91, 99], "goofi": 69, "googl": [53, 54, 84, 87, 91], "google_drive_path": 84, "goos": 77, "got": [20, 33, 64, 80, 86, 87], "gpt": [1, 10, 14, 25, 27, 28, 31, 39, 41, 43, 45, 46, 47, 51, 54, 55, 56, 58, 62, 63, 64, 67, 72, 73, 74, 76, 77, 78, 80, 89], "gpt2": [8, 10, 14, 15, 16, 18, 22, 24, 29, 36, 37, 42, 43, 45, 46, 47, 58, 60, 64, 66, 67, 72, 73, 74, 75, 84], "gpt2_custom": 60, "gpt2_model": [18, 24], "gpt2_token": 24, "gpt2attent": 46, "gpt2block": 46, "gpt2mlp": 46, "gpt2model": 46, "gpt2token": 18, "gpt2tokenizerfast": 18, "gpt4": [80, 81, 85], "gpt4_model_1": 81, "gpt_class_finetun": [1, 63, 64], "gpt_config": 37, "gpt_config_124m": [8, 36, 37, 42, 43, 53, 60], "gpt_config_1558m": 53, "gpt_download": [10, 41, 42, 43, 63, 64, 72, 73], "gpt_gener": [1, 41, 42, 43], "gpt_hf": 46, "gpt_instruction_finetun": [1, 72, 73], "gpt_train": [1, 41, 42, 43], "gptdatasetv1": [14, 15, 16, 22, 29, 90], "gptmodel": [8, 10, 36, 37, 39, 41, 42, 43, 45, 46, 47, 53, 58, 60, 64, 66, 73, 75, 84, 90], "gptmodelfast": 90, "gpu": [1, 2, 10, 33, 39, 42, 48, 53, 54, 58, 64, 67, 73, 74, 79, 83, 84, 86, 91, 94, 97, 98, 99], "gpu_model": 39, "gqa": 54, "gqa_total_param": 54, "grab": 73, "grad": [3, 8, 31, 36], "grad_fn": [3, 14, 20, 27, 28, 36], "grad_l_b": 3, "grad_l_w1": 3, "grad_valu": 8, "grade": 73, "gradient": [27, 33, 36, 42, 48, 49, 53, 54, 55, 56, 62, 64, 67, 84], "gradual": 8, "grafton": 14, "grain": [54, 73], "gram": 87, "grammar": [84, 87], "grammat": [8, 27, 42, 73, 87], "grass": [52, 53, 54, 55, 56, 73, 79, 83, 86], "grassi": [52, 55], "grayish": [8, 42], "graze": [52, 54, 55, 56, 73, 79, 83, 86], "great": [14, 64, 73, 74, 86, 91], "greater": [8, 14, 42], "greec": 83, "greedi": 36, "greek": [14, 27, 86], "green": 73, "grei": 31, "grew": 48, "grid": [31, 36, 50], "grindl": 14, "ground": [1, 73, 84, 86], "group": [24, 55, 56, 86], "group_siz": [54, 55, 56], "groupedqueryattent": [54, 55, 56], "grow": 86, "growth": 86, "gt": 64, "guarante": 54, "guess": 86, "guest": 86, "gui": 64, "guid": [1, 31, 40, 55, 56, 62, 64, 73, 91, 93, 94], "guidanc": 1, "guidelin": 61, "gutenberg": [1, 42, 62], "gutenberg_preprocess": 48, "g\u00fcc": 54, "h": [31, 45, 46, 79, 83, 86], "h100": [39, 73], "ha": [2, 14, 24, 27, 28, 31, 33, 36, 42, 43, 53, 54, 55, 60, 61, 64, 73, 79, 83, 84, 86, 87, 91, 93, 97, 99], "hack": 86, "had": [8, 14, 42, 60, 64, 73, 77], "hai": [52, 54, 55, 56, 73], "half": [8, 10, 24, 27, 53, 54, 55, 56, 64], "ham": [10, 64], "ham_subset": 64, "hand": [27, 32, 33, 83, 86, 97], "handl": [2, 14, 24, 27, 31, 54, 60, 69, 74, 79, 83, 86, 93], "handle_open": [79, 83, 86], "handler": [79, 83, 86], "handwritten": 64, "hang": 14, "happen": [14, 20, 27, 33, 42, 60, 73, 84], "happi": [73, 83, 86], "hard": [53, 84], "hardcod": 53, "hardwar": [8, 42, 61, 68, 74, 99], "harsh": 86, "has_head": [79, 83, 86], "has_index_nam": 69, "hasn": 97, "hat": 24, "have": [0, 1, 8, 10, 14, 20, 24, 27, 28, 33, 36, 42, 43, 45, 48, 53, 54, 55, 56, 58, 60, 61, 64, 66, 67, 69, 73, 74, 77, 79, 80, 83, 84, 86, 87, 90, 91, 93, 97, 99], "haven": [1, 24, 53, 54, 64, 91], "he": [8, 14, 42, 43, 61, 64, 74, 83, 84, 87], "head": [1, 8, 28, 36, 37, 42, 43, 48, 53, 54, 55, 56, 60, 61, 67, 69, 91, 93, 99], "head_dim": [27, 29, 31, 53, 54, 55, 56], "header": [10, 64, 69, 73, 79, 83, 86], "header_end": [54, 55, 56], "heal": 86, "health": 73, "hear": [14, 86], "heat": 86, "heaven": 86, "hei": [64, 66], "held": 61, "helium": [78, 79, 80, 86], "hello": [14, 18, 24, 36, 54, 60, 80, 87], "help": [1, 8, 14, 36, 58, 67, 73, 83, 86, 87, 91, 99], "helper": [53, 54, 55], "henc": [27, 36, 43, 64, 73, 79, 84], "hendryck": 36, "henetflix": 42, "her": 14, "herbivor": [52, 53, 54, 55, 56, 73], "herd": 54, "here": [8, 14, 20, 24, 27, 31, 36, 39, 42, 45, 46, 47, 53, 54, 58, 61, 64, 73, 74, 79, 80, 83, 84, 86, 87, 91, 94, 97], "hermetic": 86, "hermia": 14, "hesit": [1, 99], "hf": 44, "hf_access_token": [53, 54], "hf_hub_download": [53, 54, 55, 56], "hf_token": [18, 53], "hf_tokenizer_fast": 18, "hi": [8, 42, 43], "hidden": [3, 4, 5, 27, 36], "hidden_dim": [53, 54, 55, 56], "hide": 86, "higginsdynamiczhgmt": 54, "high": [10, 14, 31, 39, 55, 58, 64, 73, 79, 83, 86], "high_freq_factor": [54, 55, 56], "high_freq_wavelen": [54, 55, 56], "higher": [24, 42, 43, 54, 61, 79, 86, 91], "highest": [8, 20, 24, 36, 42, 55, 56], "highli": [24, 27, 74, 80, 91, 94, 97, 99], "highlight": [27, 54, 58, 73, 87], "him": [8, 42, 43, 61], "himself": [8, 14, 42], "hindu": 86, "hinduism": 86, "hold": [8, 27, 36, 60, 84], "holder": 54, "home": [64, 86, 98], "homebrew": 91, "hood": [20, 23, 84, 92], "hope": [24, 73], "hor": 64, "host": [79, 83, 86], "hot": [14, 19, 20, 25, 36, 73], "hour": [42, 48], "hourli": 42, "how": [1, 10, 14, 23, 24, 27, 36, 37, 42, 47, 53, 54, 57, 58, 59, 60, 62, 64, 73, 74, 84, 87, 91, 97], "howev": [8, 10, 24, 27, 33, 36, 42, 43, 47, 48, 53, 54, 58, 60, 61, 64, 67, 73, 74, 79, 80, 84, 86, 87, 91, 94], "hparam": [10, 42, 43, 60, 64, 72, 73, 74], "hparam_grid": 50, "hparam_search": 50, "html": [18, 73], "http": [1, 3, 4, 5, 8, 10, 14, 15, 16, 18, 20, 22, 24, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 48, 51, 52, 53, 54, 55, 56, 58, 60, 64, 65, 66, 69, 70, 73, 74, 75, 77, 79, 80, 81, 83, 84, 86, 87, 88, 90, 91, 92, 93, 94, 95, 97, 99], "http_class": [79, 83, 86], "http_conn_arg": [79, 83, 86], "http_open": [79, 83, 86], "httpconnect": [79, 83, 86], "httperror": [10, 64], "httphandler": [79, 83, 86], "hu": 10, "hub": [42, 44, 47, 53, 54, 55, 56], "hug": [24, 42, 44, 47, 53, 54, 55, 56], "huge": 74, "huggingfac": [42, 45, 47, 52, 53, 54, 55], "huggingface_hub": [53, 54, 55, 56], "human": [14, 73, 81, 83, 84, 86], "humor": 86, "hundr": [42, 53, 54], "hunt": 86, "hybrid": 74, "hyperparamet": [1, 10, 20, 24, 61, 62, 64, 74, 84], "hypotenus": [79, 80], "hypothes": 74, "i": [0, 1, 2, 5, 8, 10, 12, 13, 14, 15, 16, 17, 18, 20, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 50, 51, 52, 53, 54, 55, 56, 58, 60, 61, 63, 64, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 86, 87, 88, 91, 92, 93, 94, 95, 96, 97, 98, 99], "iam": 98, "ic": [10, 64], "id": [20, 24, 42, 53, 60, 67, 73, 74, 84, 97], "id_num": 24, "idea": [1, 27, 32, 34, 36, 54, 73, 84, 86], "ideal": [42, 43, 54, 94], "ident": [14, 43, 54, 64, 84], "identif": 79, "identifi": [24, 64, 73, 74, 76, 77, 79, 80, 84], "ids_in_python_list": 84, "idx": [3, 4, 10, 14, 15, 16, 20, 22, 27, 29, 36, 42, 43, 45, 46, 47, 52, 53, 54, 55, 56, 64, 73, 74, 75, 84], "idx_cond": [36, 42, 55, 56], "idx_next": [36, 42, 55, 56], "ier": 16, "iex": 92, "ignor": [8, 33, 53, 54, 67, 73, 84], "ignore_index": [67, 73, 74], "iliar": 14, "illeg": 42, "illustr": [10, 14, 27, 36, 42, 48, 54, 57, 73, 83, 84, 86, 99], "iloc": 64, "imag": 48, "image_id_or_nam": 97, "imagin": [58, 86], "imbalanc": 64, "imbu": 86, "imdb": [1, 71], "immedi": [33, 91], "immens": 74, "impact": [10, 84, 87], "imperi": 87, "implement": [1, 8, 10, 13, 14, 17, 20, 25, 26, 32, 33, 34, 37, 39, 41, 42, 48, 51, 52, 55, 56, 59, 60, 61, 62, 63, 64, 66, 70, 71, 72, 73, 74, 79, 82, 83, 84, 85, 86, 87, 88, 89], "impli": [27, 64], "implicitli": [27, 29, 31, 53], "impolit": [83, 84], "import": [3, 4, 5, 8, 10, 14, 15, 16, 18, 20, 22, 24, 27, 28, 29, 31, 33, 35, 36, 37, 39, 41, 42, 43, 45, 46, 47, 52, 53, 54, 55, 56, 58, 60, 64, 66, 69, 73, 74, 75, 77, 79, 80, 81, 83, 84, 86, 87, 90, 95, 99], "import_definitions_from_notebook": [18, 54], "import_from_notebook": [18, 54], "importantli": [73, 84], "imported_modul": [18, 54], "importerror": [77, 80, 87], "importlib": [8, 10, 14, 15, 16, 18, 22, 27, 28, 29, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 73, 75, 77, 79, 80, 83, 84, 86, 87], "improv": [1, 8, 14, 24, 36, 45, 47, 49, 53, 54, 58, 61, 62, 64, 65, 67, 68, 73, 74, 84, 89, 99], "in_dim": 10, "in_featur": [3, 10, 37, 60, 64, 66], "in_idx": [36, 37, 53, 54, 55, 56], "inaccur": 80, "inaccuraci": [73, 80], "inch": [42, 43, 87], "inclu": 64, "includ": [1, 2, 8, 14, 24, 27, 31, 33, 35, 36, 38, 42, 52, 55, 56, 61, 62, 64, 71, 73, 74, 79, 80, 83, 84, 86, 87, 98], "inclus": 80, "incomprehens": [8, 42], "inconsist": [27, 42], "incorpor": [10, 27, 36, 60, 68, 86], "incorrect": [37, 52, 73, 79, 80, 83, 84, 87], "incorrectli": [73, 79, 80], "increas": [8, 14, 42, 54, 58, 60, 64, 67, 73, 74, 76, 84, 86], "incredibli": 86, "increment": 8, "inde": [27, 79, 80, 83, 84, 86, 87], "indent": [24, 73, 77, 83, 86, 87], "independ": [14, 36, 39, 87], "index": [3, 4, 10, 18, 20, 27, 31, 36, 42, 43, 54, 55, 56, 60, 64, 72, 73, 74, 81, 84, 90], "index_col": 69, "indexerror": 60, "indic": [18, 36, 39, 42, 53, 54, 55, 56, 64, 67, 73, 74, 80, 91], "individu": [14, 24, 27, 31, 33, 43, 55, 56, 58, 60, 74], "indivis": 31, "indoor": 86, "induc": 84, "industri": 87, "ineffici": [20, 74, 84], "inevit": 8, "inf": [24, 27, 29, 31, 33, 42, 53, 54, 55, 56], "infer": [8, 27, 42, 64, 66, 68, 73, 79, 83, 86, 97], "infer_datetime_format": 69, "infin": 27, "influenc": [10, 54, 73], "info": [24, 73], "inform": [10, 14, 27, 31, 39, 48, 53, 54, 64, 73, 79, 80, 90, 91, 92, 94, 96, 99], "ingredi": 54, "inher": 74, "init": [10, 31], "initi": [2, 8, 14, 15, 16, 20, 22, 24, 27, 29, 31, 33, 36, 42, 43, 58, 66, 67, 72, 73, 74, 84], "initial_lr": 8, "initializelog": 73, "inner": [10, 86], "innov": 86, "inp": 87, "inplac": [10, 37, 46, 64], "input": [3, 10, 14, 20, 22, 24, 28, 31, 33, 36, 37, 39, 42, 53, 54, 55, 56, 60, 61, 64, 65, 66, 67, 71, 73, 76, 77, 78, 79, 80, 84, 86, 87], "input_batch": [8, 10, 42, 64, 65], "input_chunk": [14, 15, 16, 22, 29], "input_dtyp": [53, 54, 55, 56], "input_embed": [14, 15, 29], "input_id": [14, 15, 16, 18, 22, 29, 64, 66], "input_layernorm": [54, 55, 56], "input_prompt": [10, 64], "input_tensor": [39, 64, 66], "input_text": [24, 73, 74, 75, 79, 80, 83, 84], "inputs_1": 73, "inputs_2": 73, "inputs_3": 73, "inputs_lst": [73, 74], "inputs_tensor": [73, 74], "ins": 87, "insens": [8, 42, 43], "insert": 10, "insid": [1, 27, 42, 84, 86, 91, 93, 97], "inspect": [24, 51, 70, 84, 88], "inspir": [55, 56, 86], "instabl": 27, "instal": [1, 5, 6, 7, 14, 18, 31, 38, 39, 42, 45, 46, 47, 48, 53, 54, 55, 56, 60, 73, 76, 77, 80, 84, 87, 95, 98, 99], "instanc": [8, 10, 14, 15, 16, 18, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 48, 53, 54, 55, 56, 58, 64, 66, 73, 74, 75, 79, 83, 84, 86, 87, 91, 92, 93, 94, 98], "instanti": [36, 44, 53, 54, 64, 73, 84], "instead": [2, 8, 10, 14, 22, 24, 27, 31, 36, 42, 43, 53, 54, 55, 56, 61, 64, 65, 67, 68, 73, 74, 84, 86, 87, 91, 97], "instr_prompt_no_input": 87, "instruct": [1, 10, 31, 39, 42, 45, 46, 47, 52, 55, 56, 60, 64, 67, 75, 81, 83, 84, 91, 97], "instruction_length": 74, "instruction_plus_input": [73, 74], "instruction_text": [73, 74, 79, 80, 83, 84], "instructiondataset": [73, 74, 84, 90], "int": [8, 22, 24, 42, 43, 64, 73, 79, 80, 84], "int64": [3, 64], "int_to_str": 14, "intact": 86, "integ": [3, 14, 16, 18, 22, 24, 42, 64, 73, 79], "integr": 27, "intend": [8, 10, 58, 74, 77, 79, 80, 83, 86, 87], "intens": [36, 39], "intent": 80, "inter": 53, "interact": [1, 24, 61, 62, 71, 73, 79, 83, 86, 89], "interactiveshel": 24, "interest": [1, 10, 14, 27, 37, 40, 42, 53, 54, 55, 56, 58, 61, 64, 73, 74, 84, 86, 87, 96], "interfac": [1, 24, 36, 53, 62, 71, 79, 83, 86, 89, 91, 93], "interject": [79, 80], "intermedi": [15, 27, 53, 54, 55, 56, 58], "intern": [42, 87], "internet": 97, "interpret": [27, 42, 84, 87], "interrog": [79, 80], "interrupt": 48, "intimid": 84, "introduc": [1, 10, 24, 27, 42, 48, 54, 58, 61, 64, 73], "introduct": [1, 27, 60, 64], "introspect": 86, "intuit": [1, 21, 22, 25, 27], "inv_freq": [53, 54, 55, 56], "inv_freq_llama": [54, 55, 56], "invalid": 24, "invers": [53, 54, 55, 56], "inverse_vocab": [24, 42, 43], "investig": [80, 86], "invis": [42, 64, 97], "invit": 86, "involv": [27, 73, 87], "io": [18, 54, 69, 73], "io_open": 24, "ioarg": 69, "iohandl": 69, "ipproto_tcp": [79, 83, 86], "iprogress": 18, "ipykernel_2321": 4, "ipynb": [1, 2, 3, 13, 14, 15, 17, 18, 19, 21, 23, 26, 27, 29, 30, 32, 35, 36, 38, 41, 42, 44, 48, 51, 52, 53, 54, 56, 57, 59, 63, 64, 66, 70, 72, 73, 74, 75, 76, 78, 82, 84, 85, 88, 94], "ipython": [24, 33], "ipywidget": 18, "ir": 16, "irm": [92, 93], "ironi": [8, 42, 43], "irrelev": 73, "is_avail": [3, 4, 8, 10, 31, 33, 39, 42, 43, 45, 46, 47, 52, 53, 54, 55, 56, 58, 64, 66, 73, 84], "is_caus": 31, "is_medium_freq": [54, 55, 56], "is_text": 69, "isbn": 1, "isfil": [54, 55, 56], "isinst": [10, 53, 54, 55, 56, 69], "islam": 86, "isn": 27, "isol": [73, 97, 99], "issu": [27, 36, 42, 45, 79, 80, 86, 91, 92, 94], "item": [3, 4, 14, 24, 36, 42, 43, 58, 64, 66, 73, 74, 84, 86], "iter": [8, 10, 14, 22, 24, 36, 42, 43, 60, 61, 64, 69, 84], "its": [8, 14, 24, 28, 36, 42, 54, 55, 56, 60, 64, 73, 79, 80, 83, 84, 86, 87, 91, 97], "itself": [14, 24, 27, 28, 42, 53, 58, 87, 97], "j": [24, 27, 81, 84], "jack": [8, 14, 24, 42], "jane": [73, 74, 84], "jargon": 84, "jesu": 86, "jewelri": 86, "jit": 48, "job": 73, "joi": 86, "join": [14, 18, 24, 54, 58], "jointli": 27, "joke": [64, 86], "journei": [27, 28, 33, 73], "json": [10, 18, 24, 42, 43, 53, 54, 60, 64, 72, 73, 74, 76, 78, 81, 84, 86, 97], "json_data": [73, 77, 79, 80, 83, 87], "json_fil": [76, 77, 79, 80, 83, 87], "json_kei": [73, 79, 80], "judaism": 86, "judg": 73, "judgment": 86, "jupit": 86, "jupyt": [2, 18, 36, 63, 72, 91, 92, 93, 99], "jupyterlab": [91, 92, 93, 94, 95], "jurong": 64, "just": [1, 14, 36, 42, 43, 48, 53, 54, 55, 56, 58, 64, 65, 66, 67, 73, 74, 86, 91, 92, 93, 94], "k": [24, 27, 31, 36, 53], "k1": [54, 55, 56], "k2": [54, 55, 56], "k_b": [42, 45, 46], "k_proj": [54, 55, 56], "k_w": [42, 45, 46], "kaiming_uniform_": [10, 31], "kale": 73, "karaok": 86, "karpathi": 61, "kathuria": 73, "kb": [73, 79, 83, 86], "keep": [1, 10, 24, 42, 43, 48, 49, 55, 56, 61, 64, 67, 73, 74, 83, 84, 86], "keep_date_col": 69, "keep_default_na": 69, "keepdim": [27, 36, 42, 53, 55, 56], "keepsak": 86, "kei": [3, 8, 10, 14, 15, 16, 18, 24, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 60, 64, 66, 73, 75, 77, 80, 83, 84, 86, 87], "kendall_tau_correl": 81, "kendalltau": 81, "kept": [8, 53, 54, 55, 56, 61, 67], "kernel": [61, 98], "key_2": 27, "keyerror": [14, 24], "keys_2": 27, "keys_rot": [53, 54], "kid": 86, "kilogram": 87, "kilomet": [83, 87], "kim": 81, "kind": [27, 42, 43, 79, 83, 86], "kindli": 31, "kingdom": 73, "kitti": 86, "km": [87, 98], "knock": 86, "know": [8, 14, 24, 42, 43, 64, 66, 86, 87], "knowledg": [73, 74, 86, 87], "known": [8, 27, 36, 53, 73, 86, 87], "knuckl": 86, "kv": 54, "kv_idx": 31, "kv_len": 31, "kwarg": [24, 33, 58], "kwd": 69, "kwds_default": 69, "l": [53, 54, 55, 56, 87, 97], "l2": 8, "l4": [39, 73, 74, 84], "l8": 42, "lab": [73, 74, 91, 92, 93, 98, 99], "label": [3, 4, 10, 36, 42, 64, 69, 73, 83, 84], "label_smooth": 84, "labelcolor": 31, "lack": [73, 87], "lack\u0443\u0441": 53, "laid": [42, 43, 64], "lambda": 24, "lamp": 86, "languag": [3, 4, 5, 8, 10, 14, 15, 16, 18, 20, 22, 24, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 60, 61, 64, 65, 66, 69, 73, 74, 75, 77, 79, 80, 81, 83, 84, 86, 87, 90, 92, 95], "laptop": [1, 8, 10, 42, 64, 67, 73, 79, 83, 84, 86, 99], "lar": 64, "larg": [3, 4, 5, 8, 10, 14, 15, 16, 18, 20, 22, 24, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 60, 61, 64, 65, 66, 67, 69, 73, 74, 75, 77, 79, 80, 81, 83, 84, 86, 87, 90, 95], "larger": [1, 8, 36, 39, 42, 48, 53, 54, 58, 61, 67, 68, 73, 79, 86], "largest": [8, 20, 42, 43, 61, 64], "last": [3, 4, 5, 8, 10, 14, 15, 16, 18, 20, 22, 24, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 60, 64, 66, 67, 68, 69, 71, 73, 74, 75, 77, 79, 80, 81, 83, 84, 86, 87], "last_batch": 22, "last_block": [67, 68], "last_index": 24, "last_lay": 67, "last_two_block": 67, "lastli": [10, 14, 36, 42, 43, 52, 53, 54, 58, 61, 64, 73, 74, 84], "late": 80, "later": [8, 14, 24, 27, 31, 36, 42, 53, 54, 58, 64, 73, 79, 80, 84, 86], "latest": [1, 54, 64, 91], "laugh": [8, 42, 43], "launch": [73, 91, 92, 93, 97], "lava": 86, "law": 10, "layer": [1, 3, 4, 5, 8, 10, 14, 19, 24, 25, 29, 31, 37, 42, 43, 45, 46, 54, 55, 56, 61, 64, 67, 68, 73, 79, 83, 86], "layer_output": 36, "layer_s": 36, "layernorm": [10, 36, 37, 46, 61, 64, 90], "layout": [42, 64], "ldot": 8, "lead": [8, 14, 27, 54, 55, 56, 67, 73, 80, 86, 87, 91], "leafi": 73, "learn": [1, 10, 14, 24, 27, 31, 36, 42, 43, 51, 53, 62, 64, 66, 68, 70, 73, 74, 75, 76, 84, 86, 88, 91], "learner": 36, "least": [4, 14, 24, 31, 86, 91, 92, 93], "leav": [53, 73, 86], "lectur": [31, 42, 64], "lecture1": 31, "left": [36, 42, 45, 46, 53, 54, 55, 56, 84, 97], "leftov": 8, "leg": [79, 80], "legaci": 93, "legend": [42, 64], "legibl": 73, "legum": [52, 55, 56], "len": [3, 4, 8, 10, 14, 15, 16, 22, 24, 29, 36, 42, 43, 45, 54, 55, 56, 64, 66, 73, 74, 75, 77, 79, 80, 83, 84, 87], "length": [8, 10, 14, 18, 24, 36, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 60, 61, 64, 66, 67, 71, 72, 73, 74, 75, 79, 84, 86], "lengthi": 87, "less": [8, 10, 24, 42, 43, 58, 64, 67, 73, 83, 84, 93], "lesson": 86, "let": [8, 10, 14, 20, 22, 24, 27, 33, 36, 42, 43, 54, 58, 60, 64, 66, 73, 74, 77, 79, 80, 83, 84, 86, 87, 90], "letter": [24, 27], "level": [60, 86, 87, 91, 92, 93], "leverag": 74, "li": [24, 45, 47, 87], "lib": [10, 14, 15, 16, 18, 24, 27, 29, 33, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 69, 73, 75, 77, 79, 80, 83, 84, 86, 87], "librari": [1, 5, 10, 14, 15, 16, 18, 24, 27, 29, 36, 37, 38, 39, 42, 43, 44, 45, 46, 47, 48, 53, 54, 55, 56, 58, 64, 66, 69, 73, 75, 77, 79, 80, 83, 84, 86, 87, 92, 93, 97, 98, 99], "licens": [48, 53, 54, 55, 56, 86], "lichen": 73, "life": [24, 43, 86], "lifecycl": [12, 98], "lifeforc": 69, "light": 86, "lighten": 86, "lightheart": 86, "lightn": [73, 74, 84], "like": [1, 8, 10, 14, 24, 27, 31, 33, 36, 42, 43, 51, 54, 55, 56, 58, 61, 64, 67, 68, 70, 73, 79, 83, 86, 87, 88, 90, 91, 92, 94, 97, 98], "likewis": 1, "limit": [8, 49, 58, 61, 73, 86], "line": [3, 4, 5, 8, 10, 14, 15, 16, 18, 20, 22, 24, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 48, 53, 54, 55, 56, 58, 60, 64, 65, 66, 69, 73, 74, 75, 77, 79, 80, 81, 83, 84, 86, 87, 91, 97], "linear": [1, 3, 4, 5, 8, 10, 14, 19, 27, 28, 29, 31, 33, 37, 42, 48, 49, 53, 54, 55, 56, 60, 61, 64, 66, 68, 73], "linear_model": 69, "linearwithlora": [10, 74], "linestyl": [42, 64], "linetermin": 69, "linewidth": 31, "link": [1, 60, 91], "linspac": [8, 10, 36, 42, 64, 73, 84], "linux": [36, 73, 79, 83, 86, 91, 92, 93, 97, 99], "list": [3, 14, 24, 36, 42, 53, 54, 61, 64, 73, 74, 79, 80, 84, 94, 97, 99], "list1": 81, "list2": 81, "listcomp": 14, "listen": 86, "liter": 87, "litgpt": 73, "littl": [24, 42], "live": [33, 48, 64], "ll": [1, 36, 43, 54, 55, 56, 73], "llama": [1, 24, 31, 36, 40, 42, 60, 61, 62, 73, 74, 78, 82, 85, 89], "llama2": [52, 53, 54, 81], "llama2_config_7b": [53, 54], "llama2model": [53, 54], "llama3": [52, 53, 73, 79, 81, 83, 85, 86, 90], "llama31_config_8b": 54, "llama32": [52, 54, 56], "llama32_config": [52, 55, 56], "llama32_config_1b": [52, 54], "llama32_config_3b": 52, "llama3_8b_model_1": 81, "llama3_config_8b": 54, "llama3model": [52, 54, 55, 56, 73, 90], "llama3token": [52, 90], "llama_2_context_len": 54, "llama_2_theta_bas": 54, "llama_3_context_len": 54, "llama_3_theta_bas": 54, "llama_size_str": [55, 56], "llamatoken": 53, "llm": [1, 3, 4, 5, 8, 10, 12, 14, 15, 16, 18, 20, 22, 24, 27, 28, 29, 31, 33, 37, 39, 43, 45, 46, 47, 48, 53, 54, 55, 56, 58, 59, 62, 65, 66, 67, 68, 69, 71, 74, 75, 77, 78, 79, 80, 81, 82, 83, 86, 87, 89, 91, 93, 94, 95, 97, 99], "llms_from_scratch": [8, 10, 36, 39, 42, 45, 46, 47, 52, 53, 54, 58, 60, 64, 73, 84, 90], "llmsfromscratchnotebook": 98, "lm": [61, 81], "lm_head": [54, 55, 56], "lmsy": 73, "ln": 36, "ln_1": [42, 45, 46], "ln_2": [42, 45, 46], "ln_f": [45, 46], "load": [1, 8, 10, 13, 14, 18, 26, 33, 36, 40, 41, 43, 48, 51, 62, 63, 64, 70, 72, 74, 77, 81, 86, 88], "load_fil": [45, 47, 54, 55, 56], "load_individual_weight": 58, "load_sequenti": 58, "load_sequentially_with_meta": 58, "load_state_dict": [3, 33, 42, 43, 45, 47, 52, 57, 58, 62, 64, 66, 73, 75, 84], "load_tiktoken_bp": [54, 55, 56], "load_vocab_and_merg": 24, "load_vocab_and_merges_from_openai": [18, 24], "load_weight": 46, "load_weights_into_gpt": [10, 42, 43, 45, 53, 54, 64, 73, 90], "load_weights_into_llama": [53, 54, 55, 56], "loaded_vocab": 24, "loader": [8, 10, 14, 21, 22, 25, 27, 42, 43, 74], "loc": 42, "local": [1, 8, 10, 33, 36, 42, 48, 51, 53, 54, 58, 64, 69, 70, 73, 84, 88, 89, 91, 97], "local_dir": [53, 54, 55, 56], "local_xla": 73, "localhost": [51, 70, 73, 79, 83, 86, 88], "locat": [1, 14, 15, 29, 33, 74, 86, 91, 92, 93], "lock": 93, "log": [3, 4, 42, 48, 73, 84, 97], "log_prob": 84, "log_proba": 42, "log_softmax": 84, "log_softmax_logit": 84, "logarithm": [42, 84], "logger": 48, "logic": 87, "login": [53, 54, 55, 56], "logist": [42, 53, 84], "logisticregress": 69, "logit": [3, 4, 5, 36, 37, 42, 43, 53, 54, 55, 56, 64, 66, 73, 84], "logits_1": 73, "logits_2": 73, "logits_flat": 42, "logprob": 84, "logsigmoid": 84, "long": [24, 36, 50, 61, 64, 66, 91], "longer": [8, 18, 33, 42, 62, 64, 67, 73, 74, 97], "longest": [14, 64, 66, 67, 73, 74, 84], "look": [8, 10, 14, 20, 22, 24, 27, 36, 40, 42, 43, 52, 58, 60, 61, 64, 73, 74, 77, 79, 83, 84, 86, 97], "lookup": 24, "loop": [1, 5, 18, 31, 42, 48, 50, 55, 56, 64, 73, 84], "lora": [1, 64, 67], "lora_alpha": 67, "lora_rank": 67, "loralay": [10, 74], "lose": 80, "loss": [3, 4, 8, 10, 31, 36, 39, 48, 61, 67, 68, 72, 73, 74, 87], "loss_1": 73, "loss_2": 73, "loss_3": 73, "lot": [20, 24, 33, 69, 84], "loud": 86, "love": [52, 54, 55, 56, 73, 79, 83, 86], "low": [8, 10, 43, 48, 64, 67, 79, 80], "low_freq_factor": [54, 55, 56], "low_freq_wavelen": [54, 55, 56], "low_memori": 69, "lower": [24, 27, 30, 31, 39, 42, 43, 53, 54, 55, 56, 58, 64, 67, 74, 97, 99], "lowest": 24, "lr": [3, 4, 8, 10, 42, 43, 64, 73, 84], "lr_increment": 8, "lru_cach": 24, "lssf": 93, "lstrip": [10, 64, 73, 75], "lt": 64, "luck": 86, "lun": [42, 43], "luncheon": 42, "lure": 86, "m": [18, 28, 31, 45, 46, 47, 54, 55, 56, 83, 84, 86, 87, 93], "m3": [8, 10, 42, 64, 67, 73, 79, 83, 84, 86, 99], "mac": 39, "macbook": [8, 10, 42, 54, 55, 64, 67, 73, 79, 83, 84, 86, 99], "machin": [2, 10, 27, 31, 33, 42, 58, 64, 73, 79, 86, 91, 92, 93, 97, 99], "maco": [73, 79, 83, 86, 91, 92, 93, 99], "macosx": 91, "made": [42, 53, 54, 74], "magnitud": 8, "magpi": 86, "maher": 61, "mai": [8, 10, 14, 20, 24, 27, 36, 42, 48, 50, 53, 54, 58, 60, 61, 64, 67, 73, 74, 80, 84, 86, 87, 91, 92, 93, 94, 97], "main": [1, 8, 9, 10, 11, 14, 27, 29, 31, 36, 38, 39, 42, 43, 45, 46, 47, 49, 52, 53, 54, 55, 56, 58, 60, 61, 64, 65, 67, 73, 74, 84, 91, 93, 94, 97, 99], "mainli": [8, 36, 42, 52, 55], "maintain": [68, 73, 80, 86, 87], "major": [31, 48, 73], "make": [10, 22, 27, 33, 42, 43, 48, 53, 54, 58, 61, 64, 69, 73, 74, 76, 77, 79, 80, 83, 84, 86, 87, 91, 97, 98, 99], "makedir": 58, "malici": [45, 47], "man": [1, 61, 95], "manag": [2, 8, 86, 91, 98], "mani": [8, 14, 24, 36, 42, 54, 63, 64, 72, 86, 91, 99], "manifest": [73, 79, 83, 86], "manner": 73, "manual": [27, 31, 33, 42, 84, 91, 97, 99], "manual_loss": 84, "manual_se": [3, 4, 8, 10, 14, 20, 22, 27, 28, 29, 31, 33, 36, 37, 42, 43, 45, 46, 47, 52, 53, 54, 55, 56, 64, 73, 75, 84], "manufactur": [39, 87], "map": [10, 24, 55, 56, 58, 64], "map_loc": [42, 58, 64, 66, 75, 84], "mar": 86, "margin": [31, 84], "mark": [14, 73, 79, 80], "markdown": 1, "markers": 31, "marktext": 1, "mask": [14, 29, 31, 33, 53, 54, 55, 56, 64, 67, 73, 84], "mask_bool": [27, 29, 31, 53, 54, 55], "mask_instruct": 74, "mask_prompt_token": 84, "mask_simpl": 27, "masked_fil": [27, 31, 56], "masked_fill_": [27, 29, 31, 33, 53, 54, 55], "masked_simpl": 27, "masked_simple_norm": 27, "maskedfillbackward0": 27, "mass": 87, "massiv": 73, "master": 86, "match": [3, 24, 27, 29, 31, 39, 42, 53, 54, 55, 56, 64, 84, 86, 95], "materi": [24, 54, 60, 73, 74], "math": [8, 10, 31], "mathbb": 84, "mathbf": 8, "mathemat": [8, 37, 42, 67, 73, 80], "matmul": [3, 14, 25], "matplotlib": [8, 10, 31, 36, 42, 64, 72, 73, 74, 81, 95, 98], "matric": [8, 10, 20, 27, 31, 36, 42, 54], "matrix": [8, 10, 14, 20, 27, 29, 31, 33, 36, 53, 54], "matter": [14, 61], "max": [8, 20, 24, 31, 39, 52, 56, 58, 64, 65, 73, 74, 84], "max_batch_s": 39, "max_context_len": [53, 54], "max_execution_tim": 31, "max_flops_per_second": 39, "max_gpu_memori": 58, "max_grad": 8, "max_grad_param": 8, "max_it": 69, "max_len": [16, 29], "max_length": [8, 10, 14, 15, 16, 18, 22, 29, 42, 43, 64, 65, 66, 73, 74], "max_length_common": 84, "max_mem_byt": [52, 56], "max_mem_gb": [52, 56], "max_memory_alloc": [52, 56, 58], "max_new_token": [10, 36, 42, 43, 45, 46, 47, 52, 53, 54, 55, 56, 64, 73, 74, 75, 84], "max_norm": 8, "max_possible_batch_s": 39, "max_size_mb": 48, "maxim": [42, 64, 73, 79, 83, 86], "maximum": [8, 18, 36, 39, 56, 58, 65, 73, 74, 84], "maxnloc": 42, "maxsiz": 24, "mayb": [24, 33], "mb": [36, 37, 48], "md": [1, 6, 91, 92, 93, 96], "me": [8, 14, 42, 43, 61, 64, 66, 77, 79], "meadow": [52, 55], "meal": [73, 74, 75, 84], "mean": [5, 8, 10, 14, 18, 24, 27, 31, 33, 36, 42, 52, 53, 54, 55, 56, 60, 61, 64, 67, 73, 79, 80, 83, 84, 86, 87], "mean_log_prob": 84, "meanbackward1": 36, "meantim": 84, "measur": [8, 10, 39, 42, 58, 61, 64, 73, 80, 87], "mechan": [1, 14, 32, 33, 36, 53, 54, 55, 56, 64, 73, 84], "media": 86, "medit": 86, "medium": [10, 36, 37, 39, 42, 43, 45, 46, 47, 58, 60, 64, 66, 67, 72, 73, 74, 75, 84], "medium355m": [72, 73, 74, 75, 84], "meet": [79, 80], "megabyt": [36, 37], "megatron": 61, "mem_usag": 58, "memor": [8, 42, 86], "memori": [1, 3, 31, 36, 39, 52, 53, 54, 55, 56, 67, 74, 86], "memory_info": 58, "memory_map": 69, "memory_usage_in_gb": 58, "mental": [1, 86], "mention": [14, 42, 53, 54, 61, 74, 84, 86], "menu": [31, 91, 94, 97, 99], "mercuri": 86, "merg": [14, 24, 60], "mergeable_rank": [54, 55, 56, 60], "merged_id": 24, "merged_token": 24, "merged_token_id": 24, "merges_list": 24, "messag": [10, 54, 55, 56, 64, 73, 77, 79, 80, 83, 86, 87], "message_bodi": [79, 83, 86], "meta": [10, 40, 42, 43, 52, 53, 54, 55, 56, 58, 60, 62, 64, 72, 73, 74, 86], "metadata": [8, 10, 14, 15, 16, 18, 22, 27, 28, 29, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 73, 75, 77, 79, 80, 83, 84, 86, 87], "metaphor": 84, "meter": [83, 87], "meth": [79, 83, 86], "meth_nam": [79, 83, 86], "method": [1, 8, 10, 24, 27, 36, 42, 45, 47, 54, 57, 62, 64, 73, 74, 79, 81, 83, 84, 86], "methodologi": 87, "metric": [39, 42, 61, 69, 84, 87], "mha": [27, 28, 29, 30, 53, 54], "mha_ch03": 31, "mha_ch03_wrapp": 31, "mha_combined_qkv": 31, "mha_einsum": 31, "mha_pytorch_class_default": 31, "mha_pytorch_class_noweight": 31, "mha_pytorch_flex": 31, "mha_pytorch_sc": 31, "mha_pytorch_sdpa_no_flash": 31, "mha_total_param": 54, "mhaeinsum": 31, "mhapytorchclass": 31, "mhapytorchflexattent": 31, "mhapytorchscaleddotproduct": [31, 48], "mhapytorchsdpawithoutflash": 31, "micro": 31, "microphon": 86, "microsoft": 48, "mid": 84, "might": [14, 24, 27, 40, 42, 48, 55, 56, 58, 61, 67, 73, 79, 83, 84, 86, 91, 92, 97], "mikaylagawarecki": 58, "milligram": 87, "millilit": 87, "million": [36, 42, 73], "mimic": [24, 36, 84], "min": [24, 42, 64, 66, 67, 74, 84], "min_batch_s": 39, "min_lr": 8, "min_rank": 24, "min_torch_vers": 31, "min_val": [42, 55, 56], "minbp": 24, "mind": [64, 74, 84, 86], "mine": 91, "miner": 73, "mini": 87, "miniconda": 98, "miniforge3": 91, "minim": [10, 13, 26, 42, 43, 48, 53, 54, 55, 56, 61, 64, 66, 67, 73, 75, 83, 84, 87], "minima": 8, "minimum": 80, "minlength": 42, "minor": [31, 73, 84], "minut": [8, 10, 42, 64, 67, 68, 72, 73, 74, 77, 79, 83, 84, 86, 97], "mirror": [1, 84], "misc": 33, "miscalcul": 87, "misidentifi": 80, "mislead": 87, "mismatch": [42, 45, 46, 53, 54, 55, 56], "missing_char": 24, "mistak": [8, 42, 64], "mistaken": 83, "mistakenli": 87, "mistral": 81, "misunderstand": [80, 86], "misunderstood": 86, "mitig": [36, 42], "mix": [73, 86], "mixtral": 81, "ml": [87, 98], "mlp": [42, 45, 46, 54, 55, 56], "mmbackward0": [20, 27, 28], "mmlu": 73, "mnth": 64, "mobil": [54, 64], "mocholi": 61, "mod": [18, 54], "mode": [24, 31, 42, 64, 69, 84], "model": [4, 5, 8, 14, 15, 16, 18, 20, 22, 24, 28, 29, 31, 33, 41, 44, 48, 51, 61, 62, 63, 67, 70, 71, 72, 73, 74, 77, 78, 80, 81, 83, 86, 87, 89, 90, 95], "model_abbrev": 37, "model_and_optim": [42, 43], "model_checkpoint": 48, "model_chosen_logprob": 84, "model_config": [10, 39, 42, 43, 45, 46, 47, 58, 60, 64, 66, 73, 75, 84], "model_context_length": [52, 65, 67], "model_fil": 52, "model_input": [73, 84], "model_logratio": 84, "model_memory_s": [53, 54, 55, 56], "model_nam": [18, 37, 42, 43, 46, 60], "model_paramet": 58, "model_path": [54, 55, 56], "model_pg_32188": 48, "model_rejected_logprob": 84, "model_respons": [73, 74], "model_s": [10, 42, 43, 60, 64, 67, 73, 75], "model_state_dict": [42, 43, 45, 47, 64], "model_with_shortcut": 36, "model_without_shortcut": 36, "models_dir": [10, 18, 42, 43, 60, 64, 73], "moder": 73, "modern": [36, 42, 68, 91, 92], "modest": 67, "modif": [22, 61, 74, 83], "modifi": [10, 14, 27, 33, 48, 53, 64, 74, 77, 84, 90, 91], "modul": [3, 4, 5, 8, 10, 18, 20, 22, 27, 28, 29, 31, 33, 35, 36, 41, 42, 55, 56, 60, 63, 64, 67, 72, 74, 81], "modulelist": [27, 29, 31, 36, 46, 56], "modulenotfounderror": [3, 4, 5, 8, 20, 22, 28, 31, 33, 60, 74, 81], "moduletyp": [18, 54], "moji": 64, "molar": 87, "mole": 87, "moment": [42, 86], "monitor": [8, 58], "monitor_memori": 58, "mont": 42, "month": [8, 42, 54], "mood": [64, 86], "moon": 86, "more": [2, 8, 10, 14, 21, 22, 24, 25, 27, 31, 36, 39, 42, 43, 48, 49, 51, 53, 54, 55, 57, 58, 60, 61, 62, 64, 67, 70, 72, 73, 74, 76, 77, 79, 80, 83, 84, 86, 87, 88, 91, 92, 93, 94, 96, 99], "moreov": 67, "morn": 80, "moss": 73, "most": [3, 4, 5, 8, 10, 14, 15, 16, 18, 20, 22, 24, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 60, 64, 66, 69, 73, 74, 75, 77, 79, 80, 81, 83, 84, 86, 87, 91, 92, 93, 94], "most_frequ": 69, "moto": 64, "mount": [84, 97], "moustach": 42, "move": [8, 10, 33, 36, 42, 43, 45, 46, 47, 53, 58, 61, 64, 84, 86, 97, 99], "moves\u03b1llradiu": 53, "movi": [1, 71, 86], "mp": [8, 10, 42, 52, 53, 54, 55, 56, 64, 73], "mr": [8, 14, 42], "mseloss": 36, "msg": [79, 83, 86], "mt": 81, "much": [8, 10, 24, 27, 48, 54, 58, 60, 64, 67, 73, 86, 87], "muhammad": 86, "mulbackward0": 27, "multi": [1, 2, 28, 36, 37, 42, 48, 54, 67], "multihead": [1, 26, 27, 29, 34], "multihead_attn": 31, "multiheadattent": [10, 27, 28, 29, 35, 36, 37, 41, 48, 54, 64, 90], "multiheadattentioncombinedqkv": 31, "multiheadattentionwrapp": [27, 28, 29, 90], "multinomi": [42, 43, 55, 56], "multipl": [2, 10, 14, 20, 33, 36, 42, 43, 48, 53, 54, 61, 64, 73, 83, 84, 92], "multipli": [27, 36, 37, 39, 87], "multiprocess": 2, "multitask": [36, 73], "munch": 73, "must": [14, 24, 27, 29, 31, 53, 54, 55, 56, 73, 79, 80, 87], "mv": [97, 99], "my": [12, 24, 42, 55, 56, 60, 61, 64, 73, 74, 79, 84, 86, 91, 99], "mynewtoken_1": 60, "mynewtoken_2": 60, "myself": 73, "mysteri": 86, "mystic": 86, "mythologi": 86, "n": [5, 14, 22, 24, 27, 31, 36, 37, 42, 43, 45, 46, 47, 52, 53, 54, 55, 56, 64, 66, 73, 74, 75, 77, 79, 80, 83, 84, 86, 87, 91], "n1": 86, "n10": 86, "n11": 86, "n12": 86, "n2": 86, "n3": [86, 87], "n4": 86, "n5": 86, "n6": 86, "n7": 86, "n8": 86, "n9": 86, "n_ctx": 42, "n_embd": 42, "n_epoch": [8, 48], "n_head": [8, 10, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 60, 64, 66, 73, 75, 84], "n_kv_group": [54, 55, 56], "n_layer": [8, 10, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 60, 64, 66, 73, 75, 84], "n_token": [29, 36], "n_vocab": [18, 42, 60], "na_filt": 69, "na_valu": 69, "nacl": 87, "nah": 64, "naiv": 27, "name": [3, 4, 5, 8, 10, 14, 15, 16, 18, 20, 22, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 48, 52, 53, 54, 55, 56, 58, 60, 64, 66, 69, 73, 74, 75, 76, 77, 80, 81, 84, 86, 87, 91, 97, 98], "named_children": 10, "named_paramet": [36, 58], "nameerror": 39, "nan": [42, 64, 84], "napul": 53, "narr": 27, "narrow": 84, "nativ": [61, 91], "natur": 86, "navig": [48, 86, 98, 99], "nb": [18, 54], "nbformat": [18, 54], "nbsp": [1, 18, 20, 31], "nbval_ignore_output": 29, "nbval_skip": 15, "nchildren": 86, "ncorrect": [73, 84], "nd": 24, "ndataset": [73, 79, 80, 83], "nearest": 61, "neatli": 64, "necessari": [10, 27, 42, 48, 53, 54, 58, 64, 73, 79, 83, 84, 86, 98], "neck": 61, "need": [3, 8, 14, 24, 27, 31, 33, 38, 42, 43, 53, 54, 55, 56, 58, 73, 74, 77, 80, 84, 86, 87, 91, 92, 93, 97, 99], "need_weight": 31, "neg": [27, 36, 42, 68, 86], "neg_avg_log_proba": 42, "neglig": 36, "nepali": 24, "nest": 3, "net": [3, 24], "network": [8, 10, 14, 27, 39, 53, 64], "neural": [8, 10, 14, 27, 36, 39, 53], "neuralnetwork": [3, 4, 5, 90], "never": 43, "new": [1, 4, 8, 10, 14, 24, 27, 29, 31, 33, 37, 39, 42, 43, 51, 53, 54, 55, 56, 58, 64, 70, 73, 74, 77, 79, 84, 86, 87, 88, 92, 93, 98, 99], "new_ca_with_buff": 33, "new_ca_without_buff": 33, "new_config": [42, 43, 60], "new_embed": 60, "new_entri": 87, "new_id": 24, "new_instr": 87, "new_item": [73, 74], "new_json_data": 87, "new_json_fil": 77, "new_linear": 60, "new_logit": 42, "new_num_token": 60, "new_out_featur": 60, "new_outp": 87, "new_respons": 87, "new_symbol": 24, "new_token": 24, "new_token_id": 60, "newer": [31, 42, 45, 47, 61, 91, 94], "newest": 91, "newgeluactiv": 46, "newli": [76, 77, 87, 97], "newlin": [24, 69, 84], "newline_token_id": 24, "next": [8, 14, 20, 22, 24, 27, 33, 36, 39, 42, 43, 48, 58, 60, 64, 74, 77, 79, 83, 84, 86, 87, 91, 93], "next_token_id": 42, "next_token_logit": [42, 43], "nhello": 54, "nhere": 86, "nh\u01b0": 54, "nice": 53, "night": [61, 86], "nin": 86, "ninput": [14, 77], "nmodel": [73, 79, 80, 84], "nn": [3, 4, 5, 8, 10, 14, 15, 16, 27, 28, 29, 33, 36, 37, 42, 45, 46, 48, 53, 54, 55, 56, 60, 64, 66, 73, 84], "no_grad": [3, 4, 33, 36, 42, 55, 56, 58, 60, 64, 66, 73, 84], "no_input": 87, "no_pad": 67, "nocturn": 79, "nod": 86, "node": 2, "nois": [43, 86], "noisi": 73, "non": [10, 27, 36, 53, 54, 55, 56, 64, 67, 73, 74], "non_masked_target": 74, "none": [8, 10, 18, 22, 24, 31, 39, 42, 43, 53, 54, 55, 56, 60, 64, 66, 69, 73, 74, 79, 83, 84, 86, 87], "nonsens": [42, 53, 54, 84], "nonzero": [73, 74], "norm": [8, 53, 54, 55, 56], "norm1": [10, 36, 37, 42, 45, 46, 53, 54, 55, 56, 64], "norm2": [10, 36, 37, 42, 45, 46, 53, 54, 55, 56, 64], "norm_x": [36, 53], "normal": [24, 27, 53, 60, 64], "normalize_vers": 31, "normalized_shap": 36, "normpath": [18, 54], "notabl": [27, 36], "notat": 36, "note": [1, 2, 8, 10, 14, 20, 24, 27, 31, 33, 36, 39, 42, 43, 48, 50, 52, 53, 54, 55, 56, 60, 61, 64, 73, 74, 77, 79, 80, 81, 83, 84, 86, 87, 90, 91, 92, 93, 94, 99], "notebook": [2, 10, 13, 14, 15, 16, 18, 24, 26, 27, 29, 31, 33, 36, 42, 43, 45, 46, 47, 52, 53, 54, 58, 60, 62, 63, 66, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 91, 99], "notebook_tqdm": 18, "noth": [8, 36, 42, 83, 86], "notic": [42, 58, 60, 61, 64, 67, 84], "notion": 24, "noun": 84, "noutput": [36, 52, 56, 77], "now": [8, 10, 14, 20, 22, 24, 27, 33, 36, 42, 43, 45, 47, 53, 54, 58, 60, 61, 64, 73, 74, 77, 79, 80, 83, 84, 86, 87, 91, 92, 93, 97], "nowadai": [24, 42], "np": [3, 31, 42, 46, 81], "npolici": 84, "nproc_per_nod": [2, 61], "nprocess": 39, "nrefer": 84, "nrememb": 86, "nrow": 69, "nscore": [73, 79, 80], "nsecond": 27, "ntarget": [14, 22, 74], "ntemperatur": 43, "nthe": [78, 79, 80], "nthese": 86, "ntotal": [54, 55, 56], "nuanc": 80, "null": 86, "num_batch": [10, 42, 64, 73, 84], "num_class": [10, 64, 66], "num_ctx": [73, 79], "num_epoch": [3, 4, 10, 42, 43, 64, 68, 73, 84], "num_exampl": 64, "num_head": [27, 28, 29, 31, 36, 37, 53, 54, 55, 56], "num_idx": 20, "num_input": [3, 4, 5], "num_kv_group": [54, 55, 56], "num_output": [3, 4, 5], "num_param": [3, 5], "num_query_group": [54, 55, 56], "num_repeat": 31, "num_sampl": [42, 43, 55, 56], "num_spam": 64, "num_token": [27, 29, 31, 33, 36, 37, 42, 53, 54, 55, 56, 60, 64, 84], "num_work": [3, 4, 8, 10, 14, 15, 22, 42, 43, 64, 73, 74, 84], "number": [1, 3, 5, 8, 10, 14, 20, 21, 24, 25, 27, 28, 29, 31, 36, 37, 39, 42, 43, 48, 50, 53, 54, 55, 56, 60, 64, 65, 72, 73, 74, 77, 78, 79, 80, 83, 84, 86, 87], "numel": [3, 5, 8, 10, 28, 36, 37, 42, 53, 54, 55, 56, 73, 74, 84], "numer": [27, 42, 73, 87], "numerologi": 86, "numpi": [3, 10, 31, 42, 43, 45, 46, 64, 73, 81, 95], "nutshel": 24, "nvalid": 42, "nvidia": [39, 53, 54, 61, 74, 97, 99], "o": [8, 14, 18, 24, 42, 43, 45, 47, 52, 54, 55, 56, 58, 64, 73, 84, 91, 97], "o2": 64, "o200k_bas": 24, "o_proj": [54, 55, 56], "oat": [54, 73, 79, 83, 86], "object": [24, 27, 33, 60, 68, 73, 79, 83, 84, 86], "oblig": 80, "observ": [39, 42, 43, 64, 74], "obtain": [20, 27, 31, 36, 42, 43, 54, 64, 73, 74], "obvious": [83, 84], "ocass": [73, 74, 84], "occas": [73, 74, 84], "occasion": 73, "occass": 74, "occur": [8, 24, 42, 79, 83, 86], "occurr": [24, 54, 55, 56], "odd": 84, "off": [1, 42, 58, 73, 79, 80], "offend": 86, "offer": [10, 24, 33, 36, 43, 73, 78, 86, 91], "offici": [1, 24, 31, 48, 54, 73, 90, 91, 92, 93, 94, 97], "often": [8, 10, 36, 42, 48, 73, 86, 87], "oh": [42, 91], "ok": [42, 45, 56, 64, 77, 95], "okai": 58, "old": [33, 54, 60], "old_context_length": [54, 55], "older": [91, 92, 93], "ollama": [1, 72, 73, 74, 82, 84, 85], "ollama_evalu": [1, 72, 73, 74], "ollama_run": 73, "olmo": 8, "omega": [27, 28], "omega_": 27, "omiss": 73, "omit": [8, 80], "on_bad_lin": 69, "onc": [24, 42, 43, 53, 90, 91, 97, 98, 99], "one": [8, 10, 14, 19, 20, 24, 25, 27, 28, 33, 36, 39, 42, 43, 51, 52, 54, 56, 58, 60, 64, 69, 73, 74, 77, 78, 80, 84, 86, 87, 91, 92, 93, 97], "one_hot": 20, "one_i": [3, 4], "one_x": [3, 4], "onednn": 73, "onehot": 20, "ones": [27, 29, 31, 33, 36, 53, 54, 55, 56, 73, 74, 79, 81, 83, 84], "oni": 64, "onli": [2, 10, 14, 18, 22, 24, 27, 33, 36, 39, 42, 43, 45, 47, 53, 54, 55, 56, 58, 61, 64, 67, 68, 69, 73, 79, 80, 83, 84, 87, 99], "onto": [27, 33, 42, 55, 56], "op": [27, 29, 31], "open": [8, 10, 14, 15, 16, 18, 22, 24, 29, 42, 43, 51, 53, 54, 55, 56, 61, 64, 69, 70, 73, 77, 79, 80, 81, 83, 84, 86, 87, 88, 91, 94, 97, 98, 99], "openaccess": 73, "openai": [1, 10, 14, 17, 36, 41, 43, 44, 45, 46, 47, 51, 53, 54, 62, 64, 73, 76], "openai_api_kei": [77, 80, 87], "openaipubl": 24, "openerdirector": [79, 83, 86], "openli": 42, "oper": [14, 27, 33, 36, 38, 39, 42, 53, 73, 79, 83, 86, 87, 91, 92, 93, 97], "opinion": 73, "opportun": [86, 91], "optim": [1, 8, 10, 14, 27, 31, 42, 43, 48, 56, 64, 68, 73, 74, 82, 89, 92], "optimizer_state_dict": [42, 43], "option": [1, 3, 4, 12, 19, 20, 21, 23, 25, 27, 28, 29, 31, 36, 37, 40, 42, 48, 49, 53, 54, 55, 56, 58, 59, 62, 64, 67, 69, 73, 74, 79, 80, 83, 84, 86, 90, 92, 94, 97], "opu": 81, "order": [24, 52, 53, 73, 74, 87], "ordereddict": 33, "org": [31, 73, 74, 81, 84, 86, 91, 92, 93, 94], "organ": [77, 80, 87], "orig": [8, 42, 43, 60], "orig_book_vers": 8, "orig_context_length": 56, "orig_token": 18, "origin": [3, 8, 10, 14, 17, 27, 29, 31, 36, 37, 42, 43, 44, 47, 51, 53, 54, 55, 56, 58, 60, 61, 64, 68, 73, 79, 80, 83, 84, 87], "original_context_length": [54, 55, 56], "original_file_path": 64, "original_in_featur": 60, "original_out_featur": 60, "original_token_id": 60, "orisha": 86, "oserror": [79, 83, 86], "other": [1, 10, 14, 24, 27, 36, 42, 48, 52, 53, 54, 55, 56, 60, 64, 67, 71, 73, 74, 80, 84, 86, 91, 92, 93, 94, 97, 98], "otherwis": [24, 36, 42, 55, 56, 91, 93], "our": [8, 10, 14, 24, 27, 33, 36, 42, 53, 54, 60, 61, 73, 76, 77, 80, 84, 86, 87], "ourselv": [2, 53, 54], "out": [1, 3, 8, 14, 20, 24, 27, 31, 36, 39, 42, 53, 54, 55, 56, 60, 64, 67, 69, 73, 74, 79, 80, 83, 84, 86, 91, 92, 93, 94, 99], "out_dim": [10, 20], "out_featur": [3, 10, 37, 60, 64, 66], "out_fil": [24, 64], "out_head": [10, 36, 37, 42, 45, 46, 53, 54, 55, 56, 60, 64, 66], "out_ln": 36, "out_norm": 36, "out_proj": [10, 27, 29, 31, 37, 42, 45, 46, 53, 54, 55, 56, 64], "outcom": [43, 74, 84, 87], "outdat": 14, "outdoor": 86, "outlin": [36, 42, 84, 97, 99], "outp": 87, "output": [3, 4, 5, 10, 14, 24, 27, 28, 29, 31, 36, 37, 39, 42, 43, 45, 46, 47, 48, 52, 53, 54, 55, 56, 64, 65, 66, 67, 68, 73, 74, 76, 77, 78, 79, 80, 84, 86, 87], "output_2": [76, 77], "output_dim": [14, 15, 16, 29], "output_dir": 48, "output_fil": 45, "output_text": [52, 54, 55, 56], "outsid": [83, 86], "over": [8, 14, 24, 27, 33, 42, 43, 60, 64, 67, 74, 84, 92, 93, 97, 99], "overal": [27, 58, 64, 73, 74, 79, 80, 84, 93], "overfit": [8, 14, 27, 36, 42, 43, 61, 64, 67, 73], "overflow": 27, "overhead": [58, 93], "overlap": [14, 15, 16, 22, 29], "overshoot": 8, "overview": [12, 24, 61, 77, 80, 84, 87], "own": [1, 55, 56, 73, 86], "owner": 73, "p": [3, 5, 10, 24, 28, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 73, 74, 75, 77, 79, 80, 83, 84, 86, 87, 97], "p0": 24, "p1": 24, "pace": [8, 86], "packag": [1, 8, 10, 14, 15, 16, 18, 24, 27, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 48, 51, 53, 54, 55, 56, 58, 64, 66, 69, 70, 73, 75, 76, 77, 80, 84, 87, 88, 97, 98, 99], "packagenam": 91, "packagenotfounderror": [10, 14, 15, 16, 18, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 73, 75, 84], "pad": [14, 64, 65, 66, 67, 73, 74, 84], "pad_token_id": [64, 66, 73, 74, 84], "page": [1, 27, 36, 48, 49, 73, 84], "paid": 10, "paint": 14, "pair": [1, 14, 17, 25, 54, 73, 76, 84, 86], "pair_id": 24, "pairwis": 27, "palac": 14, "palett": 97, "palm": 39, "panda": [10, 64, 69, 81, 95], "paper": [36, 39, 53, 54, 61, 74, 81, 84, 86, 87], "parallel": [27, 48, 61, 68], "param": [8, 10, 14, 15, 16, 18, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 60, 64, 65, 66, 73, 74, 75, 84], "param_config": [53, 54, 55, 56], "param_data": 58, "param_dir": 58, "param_group": 8, "param_s": [53, 54, 55, 56], "paramet": [1, 3, 4, 5, 8, 14, 20, 27, 28, 31, 33, 36, 39, 42, 43, 45, 46, 53, 54, 55, 56, 58, 61, 64, 65, 67, 68, 73, 79, 83, 84, 86, 98], "pardon": 14, "park": [83, 84, 87], "parliamentari": 54, "pars": [22, 31], "parse_d": 69, "parse_vers": 31, "parsed_vers": 31, "parser": 69, "part": [10, 14, 36, 43, 58, 72, 73, 79, 80, 83, 84], "part1": [1, 2], "part2": [1, 2, 3], "parti": 86, "partial": [73, 80, 84], "particular": [24, 33, 67, 73, 86], "particularli": [33, 42, 64, 73], "partli": 74, "pass": [10, 22, 36, 39, 53, 54, 55, 56, 58, 64, 73, 84, 86], "passag": [8, 42], "passiv": [1, 73, 74, 75, 84], "passthrough": 24, "past": [53, 54, 55, 56], "pat_str": [54, 55, 56, 60], "path": [8, 10, 14, 18, 24, 36, 42, 43, 45, 47, 52, 54, 55, 56, 58, 64, 66, 73, 75, 79, 80, 83, 84, 86, 87, 91, 93, 97], "path_or_buf": 69, "pathlib": [10, 54, 55, 56, 64, 66, 75, 79, 80, 83, 84, 87], "patient": [86, 87], "pattern": 87, "payload": [73, 79, 83, 86], "pcie": 39, "pd": [10, 64, 69, 81], "pdf": [8, 31, 42, 64, 72, 74, 84], "peak": [39, 58], "peak_lr": 8, "peak_mem_usage_gb": 58, "peak_memory_us": 58, "peaki": 42, "pearson_correl": 81, "peopl": [73, 86], "per": [5, 18, 31, 36, 37, 38, 39, 53, 54, 55, 56, 60, 61, 79, 83, 87], "percentag": [64, 73], "perfect": [73, 86], "perfectli": 79, "perform": [1, 10, 14, 20, 24, 27, 30, 31, 33, 36, 40, 43, 48, 54, 62, 64, 67, 68, 73, 74, 86, 87, 93], "perhap": [64, 67, 86], "period": [8, 14, 58], "permiss": [48, 53, 54, 55, 56, 98], "permit": 39, "permut": 31, "persever": 86, "persist": [55, 56, 91, 99], "person": [12, 80, 86, 87, 91, 99], "perspect": 86, "pet": 86, "pg16527_raw": 48, "pg29836_raw": 48, "pgcorpu": 48, "ph": 77, "pharmaceut": 87, "phase": [8, 14], "phenomenon": 73, "phi": [73, 74], "phi3": 74, "phi3_prompt": 74, "philip": 24, "philosophi": 77, "photo": 86, "phrase": [73, 83, 84, 87], "physic": [1, 86], "pi": [8, 36, 53, 54, 55, 56], "pi_": 84, "pick": [42, 61], "picki": 87, "pictur": [8, 42, 53, 54, 55, 56], "piecewis": 36, "pin_memori": 61, "pinpoint": 86, "pip": [14, 18, 31, 38, 39, 42, 45, 46, 48, 51, 52, 53, 54, 55, 56, 68, 70, 76, 77, 80, 84, 87, 88, 90, 91, 92, 93, 94, 99], "pipelin": [13, 26, 29], "piti": 64, "pix": 91, "pixi": 91, "pizza": [42, 43, 86], "pizza_idx": 43, "pkg": [8, 10, 36, 39, 42, 43, 45, 46, 47, 52, 53, 54, 55, 56, 58, 60, 64, 66, 73, 75, 77, 79, 80, 83, 84, 86, 87, 93], "place": [27, 29, 31, 62, 86], "placehold": [24, 27, 36, 73, 74, 91], "plai": 86, "plaintext": 48, "plan": [42, 48, 61, 73, 74, 86, 97], "plane": 86, "planet": [76, 86], "plant": [52, 54, 55, 56, 73, 79, 83, 86], "platform": [73, 77, 80, 87, 97, 99], "play": 86, "pleas": [0, 1, 6, 18, 24, 27, 31, 33, 39, 42, 48, 53, 54, 55, 56, 60, 73, 74, 80, 84, 87, 90, 91, 92, 93, 94, 95, 96, 99], "pleasant": 86, "pleasur": 86, "plot": [8, 31, 36, 42, 64, 72, 73, 74, 81, 84], "plot_execution_tim": 31, "plot_loss": [8, 42, 73, 84, 90], "plot_valu": [10, 64, 90], "plt": [8, 31, 36, 42, 64, 81], "plu": [27, 58, 68], "plug": 36, "plugin": [73, 99], "plural": 77, "pluto": 76, "point": [27, 36, 38, 39, 42, 64, 67, 73, 80, 91], "polici": 84, "policy_chosen_log_proba": 84, "policy_chosen_logprob": 84, "policy_model": 84, "policy_rejected_log_proba": 84, "policy_rejected_logprob": 84, "policy_response_text": 84, "polit": [83, 84], "poly1d": 81, "polyfit": 81, "polyglot": 92, "pool": 67, "poor": 86, "pop": 99, "popleft": 24, "popular": [8, 24, 27, 42, 60, 73, 86, 99], "port": [73, 79, 83, 86], "portion": 43, "pos_emb": [10, 36, 37, 42, 45, 46, 53, 64, 65, 66], "pos_embed": [14, 15, 29], "pos_embedding_lay": [14, 15, 16, 29], "posit": [24, 27, 36, 42, 53, 54, 55, 56, 64, 67, 68, 73, 74, 84, 86], "possibl": [20, 24, 36, 42, 43, 64, 79, 86, 87], "possible_respons": 84, "post": [1, 54, 61, 67, 73, 79, 83, 86, 97], "post_attention_layernorm": [54, 55, 56], "potato": 73, "potenti": [45, 47, 64, 87, 91], "pound": [64, 87], "pow": [36, 53], "power": [61, 73, 86], "powershel": [92, 93], "pp": [84, 87], "pprint": [84, 87], "practic": [8, 10, 24, 27, 36, 42, 43, 48, 54, 60, 64, 67, 73, 80, 84, 86], "pre": [24, 42, 48, 61, 64, 73, 74, 84, 98], "preced": [27, 64], "preciou": 83, "precis": [53, 54, 55, 56, 73, 80, 87], "precomput": [53, 54, 55, 56], "precompute_rope_param": [53, 54, 55], "predefin": [14, 54], "predict": [3, 4, 14, 27, 36, 42, 64, 67, 68, 69, 73], "predicted_label": [64, 66], "prefer": [1, 8, 27, 48, 64, 73, 82, 89, 91, 92, 93, 94, 96, 97, 99], "preferenti": 24, "prefil": 84, "prefix": 24, "prejudic": [73, 74, 84], "prepar": [14, 36, 42, 66, 69, 74, 76, 77, 87, 89], "prepare_dataset": 48, "prepare_funct": 31, "preprocess": [14, 24, 64], "present": [24, 97, 99], "press": 97, "pretrain": [1, 8, 10, 24, 36, 40, 52, 58, 63, 66, 67, 68, 71, 72, 84, 89, 99], "pretraining_simpl": 48, "pretti": [14, 73, 77, 84], "prevent": [36, 86, 91], "preview": 1, "previou": [14, 27, 35, 36, 41, 42, 43, 54, 58, 61, 63, 64, 72, 73, 84], "previous": [10, 20, 27, 36, 39, 42, 58, 74, 84], "previous_chapt": [8, 10, 35, 36, 41, 42, 43, 48, 53, 54, 58, 63, 64, 66, 72, 73, 75, 84], "pride": [14, 73, 74, 84], "primari": [10, 27, 64, 77, 87, 99], "primarili": [8, 42, 48, 52, 54, 55, 56, 73], "princess": 86, "principl": 86, "print": [1, 3, 4, 5, 8, 10, 14, 15, 16, 18, 20, 22, 24, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 48, 52, 53, 54, 55, 56, 58, 60, 64, 66, 69, 73, 74, 75, 77, 79, 80, 83, 84, 86, 87], "print_gradi": 36, "print_memory_usag": 58, "print_sampled_token": [42, 43], "prior": [27, 79, 83, 86], "prioriti": 24, "prize": 86, "pro": 52, "prob": [42, 55, 56], "proba": [3, 36, 42, 43, 64], "probabl": [27, 36, 42, 55, 56, 64, 84], "problem": [36, 42, 58, 60, 86, 87, 91, 93, 94], "proc": 73, "proce": [36, 84], "procedur": [60, 64, 73, 91, 93, 97], "proceed": 73, "process": [2, 8, 14, 24, 27, 33, 36, 39, 42, 43, 48, 58, 60, 61, 73, 79, 80, 83, 86, 87, 93, 97], "process_it": 73, "process_nam": 73, "processed_text": 24, "produc": [8, 14, 24, 27, 36, 42, 43, 48, 73, 84], "product": [27, 29, 53, 54, 55, 56, 87], "profession": 86, "profil": [39, 53, 54, 55, 56], "progress": [8, 42, 73, 77, 79, 80, 83, 84, 86, 87, 97], "proj": 31, "project": [1, 24, 27, 28, 29, 31, 36, 42, 53, 54, 55, 56, 62, 91, 92, 93, 97, 98, 99], "projector": 86, "prometheu": 81, "promot": 86, "prompt": [42, 52, 54, 55, 56, 64, 73, 75, 77, 79, 80, 83, 84, 86, 87], "prompt_templ": 87, "prompt_token": 84, "prone": 84, "prop": 86, "proper": 1, "properli": 73, "properti": 27, "propos": [10, 36, 39, 84, 86], "prosper": 86, "protein": 55, "protocol": [24, 79, 83, 86], "provid": [12, 25, 34, 36, 39, 40, 42, 45, 46, 47, 48, 53, 58, 73, 77, 79, 80, 83, 86, 87, 91, 93, 94, 98, 99], "provoc": 86, "proxi": 64, "ps1": [92, 93], "psutil": [58, 73, 95], "pt": 58, "pth": [3, 33, 42, 43, 45, 47, 48, 52, 53, 58, 64, 66, 72, 73, 74, 75, 84], "public": [10, 14, 42, 48, 53, 61, 64], "publicli": 73, "publish": [1, 54], "pull": [73, 79, 83, 86, 97], "punctuat": 14, "pure": [27, 73, 79, 83, 84, 86, 93], "purpos": [1, 8, 24, 27, 33, 42, 48, 54, 58, 61, 64, 67, 73, 79, 80, 83, 84, 86, 87], "purposefulli": [53, 54, 55, 56, 61], "put": [14, 27, 74, 84, 87], "py": [1, 2, 3, 4, 8, 10, 14, 15, 16, 17, 18, 24, 27, 29, 31, 33, 35, 36, 37, 39, 41, 42, 43, 45, 46, 47, 48, 50, 51, 53, 54, 55, 56, 58, 61, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 79, 80, 83, 84, 86, 87, 88, 91, 92, 93, 94, 99], "pycharm": 97, "pypi": [8, 10, 36, 42, 52, 53, 54, 58, 64, 73, 84, 93], "pyplot": [8, 31, 36, 42, 64, 81], "pyproject": [91, 93], "pythagorean": 80, "python": [1, 2, 3, 5, 7, 10, 12, 14, 15, 16, 18, 24, 27, 29, 31, 35, 36, 37, 38, 39, 41, 42, 43, 45, 46, 47, 48, 51, 53, 54, 55, 56, 58, 61, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 79, 80, 83, 84, 86, 87, 88, 90, 95, 97, 98], "python3": [10, 14, 15, 16, 18, 24, 27, 29, 33, 36, 37, 39, 42, 43, 45, 46, 47, 48, 53, 54, 55, 56, 58, 64, 66, 69, 73, 75, 77, 79, 80, 83, 84, 86, 87, 91, 93], "python_environment_check": [91, 92, 93, 94, 95], "pytorch": [1, 8, 14, 20, 27, 34, 36, 44, 45, 48, 53, 54, 55, 56, 57, 58, 60, 62, 64, 67, 73, 84, 90, 92, 93, 98], "pytorch_squar": 31, "pytorchmultiheadattent": 90, "q": [27, 31, 36, 53], "q_b": [42, 45, 46], "q_idx": 31, "q_len": 31, "q_proj": [54, 55, 56], "q_w": [42, 45, 46], "qkv": 31, "qkv_bia": [8, 10, 27, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 53, 58, 60, 64, 66, 73, 75, 84], "qo": [92, 93], "quad": [53, 84], "qualit": [73, 84], "qualiti": [42, 73, 86, 87], "quantifi": [79, 80], "quantiti": 87, "queri": [8, 10, 14, 15, 16, 18, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 53, 55, 56, 58, 60, 64, 66, 73, 75, 79, 83, 84, 86], "queries_rot": [53, 54], "query_2": 27, "query_model": [73, 79, 83, 86, 90], "question": [0, 14, 24, 43, 67, 73, 79, 80, 84, 86, 87, 91, 92, 93, 94], "quick": [1, 8, 27, 36, 42, 83], "quicker": 48, "quickest": 99, "quickli": [73, 97], "quit": [8, 42, 43, 54], "quot": 69, "quotechar": 69, "quran": 86, "r": [8, 10, 14, 15, 16, 18, 22, 24, 29, 38, 39, 42, 43, 48, 51, 53, 54, 55, 56, 64, 68, 70, 73, 76, 77, 79, 80, 81, 83, 84, 86, 87, 88, 91, 92, 93, 94, 99], "radford": 36, "rai": 31, "rain": 83, "rainbow": 86, "rais": [10, 14, 15, 16, 18, 24, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 52, 53, 54, 55, 56, 58, 64, 66, 73, 75, 79, 83, 84, 86], "ram": [54, 55, 58, 73, 83], "ran": 73, "rand": [3, 5, 27, 28, 36], "randint": 39, "randn": [31, 36, 53, 54], "random": [20, 27, 36, 43, 67, 68, 80, 83, 84], "random_split": [10, 64, 90], "random_st": 64, "randomli": [27, 64], "rang": [3, 4, 8, 14, 15, 16, 22, 24, 27, 29, 31, 36, 37, 42, 43, 45, 46, 53, 54, 55, 56, 60, 64, 73, 84, 86], "rank": [10, 24, 48, 64, 67, 74, 83], "rare": 42, "rasbt": [1, 3, 4, 5, 8, 10, 14, 15, 16, 18, 20, 22, 24, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 52, 53, 54, 55, 56, 58, 60, 64, 65, 66, 69, 73, 74, 75, 77, 79, 80, 81, 83, 84, 86, 87, 90, 91, 93, 94, 95, 97, 99], "raschka": [1, 3, 4, 5, 8, 10, 14, 15, 16, 18, 20, 22, 24, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 60, 64, 65, 66, 69, 73, 74, 75, 77, 79, 80, 81, 83, 84, 86, 87, 95], "rate": [10, 27, 36, 39, 42, 43, 45, 46, 47, 53, 58, 60, 62, 64, 66, 73, 75, 79, 81, 84, 86], "rather": [14, 21, 25, 42, 65, 80, 97], "ratio": [8, 39, 42, 43, 84], "ration": 73, "rational": [73, 74], "raw": [8, 14, 24, 42, 43, 48, 55, 56, 60, 73, 74, 84, 91, 93, 94, 99], "raw_text": [14, 15, 16, 18, 22, 29], "rcparam": 31, "re": [1, 14, 24, 27, 42, 43, 48, 54, 55, 56, 61, 64, 66, 73, 84, 86, 87, 90, 91], "reach": [27, 91, 92, 93, 94, 99], "reaction": 87, "reactiv": 91, "read": [1, 8, 14, 15, 16, 18, 22, 24, 27, 29, 40, 42, 43, 48, 52, 53, 54, 55, 56, 64, 73, 79, 83, 84, 86], "read_csv": [10, 64, 69], "readabl": [24, 36, 49, 54, 61, 74], "reader": [1, 24, 42, 54, 64, 69, 73, 74, 84, 87], "readi": [14, 27, 53, 54, 64, 84, 86, 91, 92, 93, 97], "readili": 53, "readlin": [24, 73, 79, 83, 86], "readm": [1, 6, 73, 93, 96], "readthedoc": 18, "real": [27, 73, 86], "realiti": 61, "realiz": 91, "realli": [42, 84], "realm": 86, "reason": [1, 20, 36, 42, 43, 53, 58, 60, 69, 73, 80, 83, 84, 87, 93, 99], "reassign": 20, "rebuild": 73, "recal": 53, "recap": [42, 84], "receiv": [64, 66, 73, 80, 87], "recent": [3, 4, 5, 8, 10, 14, 15, 16, 18, 20, 22, 24, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 60, 64, 66, 69, 73, 74, 75, 77, 79, 80, 81, 83, 84, 86, 87, 91, 92, 93], "reckon": 86, "recogn": [74, 87], "recommend": [1, 5, 7, 24, 27, 42, 44, 48, 52, 53, 54, 58, 61, 73, 74, 84, 91, 92, 93, 94, 99], "record": [24, 31, 32, 42, 73, 93], "recreat": [54, 55, 56], "rect": 42, "rectifi": 36, "recur": 86, "recurs": 10, "recycl": 87, "red": 77, "redefin": 54, "reduc": [8, 10, 14, 27, 29, 31, 42, 50, 53, 54, 55, 56, 58, 61, 64, 67, 74, 80, 84, 86, 87], "reduct": 8, "redund": 73, "ref": [55, 56, 84, 91, 93, 99], "ref_chosen_log_proba": 84, "ref_rejected_log_proba": 84, "refer": [1, 8, 24, 27, 31, 36, 42, 53, 60, 61, 67, 73, 77, 79, 83, 84, 86, 91, 92, 93], "referenc": 36, "reference_chosen_logprob": 84, "reference_logratio": 84, "reference_model": 84, "reference_rejected_logprob": 84, "reference_response_text": 84, "refin": [85, 87], "reflect": [54, 73, 80, 85, 86, 90], "reflect_instruct": 87, "reflect_respons": 87, "refr": 42, "refus": [79, 83, 86], "regain": 86, "regard": [53, 84], "regardless": [14, 97], "regex": 24, "region": 55, "regist": [33, 73, 98], "register_buff": [27, 29, 31, 33, 53, 54, 55, 56], "register_paramet": 31, "registri": 97, "regress": 42, "regul": 10, "regular": [8, 10, 14, 33, 42, 54, 55, 56, 67, 91, 93], "reimplement": [36, 68], "reinforc": [53, 84], "reiniti": 64, "reinstal": 91, "reject": [83, 84], "rejected_full_text": 84, "rejected_full_token": 84, "rejected_mask": 84, "rejected_respons": 84, "rejected_reward": 84, "rel": [10, 24, 39, 42, 43, 49, 53, 61, 64, 73, 87], "relat": [42, 73, 87], "relationship": [20, 55, 56, 84], "relative_path": 84, "releas": [31, 54, 55, 56, 91, 92, 93], "relev": [27, 73, 74, 80, 86, 87, 97, 99], "reliabl": [10, 87], "religion": 86, "relu": [3, 4, 5, 36], "relubackward0": 36, "remain": [3, 8, 24, 33, 42, 48, 54, 58, 73, 84, 91], "remaind": [24, 36, 55, 56, 64], "remark": 86, "remateri": 39, "rememb": [33, 86], "remind": [64, 86], "remot": [93, 97], "remov": [14, 24, 27, 36, 42, 53, 54, 55, 56, 65, 73, 79, 83, 84, 86, 92, 93, 97], "renam": [64, 84], "render": 1, "renorm": 27, "rentinget": 42, "reopen": 97, "repeat": [10, 24, 27, 36, 43, 53, 54, 55, 56, 73, 80], "repeat_interleav": [54, 55, 56], "repeatedli": 24, "repetit": [24, 80], "rephras": 73, "replac": [10, 14, 24, 42, 48, 52, 54, 60, 64, 67, 73, 74, 75, 77, 79, 80, 83, 84, 86, 87, 90, 91, 94], "replace_linear_with_lora": [10, 74], "replace_pair": 24, "repo_id": [53, 54, 55, 56], "report": [36, 79, 80, 81], "report\u5b98\u65b9": 54, "repositoi": 99, "repositori": [0, 3, 4, 5, 8, 10, 14, 15, 16, 18, 20, 22, 24, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 48, 52, 53, 54, 55, 56, 58, 60, 64, 65, 66, 69, 73, 74, 75, 77, 79, 80, 81, 83, 84, 86, 87, 91, 92, 93, 94, 95, 97, 98, 99], "repostiori": 91, "repres": [14, 20, 24, 27, 36, 42, 73, 84, 86], "represent": [14, 20, 24, 27, 36, 42], "reproduc": [20, 24, 42, 43, 64, 67, 84, 87], "req": [79, 83, 86], "request": [8, 14, 24, 42, 43, 45, 47, 48, 52, 64, 73, 74, 75, 79, 80, 83, 84, 86, 87], "requir": [10, 18, 27, 31, 33, 36, 38, 39, 42, 43, 48, 51, 52, 53, 54, 55, 56, 58, 67, 68, 70, 73, 74, 76, 77, 79, 80, 83, 84, 86, 87, 88, 91, 92, 93, 94, 99], "required_vers": 31, "requires_grad": [3, 5, 10, 14, 20, 27, 28, 53, 54, 55, 56, 64, 65, 74], "rerun": 73, "res_gen_prompt_input": 87, "res_gen_prompt_no_input": 87, "rescal": [42, 43, 54, 55, 56], "rescale_theta": [54, 55, 56], "research": [1, 27, 36, 87, 99], "reserv": 61, "reserved_": [54, 55, 56], "reset": [24, 42, 58, 64, 84], "reset_index": 64, "reset_paramet": 31, "reset_peak_memory_stat": 58, "reshap": [3, 31, 53, 54, 55, 56], "resid_dropout": 46, "residu": [36, 68], "resili": 86, "resolut": [92, 93], "resolv": [42, 45, 47, 52, 91], "reson": 86, "resourc": [42, 43, 48, 61, 73, 83, 86], "respect": [27, 36, 54, 73, 86, 99], "respond": [73, 79, 80, 86, 87], "respons": [1, 8, 24, 42, 43, 64, 72, 74, 75, 77, 81, 83, 84, 89], "response_data": [73, 79, 83, 86], "response_json": [73, 79, 83, 86], "response_text": [73, 74, 75, 84], "rest": [73, 91], "restart": [53, 73, 91, 97], "restor": [24, 33], "restrict": [42, 53], "result": [8, 10, 14, 18, 24, 27, 33, 36, 37, 42, 43, 48, 53, 60, 61, 64, 65, 66, 67, 69, 73, 74, 79, 81, 83, 86, 87, 94], "retain_graph": 3, "retreat": 86, "retriev": [8, 84], "return": [3, 4, 5, 8, 10, 14, 15, 16, 18, 22, 24, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 69, 73, 74, 75, 77, 79, 80, 83, 84, 86, 87, 91], "reus": [8, 10, 24, 27, 36, 53, 55, 56, 63, 64, 72, 73, 84], "reusuabl": 56, "reveal": 86, "revel": 86, "revers": 24, "revert": 54, "review": [1, 71, 98], "review_classifi": [64, 66], "revisit": [36, 54, 84], "reward": [74, 84], "rewrit": [73, 74, 79, 80, 83, 84], "rewritten": 27, "rewrot": 54, "rexmechicular": 42, "rhythmic": 86, "right": [14, 31, 36, 42, 45, 46, 53, 54, 55, 56, 73, 74, 79, 80, 84, 94, 97, 99], "rightarrow": 84, "ring": 64, "risk": 8, "rlhf": 84, "rm": [53, 97], "rms_norm": 53, "rmsnorm": [54, 55, 56], "rmsnorm_pytorch": 53, "rnn": 27, "rofl": 64, "roform": 53, "role": [54, 55, 56, 73, 77, 79, 80, 83, 86, 87, 98], "roman": 86, "room": [42, 61, 64], "root": [27, 36, 53, 54, 94, 97, 99], "rope": [55, 56], "rope_bas": [54, 55, 56], "rope_config": [54, 55], "rope_freq": [54, 55, 56], "rotari": [53, 54, 55, 56], "rotat": [31, 42, 53, 54, 55, 56], "roughli": [24, 42, 43], "round": [31, 53, 73], "row": [14, 20, 27, 36, 42, 54, 61, 64, 67, 84], "row_2_sum": 27, "row_sum": 27, "rsqrt": 53, "rss": 58, "rstrip": [10, 64, 73, 75], "rsync": 48, "rtx": 39, "rtx_3080": 39, "rtx_3090": 39, "rubric": 73, "rumin": [73, 79, 83, 86], "run": [1, 2, 5, 8, 10, 18, 24, 27, 31, 33, 36, 42, 50, 53, 54, 55, 56, 58, 61, 64, 65, 66, 67, 72, 73, 74, 75, 77, 79, 80, 83, 84, 86, 87, 91, 94, 97, 99], "run_chatgpt": [77, 80, 87], "runarg": 97, "runtim": [73, 74, 93, 97, 99], "runtimearg": 97, "runtimeerror": [4, 33, 39, 73], "rust": 14, "s3": 54, "sa": [79, 83, 86], "sa_v1": [27, 28], "sa_v2": [27, 28], "sabbat": 86, "sacr": 86, "sacrament": 86, "sacramento": 83, "safe": [8, 86], "safetensor": [44, 54, 55, 56], "safeti": [45, 47, 87], "sagemak": 98, "sai": [45, 46, 47, 64, 73, 79, 80, 83, 86], "said": [8, 10, 14, 42, 86], "sake": 14, "same": [4, 8, 10, 20, 24, 27, 33, 36, 42, 43, 45, 47, 53, 54, 55, 56, 58, 60, 64, 67, 68, 73, 74, 81, 84, 87, 91, 92, 93, 94], "sampl": [8, 24, 29, 36, 55, 56, 60, 64, 73, 74, 77, 84, 86], "sample_data": 74, "sample_input": 36, "sample_text": 60, "sampled_id": [42, 43], "saniti": [27, 42], "saptamatrika": 86, "save": [10, 33, 40, 43, 48, 53, 54, 55, 56, 58, 61, 64, 66, 72, 74, 75, 77, 79, 80, 84, 87, 93], "save_path": [79, 80], "save_vocab_and_merg": 24, "savefig": [8, 31, 42, 64], "saw": [69, 73], "scalabl": 68, "scale": [1, 8, 10, 27, 29, 36, 45, 46, 53, 54, 55, 56, 73, 79, 80, 86], "scaled_dot_product_attent": [48, 90], "scaled_logit": [42, 43], "scaled_proba": [42, 43], "scaling_factor": [54, 55, 56], "scan": 24, "scare": 86, "scatter": 81, "scaveng": 86, "scenario": [80, 87], "scene": 69, "schedul": [49, 62], "scheme": [27, 60], "sci_mod": [3, 36], "scienc": [8, 86], "scientif": [36, 87, 91], "scikit": [68, 76], "scipi": 81, "scope": [24, 42, 64], "score": [27, 29, 31, 42, 53, 54, 55, 56, 64, 72, 73, 74, 79, 80, 83, 84], "scratch": [3, 4, 5, 8, 10, 14, 15, 16, 20, 22, 25, 27, 28, 29, 31, 33, 37, 39, 42, 43, 45, 46, 47, 48, 58, 60, 62, 64, 65, 66, 69, 73, 74, 75, 77, 79, 80, 81, 82, 83, 86, 87, 91, 93, 94, 95, 97, 99], "screen": 86, "script": [1, 2, 3, 4, 35, 36, 41, 42, 50, 61, 62, 63, 64, 72, 73, 74, 91, 92, 93, 98, 99], "scropt": 72, "sdpa": 31, "seal": 86, "search": [24, 50, 76], "search_dir": 24, "search_directori": 24, "seat": 86, "sebastian": [1, 3, 4, 5, 8, 10, 14, 15, 16, 18, 20, 22, 24, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 48, 53, 54, 55, 56, 58, 60, 64, 65, 66, 69, 73, 74, 75, 77, 79, 80, 81, 83, 84, 86, 87, 95], "sec": [39, 52, 56, 61, 64], "second": [10, 20, 24, 27, 38, 39, 42, 53, 55, 56, 58, 61, 64, 73, 84], "second_batch": [14, 22], "second_head": 27, "second_r": 27, "secret": [53, 77, 80, 87], "secretli": 84, "section": [2, 8, 10, 14, 27, 33, 36, 42, 43, 48, 53, 54, 55, 56, 58, 60, 61, 64, 66, 73, 74, 84, 90, 91, 96, 99], "secur": [45, 47, 86], "see": [1, 4, 6, 8, 10, 14, 18, 20, 24, 27, 28, 31, 33, 36, 39, 42, 45, 46, 47, 48, 53, 54, 55, 56, 58, 60, 61, 64, 67, 72, 73, 74, 77, 79, 80, 83, 84, 86, 87, 90, 91, 94, 96, 97, 99], "seed": [20, 42, 73, 79, 80, 83, 86, 87], "seek": [1, 86], "seem": [33, 45, 79], "seen": [10, 14, 42, 54, 64, 67, 84], "seg_in": 87, "seg_oup": 87, "segment": [14, 27, 87], "select": [27, 42, 61, 64, 66, 67, 73, 74, 84, 97], "selected_log_prob": 84, "selection_mask": 84, "selector": [79, 83, 86], "self": [3, 4, 5, 10, 14, 15, 16, 22, 24, 28, 29, 31, 33, 36, 37, 42, 43, 53, 54, 55, 56, 60, 61, 64, 67, 69, 73, 74, 79, 83, 84, 86], "self_attn": [54, 55, 56], "selfattention_v1": [27, 28, 90], "selfattention_v2": [27, 28, 90], "send": [73, 79, 83, 86], "sens": [42, 53, 73, 86], "sensit": 76, "sentenc": [8, 24, 27, 42, 73, 74, 75, 76, 77, 79, 80, 84, 87], "sentencepiec": [53, 54], "sentencepieceprocessor": 53, "sentiment": 71, "sep": [10, 64, 69], "separ": [10, 14, 27, 31, 53, 58, 64, 73, 74, 79, 83, 84, 86, 91, 92, 93, 98], "seq_len": [36, 37, 53, 55, 56], "sequenc": [14, 15, 16, 18, 22, 29, 31, 36, 39, 42, 55, 56, 64, 66, 73, 74, 84], "sequenti": [3, 4, 5, 10, 36, 37, 53, 54, 55, 56, 61, 64], "seriou": 86, "sermon": 86, "serv": [8, 42, 61, 73, 79, 83, 86], "server": [42, 51, 61, 70, 79, 83, 86, 88], "servic": 98, "session": [25, 34, 40, 43, 53, 58, 64, 73, 79, 83, 86, 91, 92, 93], "set": [1, 5, 8, 10, 12, 14, 24, 27, 31, 36, 48, 53, 54, 55, 56, 58, 60, 61, 64, 65, 67, 72, 73, 74, 76, 77, 78, 79, 80, 83, 86, 87, 91, 94, 97, 98], "set_float32_matmul_precis": 31, "set_major_loc": 42, "set_printopt": [3, 36], "set_xlabel": [42, 64], "set_xtick": 42, "set_xticklabel": 42, "set_ylabel": [42, 64], "setattr": 10, "setback": 86, "setsockopt": [79, 83, 86], "setup": [1, 2, 5, 7, 27, 42, 48, 92, 93, 94], "seven": 86, "sever": [1, 27, 36, 54, 64, 73, 87, 91], "sevr": [42, 43], "sex": 64, "sft": [72, 73, 74, 75, 84], "sgd": [3, 4, 42], "sh": [91, 92, 93], "sha256": [73, 79, 83, 86], "shakedown": 69, "shape": [3, 4, 10, 14, 15, 27, 28, 29, 31, 33, 36, 37, 42, 45, 46, 53, 54, 55, 56, 58, 60, 61, 64, 65, 66, 73, 74, 84], "share": [1, 3, 12, 31, 42, 53, 54, 60, 64, 77, 80, 86, 87, 91, 99], "sharedbuff": [54, 55], "sharper": 42, "sharpli": 73, "she": [8, 42, 43], "shell": 48, "shift": [14, 36, 42, 45, 46, 53, 73, 74, 84, 97], "shine": 86, "shirt": 86, "shock": 69, "short": [14, 24, 27, 36, 42, 54, 56, 58, 60, 73], "shortcom": 24, "shortcut": [37, 53, 54, 55, 56], "shorten": [8, 42, 43, 60], "shorter": [14, 36, 74], "shortest": 64, "shorthand": 27, "should": [24, 27, 36, 42, 47, 51, 52, 54, 67, 68, 70, 73, 74, 79, 80, 83, 84, 86, 87, 88, 91, 92, 93, 94, 97], "shouldn": 61, "show": [8, 23, 31, 36, 42, 47, 60, 62, 64, 67, 81, 84, 86, 97], "shown": [10, 14, 27, 42, 48, 51, 52, 53, 54, 61, 64, 70, 73, 74, 76, 77, 79, 83, 84, 88, 99], "shrub": [55, 73], "shrunk": 54, "shuffl": [3, 4, 8, 10, 14, 15, 22, 29, 42, 43, 64, 73, 84], "shutil": 84, "si": 87, "side": [10, 14, 27, 54, 80, 97], "sigma": [53, 84], "sigmoid": [3, 36, 53, 84], "sign": 84, "signal": 73, "signific": [27, 58, 61, 73, 86, 87, 91], "significantli": [10, 54, 67, 79, 80, 84, 87], "silicon": [8, 10, 42, 53, 54, 64, 73], "silu": [54, 55, 56], "silver": [42, 43], "simil": [73, 74, 84], "similar": [10, 24, 27, 31, 33, 36, 42, 52, 53, 54, 60, 64, 67, 73, 76, 79, 83, 84, 86, 87, 91, 92, 93, 97], "similarli": [8, 10, 20, 24, 27, 31, 43, 53, 64, 67, 73, 84], "simpl": [1, 14, 20, 21, 25, 36, 42, 48, 49, 53, 54, 58, 61, 73, 74, 77, 79, 80, 83, 84, 86, 87], "simpler": [27, 36, 60], "simplest": [27, 86], "simpletokenizerv1": 14, "simpletokenizerv2": 14, "simpli": [8, 14, 27, 42, 54, 60, 73, 79, 86, 87, 91, 93, 97, 99], "simplic": [14, 27, 36, 42, 48, 53, 54, 58, 60, 64, 86, 87], "simplifi": [2, 27, 53, 84, 86], "simul": 27, "simultan": 53, "sin": [53, 54, 55, 56, 86], "sinc": [1, 8, 10, 14, 20, 24, 27, 33, 36, 42, 43, 48, 53, 54, 56, 61, 64, 67, 73, 74, 80, 84, 87, 91, 93, 94, 97, 99], "sine": [53, 54, 55, 56], "sing": 86, "singl": [2, 24, 28, 31, 36, 37, 60, 84, 86], "singular": 87, "sinusoid": 54, "sit": [45, 46, 47], "site": [18, 24, 69, 77, 80, 87], "situat": 86, "six": 80, "size": [3, 8, 10, 14, 15, 22, 24, 27, 28, 29, 31, 36, 37, 42, 43, 45, 46, 47, 48, 52, 53, 54, 55, 56, 58, 60, 64, 66, 67, 73, 74, 75, 84], "sk": [77, 80, 87], "sketch": [8, 43], "skill": 86, "skin": 86, "skip": [8, 24, 36, 44, 45, 47, 48, 58, 64, 91, 93], "skip_blank_lin": 69, "skipfoot": 69, "skipinitialspac": 69, "skiprow": 69, "sklearn": 69, "sleep": [58, 76, 77, 86], "slide": [15, 16, 29], "slight": [27, 42, 73, 74, 84], "slightli": [2, 8, 10, 42, 43, 53, 61, 64, 67, 73, 74, 79, 80, 83, 84, 91], "slime": 86, "slope": [64, 84], "slow": [8, 61, 84], "slowest": 93, "slowli": 54, "sm": [10, 64], "small": [1, 8, 10, 14, 20, 22, 24, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 48, 53, 54, 58, 60, 64, 66, 67, 68, 73, 75, 77, 79, 80, 83, 84, 86, 87], "smaller": [8, 10, 14, 27, 36, 39, 42, 48, 73, 74, 83, 84, 87], "smallest": [36, 43, 73], "smell": 86, "smile": [8, 42], "smooth": [1, 36, 99], "smooth_factor": [54, 55, 56], "smoothed_inv_freq": [54, 55, 56], "smoother": 8, "sms_spam_collect": [10, 64], "smsspamcollect": [10, 64], "snack": [73, 86], "so": [2, 8, 10, 14, 24, 27, 31, 33, 36, 42, 43, 47, 48, 53, 54, 55, 56, 58, 64, 67, 68, 73, 74, 79, 83, 84, 86, 87, 91, 93], "social": 86, "sock": [79, 83, 86], "socket": [79, 83, 86], "sodium": 87, "soft": 86, "softli": 86, "softmax": [3, 27, 28, 29, 31, 33, 36, 42, 53, 54, 55, 56, 64], "softmax_na": 27, "softmax_with_temperatur": [42, 43], "softmaxbackward0": 27, "softwar": [97, 99], "solar": 76, "solut": [1, 2, 14, 25, 27, 36, 41, 42, 54, 63, 64, 72, 73, 86, 87, 89, 97, 99], "solv": [42, 86, 87], "some": [14, 24, 25, 33, 34, 36, 40, 42, 53, 54, 58, 60, 61, 64, 67, 73, 74, 77, 80, 84, 86, 87, 91], "someon": [87, 92], "someth": [42, 64, 73, 84, 86], "sometim": [42, 61, 73, 86], "someunknownplac": 14, "song": 86, "sooth": 86, "sophist": [8, 49, 61, 62, 73], "sort": [1, 14, 24, 74], "sound": [84, 86], "sourc": [1, 14, 18, 24, 27, 51, 52, 54, 55, 56, 68, 70, 88, 91, 93, 99], "source_address": [79, 83, 86], "sp": 53, "space": [8, 14, 24, 27, 48, 54, 73, 79, 86], "spam": [1, 10, 66, 68, 73], "spamdataset": [10, 64, 65, 73, 90], "sparsiti": 74, "spawn": 2, "speak": 86, "spearman_correl": 81, "spearmanr": 81, "spec": 39, "special": [1, 24, 58, 59, 64, 66, 86, 99], "special_pattern": 24, "special_token": [24, 54, 55, 56, 60], "special_tokens_set": 60, "specif": [8, 10, 27, 31, 36, 42, 43, 52, 54, 56, 58, 60, 64, 73, 80, 81, 84, 86, 87, 91, 92, 93, 97], "specifi": [8, 14, 18, 31, 42, 52, 53, 55, 56, 64, 67, 73, 84, 87, 92, 93, 97, 98], "speech": 80, "speed": [48, 52, 62, 73, 91, 93, 97], "spell": [73, 74, 83, 84, 87], "spend": [8, 42], "spinach": 73, "spiritu": 86, "split": [10, 14, 22, 24, 29, 31, 42, 46, 53, 55, 56, 64, 73, 75, 86], "split_idx": [8, 42, 43], "spm": 53, "spoken": 86, "spot": 86, "spring": 54, "sqrt": [8, 10, 27, 31, 36, 53, 80], "squar": [27, 36, 53, 80, 83], "squeez": [36, 42, 55, 56, 73, 74, 84], "squirrel": [79, 80], "ssl": 14, "sslcertverificationerror": 14, "stabil": [8, 27, 36, 49], "stabl": [18, 27], "stack": [33, 36, 73, 74, 84, 98], "stage": [1, 8, 53, 73, 86], "stai": 86, "stand": [24, 27, 42, 84], "standalon": [24, 35, 41, 43, 52, 53, 54, 63, 72, 73, 84], "standard": [10, 27, 36, 42, 54, 73, 87], "stanford": [73, 74], "stanford_alpaca": 74, "stapl": [54, 73], "star": 69, "start": [8, 24, 27, 28, 31, 33, 36, 39, 42, 43, 51, 52, 54, 55, 56, 58, 60, 64, 68, 70, 73, 79, 83, 84, 86, 87, 88, 91, 93, 97, 98, 99], "start_context": [8, 36, 42, 43, 73, 84], "start_header_id": [54, 55, 56, 86], "start_memory_track": 58, "start_tim": [8, 10, 39, 42, 64, 73, 84], "startl": 86, "startswith": 24, "stat": [31, 81], "state": [27, 44, 45, 48, 57, 80, 83, 87], "state_dict": [3, 42, 45, 46, 47, 58, 64, 73], "statement": [80, 84, 86], "static": [10, 64], "staticmethod": [24, 54, 55], "station": 86, "statist": 84, "std": [5, 18, 31], "stderr": 73, "stem": 86, "step": [1, 3, 4, 8, 10, 15, 20, 28, 31, 33, 36, 40, 42, 43, 48, 52, 55, 56, 61, 62, 64, 67, 72, 73, 74, 83, 84, 85, 86, 87, 91, 97, 99], "still": [20, 33, 42, 43, 53, 54, 58, 64, 66, 73, 79, 83, 86, 91, 93], "stomach": [73, 79, 83, 86], "stop": [24, 42, 55, 56, 58], "storag": [54, 58, 73, 79, 86, 98], "storage_opt": 69, "store": [8, 18, 24, 42, 45, 47, 48, 53, 54, 55, 56, 58], "stori": [14, 42, 84], "storm": 84, "str": [24, 39, 69], "str_to_int": 14, "straightforward": [73, 87], "strategi": [44, 69, 74, 86], "stream_executor": 73, "streamlin": 27, "stren": 42, "strict": 69, "strictli": [27, 73, 80], "stride": [8, 14, 15, 16, 22, 29, 42, 43], "string": [8, 10, 14, 15, 16, 18, 24, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 73, 75, 79, 83, 84, 86], "string_a": 24, "string_b": 24, "strip": [14, 22, 24, 48, 54, 55, 56, 73, 74, 75, 84, 86, 87], "strong": [73, 79, 80], "strongest": 42, "stroud": 42, "structur": [27, 53, 79, 80, 83, 87], "struggl": 86, "student": 84, "studio": [55, 61], "stupid": 84, "style": [1, 24, 31, 68, 73, 84], "sub": [14, 73], "subclass": [10, 14, 15, 16, 18, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 73, 75, 84], "subfold": [93, 99], "subject": 87, "submodul": [36, 54], "suboptim": 74, "subplot": [31, 36, 42, 64], "subprocess": 48, "subsampl": 64, "subscript": 27, "subsect": 42, "subsequ": 27, "subset": [10, 64, 84, 87], "subspac": 27, "substanti": [8, 65, 67, 74, 80, 99], "substitut": [24, 52, 56], "substr": [54, 55, 56], "subsystem": 48, "subtl": 54, "subtract": [36, 73], "subword": [14, 24, 60], "success": [10, 39, 53, 54, 55, 73, 74, 79, 83, 86], "successfulli": [3, 27, 60, 64, 91], "sudo": [48, 91], "suffer": 27, "suffici": 64, "suggest": [1, 61, 73, 86], "suit": [10, 36, 54, 99], "suitabl": [24, 42, 83, 84, 86], "sum": [3, 4, 5, 10, 27, 28, 31, 36, 37, 39, 53, 54, 55, 56, 64, 73, 74, 79, 80, 84], "sum_i": 36, "summar": [1, 24, 30, 41, 42, 54, 73], "summari": [1, 24, 63, 72, 86, 87], "summat": [27, 31], "summer": 54, "sun": 86, "sunlit": 14, "super": [3, 4, 5, 10, 27, 28, 29, 31, 33, 36, 37, 53, 54, 55, 56, 61], "superhero": 86, "supplement": 73, "supplementari": [1, 3, 4, 5, 8, 10, 14, 15, 16, 18, 20, 22, 24, 25, 27, 28, 29, 31, 33, 34, 36, 37, 39, 40, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 60, 64, 65, 66, 69, 73, 74, 75, 77, 79, 80, 81, 83, 84, 86, 87, 95], "support": [2, 24, 27, 31, 36, 42, 52, 54, 61, 65, 67, 69, 73, 74, 79, 84, 86, 87, 91, 92, 93, 97, 98], "supported_context_length": [64, 66], "suppos": [10, 14, 20, 24, 27, 33, 36, 42, 58, 60, 74], "suppress_error": 31, "sure": [27, 43, 48, 58, 73, 74, 77, 79, 80, 83, 84, 86, 87, 97, 99], "surpris": [14, 42, 79, 80, 86], "surprisingli": 86, "surround": 53, "swap": [48, 54, 84], "sweet": [64, 73], "swiglu": [36, 53, 54], "swish": [36, 53], "switch": [61, 99], "sy": [18, 54, 79, 83, 86], "sym": 24, "symbol": [24, 27, 86], "sync": 93, "synchron": [31, 39], "synonym": 83, "syntax": 91, "synthesi": 86, "synthet": [82, 85, 86, 89], "sys_prompt": 87, "system": [27, 36, 39, 42, 58, 73, 76, 79, 83, 86, 87, 91, 92, 93, 97, 99], "system_prompt": 87, "t": [1, 3, 10, 14, 20, 24, 27, 28, 31, 33, 36, 42, 43, 45, 46, 48, 53, 54, 55, 56, 58, 60, 61, 64, 67, 69, 73, 77, 79, 80, 83, 84, 86, 87, 91, 93, 97, 99], "t4": 39, "tab": [51, 53, 64, 70, 88, 91], "tabl": [24, 42, 67, 93], "tackl": 73, "take": [10, 14, 20, 42, 43, 48, 50, 52, 54, 60, 61, 64, 67, 69, 73, 74, 77, 79, 83, 84, 86, 97, 99], "takeawai": [15, 29, 61], "talk": [36, 84], "tall": [52, 55, 56, 73, 79, 83, 86], "tanh": [36, 53], "target": [14, 22, 27, 33, 36, 42, 58, 73, 74, 78, 79, 80, 84], "target_batch": [8, 10, 42, 64], "target_chunk": [14, 15, 16, 22, 29], "target_id": [14, 15, 16, 22, 29], "target_path": 24, "target_probas_1": 42, "target_probas_2": 42, "targets_1": 73, "targets_2": 73, "targets_3": 73, "targets_flat": 42, "targets_lst": [73, 74], "targets_tensor": [73, 74], "task": [10, 27, 36, 42, 43, 58, 64, 73, 74, 75, 79, 80, 83, 84, 87], "tast": 73, "tasti": 73, "tatsu": [73, 74], "tcp_nodelai": [79, 83, 86], "tea": 14, "teach": [73, 84, 86], "team": 61, "teamspac": 55, "teamwork": 86, "technic": [27, 36, 43, 64, 84], "techniqu": [8, 10, 27, 42, 61, 74, 87], "techpowerup": 39, "tediou": 33, "tee": 48, "tell": [43, 73, 84], "temp5_idx": 43, "temperatur": [45, 46, 47, 52, 53, 54, 55, 56, 73, 77, 79, 80, 83, 84, 86, 87], "templat": [54, 73, 74, 86], "temporarili": 58, "tempt": 84, "ten": [42, 53, 54], "tend": 64, "tension": 86, "tensor": [4, 8, 14, 15, 16, 20, 22, 27, 28, 29, 31, 33, 36, 42, 43, 45, 47, 53, 54, 55, 56, 58, 60, 64, 66, 73, 74, 84], "tensor0d": 3, "tensor1d": 3, "tensor2d": 3, "tensor3d_1": 3, "tensor3d_2": 3, "tensor3d_3": 3, "tensor_1": 4, "tensor_2": 4, "tensor_nam": [54, 55, 56], "tensor_stack": 84, "tensorflow": [10, 42, 43, 44, 47, 64, 72, 73, 74, 95, 98], "tensorfo": 93, "tent": 84, "term": [53, 54, 55, 56, 61, 67, 80, 84, 86, 87], "termin": [1, 48, 51, 70, 73, 79, 83, 86, 88, 92, 93, 97, 99], "terrac": 14, "terracesof": 14, "tesla": 39, "test": [10, 14, 18, 22, 48, 58, 64, 65, 67, 68, 69, 72, 73, 74, 78, 79, 83, 86, 99], "test_accuraci": [10, 64], "test_d": [3, 4], "test_data": [73, 74, 84], "test_dataset": [10, 64, 65, 73, 84], "test_df": [10, 64, 69], "test_input": 58, "test_load": [3, 4, 10, 64, 73, 84], "test_loss": 64, "test_port": [73, 84], "text": [1, 8, 10, 15, 16, 18, 22, 24, 27, 29, 31, 39, 43, 45, 46, 48, 53, 54, 60, 61, 66, 69, 73, 74, 77, 79, 80, 83, 84, 86, 87], "text1": 14, "text2": 14, "text_1": [10, 64, 66], "text_2": [64, 66], "text_data": [8, 42, 43, 73, 84], "text_idx": 42, "text_to_token_id": [10, 42, 43, 45, 46, 47, 52, 53, 54, 55, 56, 64, 73, 74, 75, 84, 90], "textfileread": 69, "textur": [73, 86], "tf_enable_onednn_opt": 73, "tflop": 39, "th": 24, "than": [2, 8, 10, 14, 18, 21, 24, 25, 27, 31, 39, 42, 43, 48, 54, 58, 64, 65, 67, 73, 74, 79, 80, 84, 87, 91, 92, 93], "thank": [33, 64], "thei": [1, 8, 14, 24, 27, 31, 33, 36, 42, 45, 47, 48, 52, 53, 54, 55, 56, 64, 66, 73, 74, 77, 79, 80, 83, 84, 86, 87, 92], "them": [1, 8, 10, 14, 20, 24, 27, 42, 48, 54, 58, 60, 64, 73, 77, 80, 83, 84, 86, 91], "theme": 86, "themselv": 86, "theorem": 80, "theoret": 39, "theoretical_max_tokens_per_second": 39, "theosophi": 86, "therebi": 84, "therefor": [8, 33, 36, 80, 87, 91], "thereof": [10, 14, 15, 16, 18, 27, 29, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 64, 66, 73, 75, 84], "theta": [54, 55, 56, 84], "theta_bas": [53, 54, 55, 56], "theta_i": 54, "theta_new": [54, 55, 56], "theta_old": [54, 55, 56], "thi": [0, 2, 5, 8, 10, 12, 13, 14, 15, 16, 18, 20, 24, 26, 27, 29, 31, 33, 35, 36, 39, 41, 42, 43, 44, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 92, 93, 96, 97, 98, 99], "thing": [27, 48, 54, 58, 69, 86], "think": [14, 27, 36, 41, 54, 61, 63, 64, 72, 73, 86, 91], "third": [53, 87], "third_batch": 22, "this_studio": 55, "thop": 39, "those": [1, 14, 27, 37, 42, 55, 56, 87, 90, 92, 93, 96], "though": [14, 42, 43, 54], "thought": [14, 42, 87], "thoughtfulli": 86, "thousand": [14, 42, 53, 54, 69], "thread": 58, "three": [8, 27, 31, 61, 67, 77, 79, 80, 84, 91], "threshold": [8, 76], "threw": [8, 42, 43], "through": [1, 10, 18, 24, 27, 36, 53, 54, 64, 73, 79, 83, 84, 86, 91, 93, 97, 99], "throughout": [42, 86], "throughput": 39, "thu": [35, 36, 58, 74, 84, 87], "thunder": [48, 84], "thunderstorm": [73, 74, 84], "tibetan": 86, "tick": [42, 64], "ticker": 42, "tid": 24, "tiger": [79, 80], "tight_layout": [8, 31, 36, 42, 64], "tik_token": 18, "tiktoken": [1, 8, 10, 14, 15, 16, 17, 22, 24, 29, 36, 42, 43, 45, 46, 47, 53, 54, 55, 56, 64, 66, 72, 73, 74, 75, 84, 95], "time": [8, 10, 14, 24, 27, 31, 33, 36, 39, 42, 43, 48, 50, 52, 53, 54, 55, 56, 58, 61, 64, 67, 73, 77, 79, 84, 86, 87, 91, 93, 99], "time_pytorch_funct": 31, "time_pytorch_function_forward_backward": 31, "timefram": [1, 99], "timeit": [5, 18, 31], "timeout": [79, 83, 86], "timeouterror": [10, 64], "timothi": [54, 73], "tip": [1, 5, 14, 42, 48, 52, 53, 54, 58, 62, 84, 86], "titl": [1, 36], "tmp": 4, "to_csv": [10, 64], "to_empti": 58, "todai": 24, "togeth": [14, 27, 83, 84, 86], "toi": 86, "tok": [24, 61], "tok_emb": [10, 36, 37, 42, 45, 46, 53, 54, 55, 56, 60, 64], "tok_embed": 53, "token": [1, 8, 10, 15, 16, 20, 22, 25, 29, 31, 36, 39, 42, 43, 45, 46, 47, 48, 61, 62, 64, 66, 67, 71, 73, 75, 84], "token1": 24, "token2": 24, "token_embed": [14, 15, 29], "token_embedding_lay": [14, 15, 16, 29], "token_id": [10, 14, 15, 16, 22, 24, 29, 42, 43, 45, 46, 47, 52, 53, 54, 55, 56, 60, 64, 73, 74, 75, 84], "token_id1": 24, "token_id2": 24, "token_ids_to_text": [10, 42, 43, 45, 46, 47, 52, 53, 54, 55, 56, 64, 73, 74, 75, 84, 90], "token_str": 24, "tokenize_with_bp": 24, "tokenizer2": 24, "tokenizer_fil": [52, 53], "tokenizer_file_path": [54, 55, 56], "tokenizer_gpt2": [18, 24], "tokens_per_second": 39, "tokens_process": 39, "tokens_seen": [8, 42, 43, 73, 84], "told": [8, 42, 64], "tolist": [36, 42, 55, 56, 84], "toml": [91, 92, 93], "tone": 86, "tonight": [64, 66], "too": [42, 64, 66, 73, 86], "took": 84, "tool": [58, 73, 77, 79, 83, 86, 92, 97, 99], "toolkit": 97, "top": [20, 27, 50, 54, 91, 92, 93, 99], "top_k": [42, 43, 45, 46, 47, 52, 53, 54, 55, 56], "top_logit": [42, 55, 56], "top_p": 86, "top_po": 42, "topic": [42, 54, 61, 64, 73, 87], "topk": [42, 55, 56], "topk_proba": 42, "torah": 86, "torch": [3, 4, 5, 8, 10, 14, 15, 16, 20, 22, 27, 28, 29, 33, 36, 37, 39, 42, 43, 45, 46, 47, 48, 52, 53, 54, 55, 56, 58, 60, 61, 64, 66, 72, 73, 74, 75, 84, 91, 94, 95], "torchaudio": 31, "torchrun": [2, 61], "torchvis": 31, "total": [3, 5, 8, 10, 14, 27, 28, 36, 37, 39, 42, 48, 53, 54, 55, 56, 64, 73, 74, 77, 84], "total_buff": [53, 54, 55, 56], "total_charact": 42, "total_chosen_reward": 84, "total_exampl": [3, 4], "total_flop": 39, "total_grad": [53, 54, 55, 56], "total_loss": [42, 64, 84], "total_memory_byt": [53, 54, 55, 56], "total_memory_gb": [53, 54, 55, 56], "total_param": [10, 36, 37, 53, 54, 55, 56, 74], "total_params_gpt2": [36, 37], "total_params_norm": [54, 55, 56], "total_rejected_reward": 84, "total_size_byt": [36, 37], "total_size_mb": [36, 37], "total_step": 8, "total_time_second": 39, "total_token": 42, "total_training_step": 8, "toward": [8, 36, 42, 43, 86], "toxic": 86, "toydataset": [3, 4, 90], "tqdm": [18, 42, 72, 73, 74, 77, 79, 80, 83, 86, 87, 95], "tqdmwarn": 18, "trace": 54, "traceback": [3, 4, 5, 8, 10, 14, 15, 16, 18, 20, 22, 24, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 60, 64, 66, 69, 73, 74, 75, 77, 79, 80, 81, 83, 84, 86, 87], "track": [42, 58, 64, 84], "track_lr": 8, "track_tokens_seen": [8, 42], "tracker": [27, 36, 42], "traction": 91, "trade": [58, 87], "tradit": [36, 53, 86, 92], "trail": [54, 55, 56, 86], "train": [1, 10, 14, 20, 25, 27, 31, 33, 36, 39, 41, 48, 50, 53, 54, 56, 58, 60, 62, 63, 64, 65, 67, 68, 69, 71, 72, 74, 79, 83, 86, 97], "train_acc": [10, 64], "train_accuraci": [10, 64], "train_bert_hf": 68, "train_chosen_reward": 84, "train_classifier_simpl": [10, 64, 90], "train_d": [3, 4], "train_data": [42, 43, 73, 84], "train_dataset": [10, 64, 65, 73, 74, 84], "train_df": [10, 64, 69], "train_end": 64, "train_frac": 64, "train_gpt": 68, "train_load": [3, 4, 8, 10, 42, 43, 64, 73, 74, 84], "train_loss": [8, 10, 42, 43, 64, 73, 84], "train_model": [8, 90], "train_model_dpo_simpl": 84, "train_model_simpl": [8, 42, 43, 48, 64, 73, 90], "train_port": [73, 84], "train_ratio": [8, 42, 43], "train_rejected_reward": 84, "train_reward_margin": 84, "train_sklearn_logreg": 68, "train_token": 42, "train_valu": 64, "trainabl": [3, 5, 10, 36, 37, 53, 54, 64, 67, 74], "trainable_lay": [65, 67, 68], "trainable_token": 65, "trainable_token_po": 67, "training_ratio": 42, "transact": 87, "transfer": [33, 42, 48, 53, 54, 73, 74, 77, 79, 80, 83, 86, 87], "transform": [10, 14, 27, 31, 37, 44, 53, 54, 55, 56, 64, 65, 67, 68, 69, 83, 84, 87], "transformerblock": [10, 36, 37, 55, 56, 64, 90], "transit": 8, "translat": 27, "transpos": [27, 29, 31, 33, 53, 54, 55, 56], "travel": 87, "treat": [24, 54, 84, 86], "tree": [8, 10, 36, 39, 42, 45, 46, 47, 53, 54, 55, 58, 60, 64, 73, 84], "tremend": 74, "trf_block": [10, 36, 37, 42, 45, 46, 53, 54, 55, 56, 64], "tri": 64, "trial": 43, "triangl": [79, 80], "trick": [61, 79], "trigonometri": 79, "tril": 27, "trillion": 42, "triu": [27, 29, 31, 33, 53, 54, 55, 56], "true": [3, 4, 8, 10, 14, 15, 18, 20, 22, 24, 27, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 48, 52, 53, 54, 55, 56, 60, 61, 64, 66, 72, 73, 74, 75, 79, 83, 84, 86, 87, 97], "true_valu": 69, "truli": 86, "truncat": [18, 27, 29, 31, 42, 53, 54, 55, 64, 66, 73, 74, 84], "try": [10, 14, 24, 33, 36, 39, 42, 54, 60, 64, 73, 74, 77, 79, 80, 83, 84, 86, 87, 91, 94], "tsv": [10, 64], "tune": [53, 54, 62, 73, 74, 83, 84, 85, 89], "tupl": [24, 54, 55, 74, 84], "turbo": [77, 80, 81], "turn": [14, 42, 64, 73, 80], "turner": 84, "tutori": [12, 24, 32, 42, 48, 60, 91, 92, 93], "tweak": [54, 64, 73], "twice": [24, 54, 58, 79, 80], "twilight": 79, "twini": [42, 64], "two": [4, 10, 14, 27, 36, 39, 42, 53, 55, 56, 64, 67, 73, 74, 79, 80, 84, 87, 91], "txt": [8, 14, 15, 16, 18, 22, 24, 29, 38, 39, 42, 43, 48, 51, 54, 55, 56, 68, 70, 76, 77, 80, 84, 87, 88, 91, 92, 93, 94, 99], "txt1": 36, "txt2": 36, "ty": [36, 37, 54, 55, 56, 60], "type": [14, 18, 33, 36, 52, 53, 54, 55, 56, 58, 73, 74, 79, 80, 83, 84, 86, 93, 97, 98], "typic": [8, 14, 33, 39, 56, 73, 74, 79, 80, 83, 84, 86], "u": [10, 14, 24, 42, 48, 53, 54, 60, 64, 84, 86, 87, 91, 92, 93], "ubuntu": [48, 61, 91, 97], "uci": [10, 64], "ui": [51, 70, 88, 97], "ultim": 87, "ultra": 92, "unabl": [27, 73], "unambigu": 87, "unavail": [44, 62], "unbias": [36, 53], "unbind": 31, "uncertain": 42, "unchang": [3, 10, 33, 84], "uncom": [8, 10, 14, 18, 24, 31, 42, 53, 54, 55, 56, 64, 73], "uncondit": 86, "uncondition": 86, "under": [20, 23, 55, 56, 84, 86, 92], "underflow": 27, "underneath": 86, "undersampl": 64, "understand": [1, 10, 22, 24, 55, 56, 60, 64, 73, 84, 86, 87], "understood": 42, "unencumb": 86, "unequ": 67, "unfam": 14, "unfamiliar": 87, "unfamiliarword": 14, "unhappi": 74, "unheard": 86, "uniform_": 31, "uniformli": 42, "unintend": 27, "uniqu": [14, 24, 54, 55, 56, 79, 81, 86], "unique_char": 24, "unit": [3, 14, 27, 36, 37, 53, 68, 73, 80, 87], "univers": 87, "unk": 14, "unknown": [14, 39, 54, 55, 56, 60], "unlabel": 1, "unlik": [33, 36, 42, 53], "unnecessari": [64, 73], "unnecessarili": [53, 54], "unnorm": 27, "unrel": 14, "unrol": [27, 29, 31, 53], "unsafeviewbackward0": [27, 36], "unsqueez": [31, 36, 42, 53, 55, 56, 64, 66, 84], "unsupervis": 36, "until": [24, 43, 64], "untrain": 36, "unus": [73, 79, 83, 86], "unusu": 43, "unzip": 64, "up": [1, 5, 8, 10, 12, 14, 20, 24, 27, 39, 42, 43, 48, 49, 52, 53, 54, 55, 56, 58, 61, 64, 72, 74, 77, 79, 80, 84, 86, 87, 91, 94, 97, 98], "up_proj": [54, 55, 56], "upcom": [14, 27, 36], "updat": [1, 8, 10, 14, 18, 24, 27, 31, 33, 39, 42, 43, 45, 46, 47, 48, 55, 56, 58, 59, 64, 66, 67, 69, 73, 74, 75, 84, 91], "upgrad": [31, 48], "upload": [47, 98, 99], "upper": [42, 53, 54, 55, 56], "upper_ylim": 31, "url": [1, 8, 10, 14, 24, 31, 42, 43, 45, 47, 52, 64, 73, 74, 79, 83, 84, 86, 94], "url_dir": 45, "urlerror": [10, 64, 79, 83, 86], "urllib": [8, 10, 14, 24, 42, 43, 45, 47, 52, 64, 73, 79, 83, 84, 86], "urlopen": [8, 24, 42, 43, 64, 73, 79, 83, 84, 86], "urlretriev": [14, 42, 45, 47, 52], "us": [1, 2, 5, 8, 10, 14, 15, 16, 17, 22, 24, 27, 29, 32, 33, 34, 36, 41, 43, 45, 47, 49, 51, 55, 56, 65, 67, 68, 70, 73, 74, 76, 77, 82, 84, 85, 88, 89, 92, 93, 96], "usabl": 36, "usag": [2, 24, 42, 53, 54, 55, 56, 58, 61, 72, 73, 79, 83, 86, 90], "usagei": 54, "use_dropout": 31, "use_shortcut": 36, "useb": 92, "usecol": 69, "user": [1, 8, 18, 24, 42, 52, 54, 55, 56, 58, 62, 64, 69, 71, 73, 74, 77, 79, 80, 83, 84, 86, 87, 89, 91, 96, 97, 98, 99], "user_instal": 18, "userwarn": 53, "usf": 64, "usr": [33, 53, 69], "usual": [10, 14, 24, 27, 36, 51, 64, 70, 88], "ut": 24, "utf": [8, 14, 15, 16, 18, 22, 24, 29, 42, 43, 54, 73, 79, 83, 84, 86], "util": [1, 3, 4, 5, 8, 10, 14, 15, 16, 22, 27, 29, 41, 42, 48, 53, 56, 63, 64, 67, 72, 73, 74, 76, 78, 79, 80, 83, 84, 85, 87, 89], "uv": [90, 92, 94, 99], "v": [8, 14, 24, 25, 27, 31, 36, 42, 43, 56, 67, 71, 84, 97], "v100": [10, 48, 64], "v_1": 8, "v_2": 8, "v_b": [42, 45, 46], "v_n": 8, "v_proj": [54, 55, 56], "v_w": [42, 45, 46], "va": 31, "val": [3, 4, 8, 10, 42, 43, 48, 61, 64, 68, 72, 73, 74, 84], "val_acc": [10, 64], "val_accuraci": [10, 64], "val_chosen_reward": 84, "val_data": [42, 43, 73, 84], "val_dataset": [10, 64, 65, 73, 84], "val_df": 69, "val_load": [8, 10, 42, 43, 64, 73, 84], "val_loss": [8, 10, 42, 43, 64, 73, 84], "val_port": [73, 84], "val_rejected_reward": 84, "val_reward_margin": 84, "val_token": 42, "val_valu": 64, "valid": [8, 10, 24, 48, 53, 54, 55, 64, 65, 67, 68, 69, 72, 73, 74, 87], "validation_df": [10, 64], "validation_end": 64, "validation_frac": 64, "valu": [8, 10, 14, 20, 24, 27, 28, 29, 31, 33, 36, 39, 42, 43, 45, 46, 47, 48, 53, 54, 55, 56, 58, 60, 61, 64, 66, 73, 75, 76, 80, 84], "valuabl": [84, 86], "value_2": 27, "value_count": 64, "valueerror": [24, 37, 42, 45, 46, 52, 53, 54, 55, 56, 73, 79, 80], "vanish": 36, "var": [36, 53], "varbackward0": 36, "vari": [52, 55, 56, 80, 86], "variabl": [36, 67, 73, 84], "varianc": [36, 53], "variant": [27, 34, 53, 55, 56, 68, 90], "varieti": [42, 73], "variou": [1, 17, 24, 36, 52, 55, 56, 67, 73, 79, 83, 86, 87, 90], "vascular": 73, "vast": [8, 36, 42, 48], "ve": [54, 55, 56, 67, 86], "vector": [8, 14, 19, 20, 25, 27, 31, 36, 42, 53, 64, 69], "veget": [54, 73], "veggi": 73, "veil": 86, "veloc": 73, "venu": 86, "venv": [18, 91, 93], "verb": [76, 77, 84, 87], "verbal": 86, "verbatim": [8, 42], "verbos": [39, 69, 73, 79, 80, 87], "verdict": [8, 14, 15, 16, 18, 24, 42, 43], "verdict_path": 24, "veri": [8, 24, 27, 36, 42, 54, 60, 61, 64, 73, 74, 79, 80, 84, 87], "verif": [10, 27, 64], "verifi": [53, 54, 73, 79, 83, 86], "vers": 86, "version": [2, 8, 10, 14, 15, 16, 18, 20, 22, 24, 27, 28, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 48, 53, 54, 55, 56, 58, 61, 64, 66, 69, 72, 73, 74, 75, 76, 77, 79, 80, 83, 84, 86, 87, 91, 92, 93, 94, 95], "versu": 53, "veterinarian": 73, "via": [1, 2, 10, 14, 24, 27, 31, 33, 38, 42, 44, 47, 48, 50, 51, 53, 54, 55, 57, 58, 59, 60, 61, 62, 64, 65, 66, 68, 70, 73, 74, 75, 76, 78, 83, 84, 88, 91, 92, 93, 94, 99], "viabl": 67, "vicuna": 81, "video": [12, 25, 32, 34, 40, 99], "view": [1, 3, 27, 29, 31, 48, 53, 54, 55, 56, 73], "viewbackward0": 27, "vindic": [8, 42, 43], "violat": [24, 42], "virtu": 86, "virtual": 92, "vision": 36, "visit": [1, 53, 54, 55, 56, 73, 79, 83, 86, 92, 93], "visual": [14, 20, 24, 27, 42, 61, 73, 79, 80], "vivid": 73, "vocab": [10, 14, 18, 24, 36, 42, 43, 55, 56, 60, 61, 64, 72, 73, 74], "vocab_path": [18, 24], "vocab_s": [8, 10, 14, 15, 16, 24, 29, 36, 37, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 60, 64, 66, 73, 75, 84], "vocabular": 24, "vocabulari": [8, 10, 14, 36, 39, 42, 43, 45, 46, 47, 53, 54, 55, 56, 58, 60, 64, 66, 73, 74, 75, 84], "voic": [1, 73, 86], "volum": 87, "vram": [52, 56, 58], "w": [8, 10, 16, 20, 22, 24, 42, 43, 73, 77, 79, 80, 83, 84, 86, 87], "w1": [3, 53], "w2": 53, "w3": 53, "w_": 10, "w_k": 27, "w_kei": [10, 27, 28, 29, 31, 33, 37, 42, 45, 46, 53, 54, 55, 56, 64], "w_q": 27, "w_queri": [10, 27, 28, 29, 31, 33, 37, 42, 45, 46, 53, 54, 55, 56, 64], "w_v": 27, "w_valu": [10, 27, 28, 29, 31, 33, 37, 42, 45, 46, 53, 54, 55, 56, 64], "wa": [8, 10, 14, 15, 16, 18, 24, 27, 29, 31, 33, 36, 37, 39, 42, 43, 45, 46, 47, 48, 53, 54, 55, 56, 58, 61, 64, 66, 67, 69, 73, 74, 75, 77, 79, 80, 83, 84, 86, 87, 91], "wahe": 31, "wai": [8, 10, 14, 24, 27, 31, 36, 42, 43, 48, 52, 60, 61, 64, 73, 74, 79, 80, 83, 84, 86, 91, 97, 99], "wait": 86, "walk": [27, 54, 83, 97], "want": [1, 8, 10, 14, 20, 24, 27, 28, 42, 43, 48, 50, 53, 54, 55, 58, 60, 64, 66, 73, 74, 77, 79, 80, 84, 86, 91, 93, 94, 97, 99], "warm": 49, "warmup": [42, 48], "warmup_step": 8, "warn": [24, 48, 53, 58, 73, 97], "wasn": 42, "wasn\u0645": 42, "wast": 20, "wastefulli": [54, 55, 56], "water": [86, 87], "watermark": [91, 94], "wavelen": [54, 55, 56], "wb": [24, 64], "we": [8, 10, 14, 20, 22, 24, 27, 28, 29, 31, 33, 35, 36, 37, 39, 41, 42, 43, 45, 46, 47, 48, 49, 51, 52, 53, 54, 55, 56, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 77, 79, 80, 83, 84, 86, 87, 88, 93], "websit": [1, 48, 53, 54, 73, 79, 83, 86, 91, 94, 99], "weed": 73, "week": [8, 42, 48, 86], "weight": [1, 3, 8, 10, 14, 20, 28, 33, 36, 37, 40, 41, 43, 48, 51, 60, 62, 63, 65, 66, 67, 68, 70, 72, 73, 74, 84, 88], "weight_decai": [8, 10, 42, 43, 64, 73, 84], "weight_path": 58, "weights_fil": [53, 54, 55, 56], "weights_onli": [3, 42, 43, 45, 47, 52, 53, 58, 64, 66, 75, 84], "welcom": 1, "well": [1, 14, 24, 27, 33, 36, 42, 43, 53, 54, 55, 64, 67, 73, 74, 86, 87, 91, 92, 93, 94, 99], "went": 83, "were": [8, 24, 27, 33, 36, 42, 43, 58, 61, 64, 73, 84, 86, 97], "wget": [92, 93], "wharton": 14, "what": [14, 20, 24, 27, 33, 42, 52, 58, 60, 61, 64, 67, 74, 78, 79, 80, 83, 84, 86, 87], "wheat": [73, 79, 83, 86], "when": [8, 10, 14, 20, 24, 27, 33, 36, 42, 43, 48, 52, 54, 58, 60, 64, 67, 73, 74, 84, 86, 87, 91, 99], "where": [10, 14, 24, 27, 29, 31, 33, 36, 39, 42, 43, 51, 53, 54, 55, 56, 58, 61, 64, 70, 73, 74, 79, 80, 83, 84, 86, 87, 88, 91], "wherea": [8, 27, 42, 53, 64, 79], "whether": [5, 10, 43, 68, 69, 73, 97], "which": [1, 2, 8, 10, 14, 20, 22, 24, 27, 31, 32, 33, 34, 35, 36, 39, 41, 42, 43, 48, 52, 53, 54, 55, 56, 58, 60, 61, 63, 64, 65, 67, 72, 73, 74, 77, 79, 80, 81, 83, 84, 86, 87, 91, 93, 94, 99], "while": [24, 27, 39, 53, 58, 68, 73, 74, 79, 80, 83, 86, 87, 91, 92, 93, 97, 99], "whistl": [1, 48, 50], "white": 31, "whitespac": [14, 24, 54, 55, 56], "whl": 31, "who": [14, 24, 61, 86, 91, 92, 93, 96], "whole": [24, 62, 79, 80, 83, 84, 87], "why": [20, 36, 53, 58, 73, 74, 79, 84, 87], "wide": [1, 87, 91], "widespread": 86, "wif": 64, "wikipedia": 14, "wild": [52, 55, 56, 73, 79, 83, 86], "wilder": 86, "willing": 86, "willow": 73, "win": [64, 86], "window": [15, 16, 24, 29, 36, 42, 53, 73, 79, 83, 86, 91, 92, 93, 99], "winner": [64, 66], "wise": [27, 53], "wish": [64, 97], "within": [1, 8, 27, 36, 49, 53, 97, 99], "without": [15, 24, 36, 42, 54, 58, 60, 64, 73, 74, 77, 84, 86, 87, 99], "witti": 86, "wizardlm": 87, "wk": 53, "wkly": 64, "wo": 53, "women": 42, "won": [24, 54, 84], "wonder": [58, 86], "word": [8, 10, 24, 36, 42, 43, 58, 60, 64, 74, 84, 87], "work": [1, 10, 20, 23, 24, 27, 33, 36, 42, 45, 48, 53, 54, 55, 58, 60, 61, 64, 67, 73, 74, 77, 78, 79, 80, 83, 84, 86, 87, 91, 92, 93, 97, 98], "workaround": 58, "workflow": [14, 91, 92, 93], "world": [14, 18, 24, 54, 73, 80, 86, 87], "worri": [84, 86], "wors": [65, 67, 74], "worth": [52, 55, 56, 86], "worthwhil": 67, "would": [1, 8, 14, 24, 27, 36, 42, 43, 48, 53, 54, 55, 58, 60, 64, 73, 74, 80, 83, 84, 86, 87, 97], "wouldn": [36, 73], "wow": [79, 80], "wpe": [42, 45, 46], "wq": 53, "wrap": 27, "wrapper": [53, 54, 73, 79, 83, 86], "write": [8, 10, 22, 24, 27, 42, 43, 48, 54, 61, 64, 73, 74, 75, 79, 80, 83, 84, 86, 87, 97], "writeln": 54, "written": [61, 64, 73, 84, 97, 99], "wrong": [53, 80, 83, 84, 94], "wsl": 48, "wte": [42, 45, 46], "wv": 53, "www": [1, 39, 91, 95], "w\u00e4hrend": 53, "x": [3, 4, 5, 10, 14, 15, 16, 20, 24, 27, 28, 29, 31, 33, 36, 37, 42, 43, 46, 53, 54, 55, 56, 64, 84], "x1": [3, 53, 55, 56], "x2": [53, 55, 56], "x_2": 27, "x_fc1": [53, 55, 56], "x_fc2": [53, 55, 56], "x_i": [27, 36, 53], "x_j": 27, "x_norm": 53, "x_rotat": [53, 55, 56], "x_test": [3, 4, 69], "x_train": [3, 4, 69], "x_val": 69, "xaxi": 42, "xl": [10, 36, 37, 39, 42, 43, 45, 46, 47, 58, 60, 64, 66, 67, 73, 75, 84], "xla": 73, "xlabel": [8, 36, 81], "xlarg": 98, "xtick": 31, "xwartz": 69, "y": [3, 4, 14, 15, 16, 24, 29, 36, 42, 48, 64], "y_gelu": 36, "y_i": 53, "y_l": 84, "y_pred_test": 69, "y_pred_train": 69, "y_pred_val": 69, "y_relu": 36, "y_test": [3, 4, 69], "y_train": [3, 4, 69], "y_val": 69, "y_w": 84, "ye": [8, 42, 43, 64, 73, 79, 83, 86], "year": [1, 86], "yellow": 77, "yerr": 31, "yet": [1, 8, 10, 31, 42, 53, 54, 64, 73, 77, 80, 87, 99], "yield": [67, 68], "ylabel": [8, 31, 36, 81], "ylim": 31, "yml": 98, "yoruba": 86, "you": [0, 1, 2, 8, 10, 14, 24, 27, 31, 36, 37, 40, 41, 42, 43, 48, 49, 50, 51, 52, 53, 54, 55, 56, 58, 60, 61, 63, 64, 65, 66, 67, 69, 70, 73, 74, 76, 77, 79, 80, 83, 84, 86, 87, 88, 90, 91, 92, 93, 94, 96, 97, 99], "younger": 14, "your": [1, 2, 5, 10, 14, 27, 28, 31, 33, 39, 42, 43, 51, 53, 54, 55, 58, 64, 70, 73, 74, 77, 79, 80, 83, 84, 86, 87, 88, 92, 93, 94, 95, 97, 98, 99], "yourself": [14, 86], "ytick": 31, "yval": 31, "z": [3, 27], "zebra": 74, "zero": [8, 10, 20, 27, 31, 36, 39, 53], "zero_": 31, "zero_grad": [3, 4, 8, 42, 64, 84], "zip": [1, 10, 24, 36, 64, 84], "zip_path": [10, 64], "zip_ref": 64, "zipfil": 64, "zone": 86, "zsh": [42, 91], "\u00b5": [5, 31], "\u00fc": 64, "\u0121": 24, "\u0121world": 24, "\u03bc": 18, "\u03c6": 36, "\u03c9": 27, "\u0438\u0437\u0434\u0430sch\u00e9on\ub85c\u0436\u0430": 53, "\u0646\u062f\u0627\u0631": 54, "\u092e\u0932": 54, "\uc0ac\uc9c4": 54}, "titles": ["&lt;no title&gt;", "Build a Large Language Model (From Scratch)", "Appendix A: Introduction to PyTorch", "Appendix A: Introduction to PyTorch (Part 1)", "Appendix A: Introduction to PyTorch (Part 2)", "Exercise A.1", "Python and Environment Setup Recommendations", "Appendix A: Introduction to PyTorch", "Appendix D: Adding Bells and Whistles to the Training Loop", "Appendix D: Adding Bells and Whistles to the Training Loop", "Appendix E: Parameter-efficient Finetuning with LoRA", "Appendix E: Parameter-efficient Finetuning with LoRA", "Chapter 1: Understanding Large Language Models", "Chapter 2: Working with Text Data", "Chapter 2: Working with Text Data", "The Main Data Loading Pipeline Summarized", "Chapter 2 Exercise solutions", "Chapter 2: Working with Text Data", "Comparing Various Byte Pair Encoding (BPE) Implementations", "Chapter 2: Working with Text Data", "Understanding the Difference Between Embedding Layers and Linear Layers", "Chapter 2: Working with Text Data", "Data sampling with a sliding window with number data", "Byte Pair Encoding (BPE) Tokenizer From Scratch", "Byte Pair Encoding (BPE) Tokenizer From Scratch", "Chapter 2: Working with Text Data", "Chapter 3: Coding Attention Mechanisms", "Chapter 3: Coding Attention Mechanisms", "Chapter 3 Exercise solutions", "Multi-head Attention Plus Data Loading", "More Efficient Multi-Head Attention Implementations", "Comparing Efficient Multi-Head Attention Implementations", "Understanding PyTorch Buffers", "Understanding PyTorch Buffers", "Chapter 3: Coding Attention Mechanisms", "Chapter 4: Implementing a GPT Model from Scratch To Generate Text", "Chapter 4: Implementing a GPT model from Scratch To Generate Text", "Chapter 4 Exercise solutions", "Chapter 4: Implementing a GPT Model from Scratch To Generate Text", "FLOPS Analysis", "Chapter 4: Implementing a GPT Model from Scratch to Generate Text", "Chapter 5: Pretraining on Unlabeled Data", "Chapter 5: Pretraining on Unlabeled Data", "Chapter 5 Exercise solutions", "Alternative Approaches to Loading Pretrained Weights", "Bonus Code for Chapter 5", "Bonus Code for Chapter 5", "Bonus Code for Chapter 5", "Pretraining GPT on the Project Gutenberg Dataset", "Adding Bells and Whistles to the Training Loop", "Optimizing Hyperparameters for Pretraining", "Building a User Interface to Interact With the Pretrained LLM", "Converting GPT to Llama", "Converting a From-Scratch GPT Architecture to Llama 2", "Converting Llama 2 to Llama 3.2 From Scratch", "Llama 3.2 From Scratch (A Standalone Notebook)", "Llama 3.2 From Scratch (A Standalone Notebook)", "Memory-efficient Model Weight Loading", "Memory-efficient Model Weight Loading", "Extending the Tiktoken BPE Tokenizer with New Tokens", "Extending the Tiktoken BPE Tokenizer with New Tokens", "PyTorch Performance Tips for Faster LLM Training", "Chapter 5: Pretraining on Unlabeled Data", "Chapter 6: Finetuning for Classification", "Chapter 6: Finetuning for Text Classification", "Chapter 6 Exercise solutions", "Load And Use Finetuned Model", "Additional Classification Finetuning Experiments", "Additional Experiments Classifying the Sentiment of 50k IMDB Movie Reviews", "Scikit-learn Logistic Regression Model", "Building a User Interface to Interact With the GPT-based Spam Classifier", "Chapter 6: Finetuning for Classification", "Chapter 7: Finetuning to Follow Instructions", "Chapter 7: Finetuning To Follow Instructions", "Chapter 7 Exercise solutions", "Load And Use Finetuned Model", "Chapter 7: Finetuning to Follow Instructions", "Create \u201cPassive Voice\u201d Entries for an Instruction Dataset", "Chapter 7: Finetuning to Follow Instructions", "Evaluating Instruction Responses Locally Using a Llama 3 Model Via Ollama", "Evaluating Instruction Responses Using the OpenAI API", "Score Correlation Analysis", "Chapter 7: Finetuning to Follow Instructions", "Generating A Preference Dataset With Llama 3.1 70B And Ollama", "Direct Preference Optimization (DPO) for LLM Alignment (From Scratch)", "Generating Datasets for Instruction Finetuning", "Generating An Instruction Dataset via Llama 3 and Ollama", "Improving Instruction-Data Via Reflection-Tuning Using GPT-4", "Building a User Interface to Interact With the Instruction Finetuned GPT Model", "Chapter 7: Finetuning to Follow Instructions", "<code class=\"docutils literal notranslate\"><span class=\"pre\">llms-from-scratch</span></code> PyPI Package", "Python Setup Tips", "Native pixi Python and package management", "Native uv Python and package management", "Installing Python Packages and Libraries Used In This Book", "&lt;no title&gt;", "Optional Docker Environment", "Docker Environment Setup Guide", "AWS CloudFormation Template: Jupyter Notebook with LLMs-from-scratch Repo", "Optional Setup Instructions"], "titleterms": {"": [31, 53, 54, 55, 56, 73, 79, 83, 86], "1": [3, 4, 5, 8, 10, 12, 14, 16, 24, 27, 28, 31, 36, 37, 42, 43, 48, 51, 52, 53, 54, 55, 56, 58, 60, 61, 64, 65, 68, 70, 73, 74, 83, 84, 88, 91, 92, 93], "10": 61, "124": 68, "149": 68, "1b": 54, "2": [3, 4, 5, 8, 10, 13, 14, 16, 17, 18, 19, 21, 24, 25, 27, 28, 29, 31, 36, 37, 42, 43, 48, 51, 52, 53, 54, 55, 56, 58, 60, 61, 64, 65, 68, 70, 73, 74, 84, 88, 91, 93], "3": [3, 4, 5, 8, 10, 14, 24, 26, 27, 28, 29, 31, 34, 36, 37, 42, 43, 48, 52, 53, 54, 55, 56, 58, 60, 61, 64, 65, 68, 73, 74, 79, 81, 83, 84, 86, 91, 92, 93], "340": 68, "355": 68, "395": 68, "4": [3, 5, 8, 10, 14, 24, 27, 31, 35, 36, 37, 38, 40, 42, 43, 52, 53, 54, 55, 56, 58, 60, 61, 64, 68, 73, 74, 81, 84, 87, 91, 92], "5": [3, 14, 27, 31, 36, 41, 42, 43, 45, 46, 47, 52, 53, 54, 55, 56, 58, 61, 62, 64, 68, 73, 84, 91], "50k": 68, "6": [3, 14, 27, 31, 36, 43, 53, 58, 61, 63, 64, 65, 68, 71, 73, 84], "66": 68, "7": [3, 14, 31, 36, 53, 58, 61, 64, 68, 72, 73, 74, 76, 78, 82, 89], "70b": 83, "8": [3, 14, 31, 61, 64, 73], "8b": [54, 81], "9": [3, 4, 31, 61, 73], "A": [2, 3, 4, 5, 7, 18, 24, 27, 29, 55, 56, 83, 84], "And": [66, 75, 83], "In": 94, "Near": 76, "The": [8, 15, 24, 27, 31], "To": [35, 36, 38, 73], "With": [51, 70, 83, 88], "a100": 31, "accuraci": 64, "activ": [36, 53], "ad": [8, 9, 14, 36, 49, 60, 64], "adamw": 61, "add": 53, "addit": [27, 67, 68], "after": 30, "air": 31, "algorithm": 24, "align": 84, "all": 27, "alpaca": 74, "altern": [29, 31, 44, 45, 46, 47], "an": [31, 33, 36, 42, 73, 77, 86], "analysi": [39, 81], "analyz": 84, "api": [77, 78, 79, 80, 83, 86, 87], "app": [51, 70, 88], "appendix": [2, 3, 4, 7, 8, 9, 10, 11], "appli": 27, "approach": 44, "architectur": [36, 53, 55, 56], "attend": 27, "attent": [26, 27, 29, 30, 31, 34, 36, 37, 54], "automat": [3, 39], "aw": 98, "b": 29, "backward": [30, 31], "base": [68, 70], "baselin": [61, 68, 69], "batch": [39, 61, 73, 84], "behavior": 43, "behind": 24, "bell": [8, 9, 49], "benchmark": [18, 39, 58], "bert": 68, "between": 20, "bfloat16": 61, "bit": 24, "block": 36, "bonu": [1, 7, 12, 25, 34, 40, 45, 46, 47, 62, 71, 89], "book": [91, 94], "bpe": [18, 23, 24, 59, 60], "brief": 84, "buffer": [32, 33], "build": [1, 24, 51, 70, 88], "byte": [18, 23, 24], "bytepair": 14, "calcul": [42, 64], "captur": 27, "categori": 64, "causal": [27, 61], "causalattent": 31, "chang": 74, "chapter": [2, 7, 12, 13, 14, 16, 17, 19, 21, 25, 26, 27, 28, 29, 31, 34, 35, 36, 37, 38, 40, 41, 42, 43, 45, 46, 47, 62, 63, 64, 65, 71, 72, 73, 74, 76, 78, 82, 89], "choos": 47, "citat": 1, "class": [27, 31, 53, 54, 61, 84], "classif": [63, 64, 67, 71], "classifi": [64, 68, 70], "clip": 8, "cloud": 99, "cloudform": 98, "code": [2, 7, 12, 13, 25, 26, 27, 34, 35, 36, 40, 41, 45, 46, 47, 48, 51, 55, 56, 61, 62, 63, 70, 71, 72, 84, 88, 89, 92, 93, 97, 99], "coeffici": 81, "colab": 99, "combin": 31, "common": 3, "compact": 27, "compar": [18, 31], "comparison": [31, 61], "compil": [30, 31, 61], "compon": 54, "comput": [3, 4, 27], "conclus": [24, 73], "concret": 24, "conda": 91, "connect": 36, "consid": 74, "content": 1, "context": [14, 65], "continu": 43, "contribut": 1, "control": 42, "convert": [14, 52, 53, 54], "core": 61, "correl": 81, "cosin": 8, "cpu": [31, 58], "creat": [14, 61, 64, 73, 76, 77, 84, 87, 91], "cross": 42, "d": [8, 9], "data": [3, 13, 14, 15, 17, 19, 21, 22, 25, 27, 29, 41, 42, 61, 62, 64, 73, 84, 87], "dataset": [10, 48, 64, 68, 73, 74, 77, 83, 84, 85, 86, 87], "date": 73, "decai": 8, "decis": 48, "decod": [24, 42, 43], "defin": 54, "depend": [27, 51, 68, 70, 88, 92, 93], "design": 48, "determinist": 43, "devcontain": [97, 99], "develop": 84, "devic": [3, 4], "dict": 47, "differ": [20, 27, 43, 64], "differenti": 3, "direct": 84, "distilbert": 68, "docker": [96, 97, 99], "doe": 98, "dot": 31, "download": [47, 48, 52, 68, 79, 83, 86, 91, 97], "dpo": 84, "dropout": [27, 37], "duplic": 76, "e": [10, 11], "easi": 3, "edit": 90, "editor": 99, "educ": 18, "effici": [3, 10, 11, 30, 31, 57, 58, 74], "einsum": 31, "embed": [14, 20, 60], "encod": [14, 18, 23, 24], "entri": [76, 77, 79, 80, 83, 87], "entropi": 42, "environ": [6, 91, 93, 96, 97], "evalu": [42, 73, 78, 79, 80], "exampl": [24, 33], "exercis": [5, 16, 28, 37, 43, 65, 74], "experi": [67, 68], "extend": [27, 59, 60], "extens": 99, "extract": [73, 86], "face": [18, 45, 46], "fast": 73, "faster": 61, "feed": [36, 37], "feedback": 1, "feedforward": 53, "field": 73, "file": 47, "final": 73, "find": [39, 76], "finetun": [10, 11, 53, 54, 63, 64, 65, 66, 67, 71, 72, 73, 74, 75, 76, 78, 82, 84, 85, 88, 89], "first": 65, "fix": 39, "flashattent": [31, 61], "flexattent": 31, "flop": 39, "fly": 61, "follow": [72, 73, 76, 78, 82, 89], "forward": [30, 31, 36, 37], "from": [1, 18, 23, 24, 29, 31, 35, 36, 38, 40, 42, 45, 46, 47, 52, 53, 54, 55, 56, 61, 84, 90, 98], "function": [8, 42, 43, 84], "fuse": 61, "futur": 27, "gelu": [36, 53], "gener": [27, 35, 36, 38, 40, 42, 47, 52, 55, 56, 83, 85, 86], "github": 90, "googl": 99, "gpt": [18, 24, 35, 36, 37, 38, 40, 42, 48, 52, 53, 60, 68, 70, 81, 87, 88], "gpu": [3, 4, 31, 61], "gradient": 8, "graph": 3, "group": 54, "guid": 97, "gutenberg": 48, "hardwar": 1, "head": [27, 29, 30, 31, 64], "hide": 27, "how": [48, 98], "hub": [45, 46], "hug": [18, 45, 46], "hyperparamet": 50, "i": 3, "id": 14, "idea": 24, "imag": 97, "imdb": 68, "implement": [3, 18, 24, 27, 29, 30, 31, 35, 36, 38, 40, 53, 54], "improv": [48, 87], "increas": [61, 65], "initi": [10, 37, 52, 53, 54, 55, 56, 64], "input": [27, 74, 83], "instal": [51, 52, 68, 70, 79, 83, 86, 88, 90, 91, 92, 93, 94, 97], "instruct": [48, 53, 54, 72, 73, 74, 76, 77, 78, 79, 80, 82, 85, 86, 87, 88, 89, 99], "interact": [51, 70, 88], "interfac": [51, 70, 88], "interpret": 67, "introduct": [2, 3, 4, 7, 10, 73, 84], "json": [77, 79, 80, 83, 87], "jupyt": 98, "k": [42, 43], "kei": 98, "kendal": 81, "languag": [1, 12], "larg": [1, 12, 68], "larger": [37, 43], "last": 65, "layer": [20, 27, 36, 53, 60], "layernorm": 53, "learn": [8, 69], "length": 65, "librari": [91, 94], "lightn": 99, "linear": [20, 36], "linux": 48, "llama": [52, 53, 54, 55, 56, 79, 81, 83, 86], "llm": [36, 42, 51, 52, 60, 61, 64, 73, 84, 90, 98], "load": [3, 15, 24, 29, 42, 44, 45, 46, 47, 52, 53, 54, 55, 56, 57, 58, 60, 66, 73, 75, 79, 80, 83, 84, 87], "loader": [3, 29, 61, 64, 73, 84], "local": [78, 79, 99], "logist": [68, 69], "long": 27, "loop": [3, 8, 9, 49], "lora": [10, 11, 74], "loss": [42, 43, 64, 84], "low": 58, "m": 68, "m3": 31, "macbook": 31, "maco": 48, "made": 3, "main": [2, 7, 12, 13, 15, 24, 25, 26, 34, 35, 40, 41, 62, 63, 71, 72, 89], "manag": [92, 93], "manual": 93, "mask": [27, 61, 74], "materi": [1, 7, 12, 25, 34, 40, 62, 71, 89], "matric": 3, "mechan": [26, 27, 34], "memori": [57, 58, 61], "method": 58, "mfu": 39, "mha": 31, "miniforg": 91, "mmap": 58, "model": [1, 3, 10, 12, 27, 35, 36, 37, 38, 39, 40, 42, 43, 45, 46, 47, 52, 53, 54, 55, 56, 57, 58, 60, 64, 65, 66, 68, 69, 75, 79, 84, 88], "modernbert": 68, "modifi": [8, 42, 54], "modul": [37, 53, 54], "more": 30, "move": 73, "movi": 68, "multi": [27, 29, 30, 31, 61], "multiheadattent": [31, 53], "multilay": 3, "multipl": [3, 4, 27], "my": 18, "nativ": [92, 93], "network": [3, 36], "neural": 3, "new": [59, 60, 91], "next": [53, 54, 55, 56, 73], "nn": [20, 31], "normal": 36, "notebook": [55, 56, 98], "number": 22, "nvidia": 31, "ollama": [78, 79, 83, 86], "onli": [30, 31], "openai": [18, 24, 42, 77, 78, 80, 87], "oper": 3, "optim": [3, 4, 50, 61, 84], "option": [2, 13, 26, 35, 41, 63, 72, 91, 93, 96, 99], "organ": 73, "origin": [18, 24, 74], "other": 58, "outlin": 24, "output": [60, 83], "overview": 68, "own": 18, "packag": [52, 90, 91, 92, 93, 94], "pad": 61, "pair": [18, 23, 24], "paramet": [10, 11, 37, 74], "part": [3, 4, 24, 27], "pass": [30, 31], "passiv": [76, 77], "pearson": 81, "perform": [3, 4, 18, 61], "perplex": 42, "pin": 61, "pipelin": 15, "pixi": 92, "plu": 29, "point": 98, "posit": 14, "precis": 61, "prefer": [83, 84], "preferencedataset": 84, "prepar": [10, 48, 64, 73, 84], "pretrain": [41, 42, 43, 44, 48, 50, 51, 53, 54, 55, 56, 60, 62, 64, 73], "probabl": 43, "problem": 27, "process": 84, "product": 31, "project": 48, "prompt": 74, "purpos": 18, "pypi": 90, "python": [6, 91, 92, 93, 94, 99], "pytorch": [2, 3, 4, 7, 31, 32, 33, 42, 47, 61, 91, 94], "queri": 54, "question": [1, 99], "quick": [18, 31], "quickstart": 99, "random": 42, "rate": 8, "recommend": 6, "recommmend": 58, "reflect": 87, "regress": [68, 69], "replac": [53, 61], "repo": 98, "repositori": 1, "requir": 1, "resourc": 99, "respons": [73, 78, 79, 80, 86, 87], "rest": [79, 83, 86], "result": 84, "reus": 54, "review": 68, "rmsnorm": 53, "roberta": 68, "rope": [53, 54], "run": [48, 51, 68, 70, 88, 92, 93], "safetensor": [45, 47], "sampl": [14, 22, 42, 43], "save": [3, 24, 42, 73], "scalar": 3, "scale": [31, 42, 43], "scaled_dot_product_attent": 31, "scikit": 69, "score": [43, 81], "scratch": [1, 18, 23, 24, 35, 36, 38, 40, 52, 53, 54, 55, 56, 61, 84, 90, 98], "script": 48, "section": 24, "see": 3, "self": 27, "selfattent": 27, "sentiment": 68, "separ": 37, "sequenc": 27, "sequenti": 58, "set": [3, 42, 43, 52, 84, 99], "setup": [6, 58, 91, 97, 99], "shortcut": 36, "silu": 53, "simpl": [24, 27, 29, 39], "singl": [3, 4, 27, 61], "size": [39, 61], "slide": [14, 22], "softmax": 43, "solut": [16, 28, 37, 43, 65, 74], "spam": [64, 70], "spearman": 81, "special": [14, 48, 60, 74], "speed": [31, 61], "split": [27, 84], "stack": 27, "stai": 73, "standalon": [55, 56], "state": 47, "state_dict": 33, "step": [24, 27, 51, 53, 54, 68, 70, 88], "strategi": 42, "studio": [97, 99], "style": [74, 91], "summar": 15, "summari": [14, 27, 30, 36, 42, 64, 73], "supervis": [64, 73], "tabl": 1, "takeawai": [14, 27, 36, 42, 64, 73], "tau": 81, "temperatur": [42, 43], "templat": 98, "tensor": [3, 61], "termin": 91, "test": [77, 80, 84, 87], "text": [13, 14, 17, 19, 21, 25, 35, 36, 38, 40, 42, 47, 52, 55, 56, 64], "thi": [1, 48, 91, 94], "tiktoken": [18, 59, 60], "tip": [61, 74, 91], "token": [14, 18, 23, 24, 27, 52, 53, 54, 55, 56, 59, 60, 65, 74], "top": [42, 43], "torch": 31, "train": [3, 4, 8, 9, 24, 42, 43, 49, 61, 73, 84], "trainabl": 27, "transform": [18, 36, 46], "transformerblock": [53, 54], "true": 58, "try": 43, "tune": 87, "type": 3, "typic": 3, "understand": [3, 12, 14, 20, 32, 33], "uninstal": 97, "unlabel": [41, 42, 62], "up": [3, 73, 99], "updat": [53, 54, 60], "us": [18, 20, 31, 37, 42, 46, 48, 52, 53, 54, 58, 60, 61, 64, 66, 75, 78, 79, 80, 83, 86, 87, 90, 91, 94, 97, 98, 99], "usag": 67, "user": [48, 51, 70, 88], "util": [39, 58], "uv": [91, 93], "v": 81, "valid": [42, 43, 84], "variant": 29, "variou": 18, "vector": 3, "versu": [37, 65], "via": [18, 45, 52, 79, 86, 87], "virtual": [91, 93], "visual": [31, 97, 99], "vocabulari": [24, 61], "voic": [76, 77], "vscode": 99, "walkthrough": 24, "warmup": [8, 31], "weight": [27, 31, 42, 44, 45, 46, 47, 52, 53, 54, 55, 56, 57, 58, 64], "what": [3, 53, 54, 55, 56, 73, 98], "whistl": [8, 9, 49], "whole": 65, "window": [14, 22, 48], "without": [27, 31, 33], "word": [14, 27, 73], "work": [13, 14, 17, 19, 21, 25], "wrapper": 31, "your": 91}})