
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Converting Llama 2 to Llama 3.2 From Scratch &#8212; LLMs from Scratch</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'ch05/07_gpt_to_llama/converting-llama2-to-llama3';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Llama 3.2 From Scratch (A Standalone Notebook)" href="standalone-llama32.html" />
    <link rel="prev" title="Converting a From-Scratch GPT Architecture to Llama 2" href="converting-gpt-to-llama2.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../README.html">
  
  
  
  
  
  
    <p class="title logo__title">LLMs from Scratch</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../README.html">
                    Build a Large Language Model (From Scratch)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Setup</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../setup/README.html">Optional Setup Instructions</a></li>



</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapters</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../ch01/README.html">Chapter 1: Understanding Large Language Models</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../ch02/README.html">Chapter 2: Working with Text Data</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../ch02/01_main-chapter-code/README.html">Chapter 2: Working with Text Data</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../ch02/01_main-chapter-code/ch02.html">Chapter 2: Working with Text Data</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../ch02/01_main-chapter-code/dataloader.html">The Main Data Loading Pipeline Summarized</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ch02/01_main-chapter-code/exercise-solutions.html">Chapter 2 Exercise solutions</a></li>


</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../ch02/02_bonus_bytepair-encoder/README.html">Chapter 2: Working with Text Data</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../ch02/02_bonus_bytepair-encoder/compare-bpe-tiktoken.html">Comparing Various Byte Pair Encoding (BPE) Implementations</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../ch02/03_bonus_embedding-vs-matmul/README.html">Chapter 2: Working with Text Data</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../ch02/03_bonus_embedding-vs-matmul/embeddings-and-linear-layers.html">Understanding the Difference Between Embedding Layers and Linear Layers</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../ch02/04_bonus_dataloader-intuition/README.html">Chapter 2: Working with Text Data</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../ch02/04_bonus_dataloader-intuition/dataloader-intuition.html">Data sampling with a sliding window with number data</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../ch02/05_bpe-from-scratch/README.html">Byte Pair Encoding (BPE) Tokenizer From Scratch</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../ch02/05_bpe-from-scratch/bpe-from-scratch.html">Byte Pair Encoding (BPE) Tokenizer From Scratch</a></li>


</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../ch03/README.html">Chapter 3: Coding Attention Mechanisms</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../ch03/01_main-chapter-code/README.html">Chapter 3: Coding Attention Mechanisms</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../ch03/01_main-chapter-code/ch03.html">Chapter 3: Coding Attention Mechanisms</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../ch03/01_main-chapter-code/multihead-attention.html">Multi-head Attention Plus Data Loading</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../ch03/01_main-chapter-code/exercise-solutions.html">Chapter 3 Exercise solutions</a></li>



</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../ch03/02_bonus_efficient-multihead-attention/README.html">More Efficient Multi-Head Attention Implementations</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../ch03/02_bonus_efficient-multihead-attention/mha-implementations.html">Comparing Efficient Multi-Head Attention Implementations</a></li>

</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../ch03/03_understanding-buffers/README.html">Understanding PyTorch Buffers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../ch03/03_understanding-buffers/understanding-buffers.html">Understanding PyTorch Buffers</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../ch04/README.html">Chapter 4: Implementing a GPT Model from Scratch to Generate Text</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../ch04/01_main-chapter-code/README.html">Chapter 4: Implementing a GPT Model from Scratch To Generate Text</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../ch04/01_main-chapter-code/ch04.html">Chapter 4: Implementing a GPT model from Scratch To Generate Text</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ch04/01_main-chapter-code/exercise-solutions.html">Chapter 4 Exercise solutions</a></li>



</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../ch04/02_performance-analysis/README.html">Chapter 4: Implementing a GPT Model from Scratch To Generate Text</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../ch04/02_performance-analysis/flops-analysis.html">FLOPS Analysis</a></li>



</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../README.html">Chapter 5: Pretraining on Unlabeled Data</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 has-children"><a class="reference internal" href="../01_main-chapter-code/README.html">Chapter 5: Pretraining on Unlabeled Data</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../01_main-chapter-code/ch05.html">Chapter 5: Pretraining on Unlabeled Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../01_main-chapter-code/exercise-solutions.html">Chapter 5 Exercise solutions</a></li>






</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../02_alternative_weight_loading/README.html">Alternative Approaches to Loading Pretrained Weights</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../02_alternative_weight_loading/weight-loading-hf-safetensors.html">Bonus Code for Chapter 5</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02_alternative_weight_loading/weight-loading-hf-transformers.html">Bonus Code for Chapter 5</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02_alternative_weight_loading/weight-loading-pytorch.html">Bonus Code for Chapter 5</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../03_bonus_pretraining_on_gutenberg/README.html">Pretraining GPT on the Project Gutenberg Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04_learning_rate_schedulers/README.html">Adding Bells and Whistles to the Training Loop</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05_bonus_hparam_tuning/README.html">Optimizing Hyperparameters for Pretraining</a></li>
<li class="toctree-l2"><a class="reference internal" href="../06_user_interface/README.html">Building a User Interface to Interact With the Pretrained LLM</a></li>
<li class="toctree-l2 current active has-children"><a class="reference internal" href="README.html">Converting GPT to Llama</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="converting-gpt-to-llama2.html">Converting a From-Scratch GPT Architecture to Llama 2</a></li>


<li class="toctree-l3 current active"><a class="current reference internal" href="#">Converting Llama 2 to Llama 3.2 From Scratch</a></li>




<li class="toctree-l3"><a class="reference internal" href="standalone-llama32.html">Llama 3.2 From Scratch (A Standalone Notebook)</a></li>






<li class="toctree-l3"><a class="reference internal" href="standalone-llama32-mem-opt.html">Llama 3.2 From Scratch (A Standalone Notebook)</a></li>






</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../08_memory_efficient_weight_loading/README.html">Memory-efficient Model Weight Loading</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../08_memory_efficient_weight_loading/memory-efficient-state-dict.html">Memory-efficient Model Weight Loading</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../09_extending-tokenizers/README.html">Extending the Tiktoken BPE Tokenizer with New Tokens</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../09_extending-tokenizers/extend-tiktoken.html">Extending the Tiktoken BPE Tokenizer with New Tokens</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../10_llm-training-speed/README.html">PyTorch Performance Tips for Faster LLM Training</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../ch06/README.html">Chapter 6: Finetuning for Classification</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../ch06/01_main-chapter-code/README.html">Chapter 6: Finetuning for Classification</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../ch06/01_main-chapter-code/ch06.html">Chapter 6: Finetuning for Text Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ch06/01_main-chapter-code/load-finetuned-model.html">Load And Use Finetuned Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ch06/01_main-chapter-code/exercise-solutions.html">Chapter 6 Exercise solutions</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../ch06/02_bonus_additional-experiments/README.html">Additional Classification Finetuning Experiments</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../ch06/03_bonus_imdb-classification/README.html">Additional Experiments Classifying the Sentiment of 50k IMDB Movie Reviews</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../ch06/03_bonus_imdb-classification/sklearn-baseline.html">Scikit-learn Logistic Regression Model</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../ch06/04_user_interface/README.html">Building a User Interface to Interact With the GPT-based Spam Classifier</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../ch07/README.html">Chapter 7: Finetuning to Follow Instructions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../ch07/01_main-chapter-code/README.html">Chapter 7: Finetuning to Follow Instructions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../ch07/01_main-chapter-code/ch07.html">Chapter 7: Finetuning To Follow Instructions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ch07/01_main-chapter-code/load-finetuned-model.html">Load And Use Finetuned Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ch07/01_main-chapter-code/exercise-solutions.html">Chapter 7 Exercise solutions</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../ch07/02_dataset-utilities/README.html">Chapter 7: Finetuning to Follow Instructions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../ch07/02_dataset-utilities/create-passive-voice-entries.html">Create “Passive Voice” Entries for an Instruction Dataset</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../ch07/03_model-evaluation/README.html">Chapter 7: Finetuning to Follow Instructions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../ch07/03_model-evaluation/llm-instruction-eval-ollama.html">Evaluating Instruction Responses Locally Using a Llama 3 Model Via Ollama</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ch07/03_model-evaluation/llm-instruction-eval-openai.html">Evaluating Instruction Responses Using the OpenAI API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ch07/03_model-evaluation/scores/correlation-analysis.html">Score Correlation Analysis</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../ch07/04_preference-tuning-with-dpo/README.html">Chapter 7: Finetuning to Follow Instructions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../ch07/04_preference-tuning-with-dpo/create-preference-data-ollama.html">Generating A Preference Dataset With Llama 3.1 70B And Ollama</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ch07/04_preference-tuning-with-dpo/dpo-from-scratch.html">Direct Preference Optimization (DPO) for LLM Alignment (From Scratch)</a></li>






</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../ch07/05_dataset-generation/README.html">Generating Datasets for Instruction Finetuning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../ch07/05_dataset-generation/llama3-ollama.html">Generating An Instruction Dataset via Llama 3 and Ollama</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ch07/05_dataset-generation/reflection-gpt4.html">Improving Instruction-Data Via Reflection-Tuning Using GPT-4</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../ch07/06_user_interface/README.html">Building a User Interface to Interact With the Instruction Finetuned GPT Model</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendices</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../appendix-A/README.html">Appendix A: Introduction to PyTorch</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../appendix-A/01_main-chapter-code/README.html">Appendix A: Introduction to PyTorch</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../appendix-A/01_main-chapter-code/code-part1.html">Appendix A: Introduction to PyTorch (Part 1)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../appendix-A/01_main-chapter-code/code-part2.html">Appendix A: Introduction to PyTorch (Part 2)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../appendix-A/01_main-chapter-code/exercise-solutions.html">Exercise A.1</a></li>



</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../appendix-A/02_setup-recommendations/README.html">Python and Environment Setup Recommendations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../appendix-D/README.html">Appendix D: Adding Bells and Whistles to the Training Loop</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../appendix-D/01_main-chapter-code/appendix-D.html">Appendix D: Adding Bells and Whistles to the Training Loop</a></li>




</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../appendix-E/README.html">Appendix E: Parameter-efficient Finetuning with LoRA</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../appendix-E/01_main-chapter-code/appendix-E.html">Appendix E: Parameter-efficient Finetuning with LoRA</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/rasbt/llms-from-scratch/main?urlpath=lab/tree/ch05/07_gpt_to_llama/converting-llama2-to-llama3.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="../../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/rasbt/llms-from-scratch/blob/main/ch05/07_gpt_to_llama/converting-llama2-to-llama3.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/rasbt/llms-from-scratch" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/rasbt/llms-from-scratch/edit/main/ch05/07_gpt_to_llama/converting-llama2-to-llama3.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/rasbt/llms-from-scratch/issues/new?title=Issue%20on%20page%20%2Fch05/07_gpt_to_llama/converting-llama2-to-llama3.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/ch05/07_gpt_to_llama/converting-llama2-to-llama3.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Converting Llama 2 to Llama 3.2 From Scratch</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Converting Llama 2 to Llama 3.2 From Scratch</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#convert-the-llama-model-implementation-step-by-step">1. Convert the Llama model implementation step by step</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reusing-llama-2-components">1.1 Reusing Llama 2 components</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modified-rope">1.2 Modified RoPE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#grouped-query-attention">1.3 Grouped-query attention</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#update-the-transformerblock-module">1.4 Update the TransformerBlock module</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-the-model-class">1.5 Defining the model class</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#initialize-model">2. Initialize model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#load-tokenizer">3. Load tokenizer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#load-pretrained-weights">4. Load pretrained weights</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-the-instruction-finetuned-model">5. Using the instruction-finetuned model</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#llama-3-1-8b">Llama 3.1 8B</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#llama-3-2-1b">Llama 3.2 1B</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#what-s-next">What’s next?</a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <table style="width:100%">
<tr>
<td style="vertical-align:middle; text-align:left;">
<font size="2">
Supplementary code for the <a href="http://mng.bz/orYv">Build a Large Language Model From Scratch</a> book by <a href="https://sebastianraschka.com">Sebastian Raschka</a><br>
<br>Code repository: <a href="https://github.com/rasbt/LLMs-from-scratch">https://github.com/rasbt/LLMs-from-scratch</a>
</font>
</td>
<td style="vertical-align:middle; text-align:left;">
<a href="http://mng.bz/orYv"><img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp" width="100px"></a>
</td>
</tr>
</table><section class="tex2jax_ignore mathjax_ignore" id="converting-llama-2-to-llama-3-2-from-scratch">
<h1>Converting Llama 2 to Llama 3.2 From Scratch<a class="headerlink" href="#converting-llama-2-to-llama-3-2-from-scratch" title="Link to this heading">#</a></h1>
<ul class="simple">
<li><p>This is a follow-up notebook to <a class="reference internal" href="converting-gpt-to-llama2.html"><span class="std std-doc">Converting a From-Scratch GPT Architecture to Llama 2</span></a>, converting Meta AI’s Llama 2 architecture model step by step to Llama 3, Llama 3.1, and Llama 3.2</p></li>
<li><p>The explanations are purposefully kept minimal in this notebook so as not to bloat it unnecessarily and focus on the main code</p></li>
<li><p>For more information about the architectures, please see the Llama 2 and Llama 3 papers</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2307.09288">Llama 2: Open Foundation and Fine-Tuned Chat Models (2023)</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2407.21783">The Llama 3 Herd of Models</a></p></li>
</ul>
<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/gpt-to-llama/gpt2-to-llama2-llama3.webp?1"><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># pip install -r requirements-extra.txt</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Packages that are being used in this notebook:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">importlib.metadata</span><span class="w"> </span><span class="kn">import</span> <span class="n">version</span>

<span class="n">pkgs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;blobfile&quot;</span><span class="p">,</span>         <span class="c1"># to download pretrained weights</span>
    <span class="s2">&quot;huggingface_hub&quot;</span><span class="p">,</span>  <span class="c1"># to download pretrained weights</span>
    <span class="s2">&quot;tiktoken&quot;</span><span class="p">,</span>         <span class="c1"># to implement the tokenizer</span>
    <span class="s2">&quot;torch&quot;</span><span class="p">,</span>            <span class="c1"># to implement the model</span>
<span class="p">]</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">pkgs</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">p</span><span class="si">}</span><span class="s2"> version: </span><span class="si">{</span><span class="n">version</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">PackageNotFoundError</span><span class="g g-Whitespace">                      </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">line</span> <span class="mi">10</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="n">pkgs</span> <span class="o">=</span> <span class="p">[</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span>     <span class="s2">&quot;blobfile&quot;</span><span class="p">,</span>         <span class="c1"># to download pretrained weights</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span>     <span class="s2">&quot;huggingface_hub&quot;</span><span class="p">,</span>  <span class="c1"># to download pretrained weights</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span>     <span class="s2">&quot;tiktoken&quot;</span><span class="p">,</span>         <span class="c1"># to implement the tokenizer</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span>     <span class="s2">&quot;torch&quot;</span><span class="p">,</span>            <span class="c1"># to implement the model</span>
<span class="g g-Whitespace">      </span><span class="mi">8</span> <span class="p">]</span>
<span class="g g-Whitespace">      </span><span class="mi">9</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">pkgs</span><span class="p">:</span>
<span class="ne">---&gt; </span><span class="mi">10</span>     <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">p</span><span class="si">}</span><span class="s2"> version: </span><span class="si">{</span><span class="n">version</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nn">File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/importlib/metadata/__init__.py:946,</span> in <span class="ni">version</span><span class="nt">(distribution_name)</span>
<span class="g g-Whitespace">    </span><span class="mi">939</span> <span class="k">def</span><span class="w"> </span><span class="nf">version</span><span class="p">(</span><span class="n">distribution_name</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">940</span><span class="w">     </span><span class="sd">&quot;&quot;&quot;Get the version string for the named package.</span>
<span class="g g-Whitespace">    </span><span class="mi">941</span><span class="sd"> </span>
<span class="g g-Whitespace">    </span><span class="mi">942</span><span class="sd">     :param distribution_name: The name of the distribution package to query.</span>
<span class="g g-Whitespace">    </span><span class="mi">943</span><span class="sd">     :return: The version string for the package as defined in the package&#39;s</span>
<span class="g g-Whitespace">    </span><span class="mi">944</span><span class="sd">         &quot;Version&quot; metadata key.</span>
<span class="g g-Whitespace">    </span><span class="mi">945</span><span class="sd">     &quot;&quot;&quot;</span>
<span class="ne">--&gt; </span><span class="mi">946</span>     <span class="k">return</span> <span class="n">distribution</span><span class="p">(</span><span class="n">distribution_name</span><span class="p">)</span><span class="o">.</span><span class="n">version</span>

<span class="nn">File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/importlib/metadata/__init__.py:919,</span> in <span class="ni">distribution</span><span class="nt">(distribution_name)</span>
<span class="g g-Whitespace">    </span><span class="mi">913</span> <span class="k">def</span><span class="w"> </span><span class="nf">distribution</span><span class="p">(</span><span class="n">distribution_name</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">914</span><span class="w">     </span><span class="sd">&quot;&quot;&quot;Get the ``Distribution`` instance for the named package.</span>
<span class="g g-Whitespace">    </span><span class="mi">915</span><span class="sd"> </span>
<span class="g g-Whitespace">    </span><span class="mi">916</span><span class="sd">     :param distribution_name: The name of the distribution package as a string.</span>
<span class="g g-Whitespace">    </span><span class="mi">917</span><span class="sd">     :return: A ``Distribution`` instance (or subclass thereof).</span>
<span class="g g-Whitespace">    </span><span class="mi">918</span><span class="sd">     &quot;&quot;&quot;</span>
<span class="ne">--&gt; </span><span class="mi">919</span>     <span class="k">return</span> <span class="n">Distribution</span><span class="o">.</span><span class="n">from_name</span><span class="p">(</span><span class="n">distribution_name</span><span class="p">)</span>

<span class="nn">File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/importlib/metadata/__init__.py:518,</span> in <span class="ni">Distribution.from_name</span><span class="nt">(cls, name)</span>
<span class="g g-Whitespace">    </span><span class="mi">516</span>         <span class="k">return</span> <span class="n">dist</span>
<span class="g g-Whitespace">    </span><span class="mi">517</span> <span class="k">else</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">518</span>     <span class="k">raise</span> <span class="n">PackageNotFoundError</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

<span class="ne">PackageNotFoundError</span>: No package metadata was found for blobfile
</pre></div>
</div>
</div>
</div>
<p> </p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="convert-the-llama-model-implementation-step-by-step">
<h1>1. Convert the Llama model implementation step by step<a class="headerlink" href="#convert-the-llama-model-implementation-step-by-step" title="Link to this heading">#</a></h1>
<ul class="simple">
<li><p>If you are new to implementing LLM architectures, I recommend starting with <a class="reference internal" href="../../ch04/01_main-chapter-code/ch04.html"><span class="std std-doc">chapter 4</span></a>, which walks you through the implementation of the original GPT architecture step by step</p></li>
<li><p>The <a class="reference internal" href="converting-gpt-to-llama2.html"><span class="std std-doc">Converting a From-Scratch GPT Architecture to Llama 2</span></a> then implements the Llama-specific components, such as RMSNorm layers, SiLU and SwiGLU activations, RoPE (rotary position embeddings), and the SentencePiece tokenizer</p></li>
<li><p>This notebook takes the Llama 2 architecture and transforms it into Llama 3 architecture by</p>
<ol class="arabic simple">
<li><p>modifying the rotary embeddings</p></li>
<li><p>implementing grouped-query attention</p></li>
<li><p>and using a customized version of the GPT-4 tokenizer</p></li>
</ol>
</li>
<li><p>Later, we then load the original Llama 3 weights shared by Meta AI into the architecture</p></li>
</ul>
<p> </p>
<section id="reusing-llama-2-components">
<h2>1.1 Reusing Llama 2 components<a class="headerlink" href="#reusing-llama-2-components" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Llama 2 is actually quite similar to Llama 3, as mentioned above and illustrated in the figure at the top of this notebook</p></li>
<li><p>This means that we can import several building blocks from the <a class="reference internal" href="converting-gpt-to-llama2.html"><span class="std std-doc">Llama 2 notebook</span></a> using the following code</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">io</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">nbformat</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">types</span>

<span class="k">def</span><span class="w"> </span><span class="nf">import_from_notebook</span><span class="p">():</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">import_definitions_from_notebook</span><span class="p">(</span><span class="n">fullname</span><span class="p">,</span> <span class="n">names</span><span class="p">):</span>
        <span class="n">current_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">()</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">current_dir</span><span class="p">,</span> <span class="n">fullname</span> <span class="o">+</span> <span class="s2">&quot;.ipynb&quot;</span><span class="p">)</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">normpath</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>

        <span class="c1"># Load the notebook</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">FileNotFoundError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Notebook file not found at: </span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">io</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">nb</span> <span class="o">=</span> <span class="n">nbformat</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">as_version</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

        <span class="c1"># Create a module to store the imported functions and classes</span>
        <span class="n">mod</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">ModuleType</span><span class="p">(</span><span class="n">fullname</span><span class="p">)</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="n">fullname</span><span class="p">]</span> <span class="o">=</span> <span class="n">mod</span>

        <span class="c1"># Go through the notebook cells and only execute function or class definitions</span>
        <span class="k">for</span> <span class="n">cell</span> <span class="ow">in</span> <span class="n">nb</span><span class="o">.</span><span class="n">cells</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">cell</span><span class="o">.</span><span class="n">cell_type</span> <span class="o">==</span> <span class="s2">&quot;code&quot;</span><span class="p">:</span>
                <span class="n">cell_code</span> <span class="o">=</span> <span class="n">cell</span><span class="o">.</span><span class="n">source</span>
                <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">names</span><span class="p">:</span>
                    <span class="c1"># Check for function or class definitions</span>
                    <span class="k">if</span> <span class="sa">f</span><span class="s2">&quot;def </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span> <span class="ow">in</span> <span class="n">cell_code</span> <span class="ow">or</span> <span class="sa">f</span><span class="s2">&quot;class </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span> <span class="ow">in</span> <span class="n">cell_code</span><span class="p">:</span>
                        <span class="n">exec</span><span class="p">(</span><span class="n">cell_code</span><span class="p">,</span> <span class="n">mod</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mod</span>

    <span class="n">fullname</span> <span class="o">=</span> <span class="s2">&quot;converting-gpt-to-llama2&quot;</span>
    <span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;precompute_rope_params&quot;</span><span class="p">,</span> <span class="s2">&quot;compute_rope&quot;</span><span class="p">,</span> <span class="s2">&quot;SiLU&quot;</span><span class="p">,</span> <span class="s2">&quot;FeedForward&quot;</span><span class="p">,</span> <span class="s2">&quot;RMSNorm&quot;</span><span class="p">,</span> <span class="s2">&quot;MultiHeadAttention&quot;</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">import_definitions_from_notebook</span><span class="p">(</span><span class="n">fullname</span><span class="p">,</span> <span class="n">names</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">imported_module</span> <span class="o">=</span> <span class="n">import_from_notebook</span><span class="p">()</span>

<span class="c1"># We need to redefine precompute_rope_params</span>
<span class="c1"># precompute_rope_params = getattr(imported_module, &quot;precompute_rope_params&quot;, None)</span>
<span class="n">compute_rope</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">imported_module</span><span class="p">,</span> <span class="s2">&quot;compute_rope&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">SiLU</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">imported_module</span><span class="p">,</span> <span class="s2">&quot;SiLU&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">FeedForward</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">imported_module</span><span class="p">,</span> <span class="s2">&quot;FeedForward&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">RMSNorm</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">imported_module</span><span class="p">,</span> <span class="s2">&quot;RMSNorm&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

<span class="c1"># MultiHeadAttention only for comparison purposes</span>
<span class="n">MultiHeadAttention</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">imported_module</span><span class="p">,</span> <span class="s2">&quot;MultiHeadAttention&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p> </p>
</section>
<section id="modified-rope">
<h2>1.2 Modified RoPE<a class="headerlink" href="#modified-rope" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Llama 3 uses rotary position embeddings (RoPE) similar to Llama 2 (for a detailed explanation, please see the <a class="reference external" href="https://arxiv.org/abs/2104.09864">RoPE paper</a>)</p></li>
<li><p>There are some subtle differences in the RoPE settings, though</p></li>
<li><p>Llama 3 now supports up to 8,192 tokens, twice as many as Llama 2 (4,096)</p></li>
<li><p>The base value for the so-called RoPE <span class="math notranslate nohighlight">\(\theta\)</span> (see equation below) was increased from 10,000 (Llama 2) to 500,000 (Llama 3) in the following equation (adapted from the <a class="reference external" href="https://arxiv.org/abs/2104.09864">RoPE paper</a>)</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\Theta = \left\{\theta_i = \text{base}^{\frac{-2(i-1)}{d}}, i \in \left[1, 2, ..., d/2\right]\right\}\]</div>
<ul class="simple">
<li><p>These <span class="math notranslate nohighlight">\(\theta\)</span> values are a set of predefined parameters that are used to determine the rotational angles in the rotary matrix, where <span class="math notranslate nohighlight">\(d\)</span> is the dimensionality of the embedding space</p></li>
<li><p>Increasing the base from 10,000 to 500,000 makes the frequencies (or rotation angles) decay more slowly across the dimensions, which means that higher dimensions will be associated with larger angles than before (essentially, it’s a decompression of the frequencies)</p></li>
<li><p>In addition, we introduce a <code class="docutils literal notranslate"><span class="pre">freq_config</span></code> section in the code below that adjusts the frequency; however, we won’t be needing it in Llama 3 (only Llama 3.1 and Llama 3.2), so we will revisit this <code class="docutils literal notranslate"><span class="pre">freq_config</span></code> later (it’s set to <code class="docutils literal notranslate"><span class="pre">None</span></code> and ignored by default)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="k">def</span><span class="w"> </span><span class="nf">precompute_rope_params</span><span class="p">(</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">theta_base</span><span class="o">=</span><span class="mi">10_000</span><span class="p">,</span> <span class="n">context_length</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span> <span class="n">freq_config</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">head_dim</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Embedding dimension must be even&quot;</span>

    <span class="c1"># Compute the inverse frequencies</span>
    <span class="n">inv_freq</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">theta_base</span> <span class="o">**</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">,</span> <span class="mi">2</span><span class="p">)[:</span> <span class="p">(</span><span class="n">head_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)]</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">/</span> <span class="n">head_dim</span><span class="p">))</span>

    <span class="c1">################################ NEW ###############################################</span>
    <span class="c1"># Frequency adjustments</span>
    <span class="k">if</span> <span class="n">freq_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">low_freq_wavelen</span> <span class="o">=</span> <span class="n">freq_config</span><span class="p">[</span><span class="s2">&quot;original_context_length&quot;</span><span class="p">]</span> <span class="o">/</span> <span class="n">freq_config</span><span class="p">[</span><span class="s2">&quot;low_freq_factor&quot;</span><span class="p">]</span>
        <span class="n">high_freq_wavelen</span> <span class="o">=</span> <span class="n">freq_config</span><span class="p">[</span><span class="s2">&quot;original_context_length&quot;</span><span class="p">]</span> <span class="o">/</span> <span class="n">freq_config</span><span class="p">[</span><span class="s2">&quot;high_freq_factor&quot;</span><span class="p">]</span>

        <span class="n">wavelen</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="n">inv_freq</span>

        <span class="n">inv_freq_llama</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="n">wavelen</span> <span class="o">&gt;</span> <span class="n">low_freq_wavelen</span><span class="p">,</span> <span class="n">inv_freq</span> <span class="o">/</span> <span class="n">freq_config</span><span class="p">[</span><span class="s2">&quot;factor&quot;</span><span class="p">],</span> <span class="n">inv_freq</span>
        <span class="p">)</span>

        <span class="n">smooth_factor</span> <span class="o">=</span> <span class="p">(</span><span class="n">freq_config</span><span class="p">[</span><span class="s2">&quot;original_context_length&quot;</span><span class="p">]</span> <span class="o">/</span> <span class="n">wavelen</span> <span class="o">-</span> <span class="n">freq_config</span><span class="p">[</span><span class="s2">&quot;low_freq_factor&quot;</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span>
            <span class="n">freq_config</span><span class="p">[</span><span class="s2">&quot;high_freq_factor&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">freq_config</span><span class="p">[</span><span class="s2">&quot;low_freq_factor&quot;</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="n">smoothed_inv_freq</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">smooth_factor</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">inv_freq</span> <span class="o">/</span> <span class="n">freq_config</span><span class="p">[</span><span class="s2">&quot;factor&quot;</span><span class="p">])</span> <span class="o">+</span> <span class="n">smooth_factor</span> <span class="o">*</span> <span class="n">inv_freq</span>
        <span class="p">)</span>

        <span class="n">is_medium_freq</span> <span class="o">=</span> <span class="p">(</span><span class="n">wavelen</span> <span class="o">&lt;=</span> <span class="n">low_freq_wavelen</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">wavelen</span> <span class="o">&gt;=</span> <span class="n">high_freq_wavelen</span><span class="p">)</span>
        <span class="n">inv_freq_llama</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">is_medium_freq</span><span class="p">,</span> <span class="n">smoothed_inv_freq</span><span class="p">,</span> <span class="n">inv_freq_llama</span><span class="p">)</span>
        <span class="n">inv_freq</span> <span class="o">=</span> <span class="n">inv_freq_llama</span>
    <span class="c1">####################################################################################</span>


    <span class="c1"># Generate position indices</span>
    <span class="n">positions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">context_length</span><span class="p">)</span>

    <span class="c1"># Compute the angles</span>
    <span class="n">angles</span> <span class="o">=</span> <span class="n">positions</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">inv_freq</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># Shape: (context_length, head_dim // 2)</span>

    <span class="c1"># Expand angles to match the head_dim</span>
    <span class="n">angles</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">angles</span><span class="p">,</span> <span class="n">angles</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Shape: (context_length, head_dim)</span>

    <span class="c1"># Precompute sine and cosine</span>
    <span class="n">cos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angles</span><span class="p">)</span>
    <span class="n">sin</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angles</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>To summarize, what’s new so far for Llama 3 compared to Llama 2 are the context length and theta base parameter:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Instantiate RoPE parameters</span>

<span class="n">llama_2_context_len</span> <span class="o">=</span> <span class="mi">4096</span>
<span class="n">llama_3_context_len</span> <span class="o">=</span> <span class="mi">8192</span>

<span class="n">llama_2_theta_base</span> <span class="o">=</span> <span class="mi">10_000</span>
<span class="n">llama_3_theta_base</span> <span class="o">=</span> <span class="mi">500_000</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>The usage remains the same as before in Llama 2:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Settings</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">head_dim</span> <span class="o">=</span> <span class="mi">16</span>

<span class="c1"># Instantiate RoPE parameters</span>
<span class="n">cos</span><span class="p">,</span> <span class="n">sin</span> <span class="o">=</span> <span class="n">precompute_rope_params</span><span class="p">(</span>
    <span class="n">head_dim</span><span class="o">=</span><span class="n">head_dim</span><span class="p">,</span>
    <span class="n">theta_base</span><span class="o">=</span><span class="n">llama_3_theta_base</span><span class="p">,</span>
    <span class="n">context_length</span><span class="o">=</span><span class="n">llama_3_context_len</span>
<span class="p">)</span>

<span class="c1"># Dummy query and key tensors</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">queries</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">llama_3_context_len</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
<span class="n">keys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">llama_3_context_len</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>

<span class="c1"># Apply rotary position embeddings</span>
<span class="n">queries_rot</span> <span class="o">=</span> <span class="n">compute_rope</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span><span class="p">)</span>
<span class="n">keys_rot</span> <span class="o">=</span> <span class="n">compute_rope</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p> </p>
</section>
<section id="grouped-query-attention">
<h2>1.3 Grouped-query attention<a class="headerlink" href="#grouped-query-attention" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>In this section, we replace multi-head attention (MHA) with an alternative mechanism called grouped-query attention (GQA)</p></li>
<li><p>In short, one can think of GQA as a more compute- and parameter-efficient version of MHA</p></li>
<li><p>In GQA, we reduce the number of key and value projections by sharing them among multiple attention heads</p></li>
<li><p>Each attention head still has its unique query, but these queries attend to the same group of keys and values</p></li>
<li><p>Below is an illustration of GQA with 2 key-value-groups (kv-groups):</p></li>
</ul>
<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/gpt-to-llama/grouped-query-attention.webp" width="500px">
<ul class="simple">
<li><p>The main idea behind GQA is to reduce the number of unique query groups that attend to the key-value pairs, reducing the size of some of the matrix multiplications and the number of parameters in MHA without significantly reducing modeling performance</p></li>
<li><p>The GQA code is very similar to MHA (I highlighted the changes below via the “NEW” sections)</p></li>
<li><p>In short, the main change in GQA is that each query group needs to be repeated to match the number of heads it is associated with, as implemented below</p></li>
</ul>
<ul class="simple">
<li><p>In addition, we also introduce a <code class="docutils literal notranslate"><span class="pre">SharedBuffers</span></code> class that will allow us to reuse the <code class="docutils literal notranslate"><span class="pre">mask</span></code>, <code class="docutils literal notranslate"><span class="pre">cos</span></code>, and <code class="docutils literal notranslate"><span class="pre">sin</span></code> tensors in the transformer blocks to improve efficiency (this will be crucial when working with models such as Llama 3.1 and 3.2 later, which support up to 131k input tokens)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>


<span class="c1">############################# NEW  #############################</span>
<span class="k">class</span><span class="w"> </span><span class="nc">SharedBuffers</span><span class="p">:</span>
    <span class="n">_buffers</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_buffers</span><span class="p">(</span><span class="n">context_length</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">,</span> <span class="n">rope_base</span><span class="p">,</span> <span class="n">freq_config</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
        <span class="n">key</span> <span class="o">=</span> <span class="p">(</span><span class="n">context_length</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">,</span> <span class="n">rope_base</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">freq_config</span><span class="o">.</span><span class="n">values</span><span class="p">())</span> <span class="k">if</span> <span class="n">freq_config</span> <span class="k">else</span> <span class="n">freq_config</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">SharedBuffers</span><span class="o">.</span><span class="n">_buffers</span><span class="p">:</span>
            <span class="c1"># Create or fetch the buffers</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">context_length</span><span class="p">,</span> <span class="n">context_length</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span> <span class="o">=</span> <span class="n">precompute_rope_params</span><span class="p">(</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">rope_base</span><span class="p">,</span> <span class="n">context_length</span><span class="p">,</span> <span class="n">freq_config</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">cos</span> <span class="o">=</span> <span class="n">cos</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
                <span class="n">sin</span> <span class="o">=</span> <span class="n">sin</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">SharedBuffers</span><span class="o">.</span><span class="n">_buffers</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">SharedBuffers</span><span class="o">.</span><span class="n">_buffers</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
<span class="c1">############################# NEW  #############################</span>


<span class="k">class</span><span class="w"> </span><span class="nc">GroupedQueryAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">context_length</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span>
            <span class="n">num_kv_groups</span><span class="p">,</span>       <span class="c1"># NEW</span>
            <span class="n">rope_base</span><span class="o">=</span><span class="mi">10_000</span><span class="p">,</span>    <span class="c1"># NEW</span>
            <span class="n">rope_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>    <span class="c1"># NEW</span>
            <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span>
        <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">d_out</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;d_out must be divisible by num_heads&quot;</span>
        <span class="k">assert</span> <span class="n">num_heads</span> <span class="o">%</span> <span class="n">num_kv_groups</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;num_heads must be divisible by num_kv_groups&quot;</span>  <span class="c1"># NEW</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">d_out</span> <span class="o">=</span> <span class="n">d_out</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">d_out</span> <span class="o">//</span> <span class="n">num_heads</span>

        <span class="c1">############################# NEW  #############################</span>
        <span class="c1"># self.W_key = nn.Linear(d_in, d_out, bias=False, dtype=dtype)</span>
        <span class="c1"># self.W_value = nn.Linear(d_in, d_out, bias=False, dtype=dtype)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_key</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">num_kv_groups</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_value</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">num_kv_groups</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_groups</span> <span class="o">=</span> <span class="n">num_kv_groups</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group_size</span> <span class="o">=</span> <span class="n">num_heads</span> <span class="o">//</span> <span class="n">num_kv_groups</span>
        <span class="c1">################################################################</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">W_query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_out</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1">############################# NEW  #############################</span>
        <span class="c1"># Fetch buffers using SharedBuffers</span>
        <span class="n">mask</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span> <span class="o">=</span> <span class="n">SharedBuffers</span><span class="o">.</span><span class="n">get_buffers</span><span class="p">(</span><span class="n">context_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">rope_base</span><span class="p">,</span> <span class="n">rope_config</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
        <span class="c1">############################# NEW  #############################</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;mask&quot;</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;cos&quot;</span><span class="p">,</span> <span class="n">cos</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;sin&quot;</span><span class="p">,</span> <span class="n">sin</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">b</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="n">d_in</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">queries</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_query</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Shape: (b, num_tokens, d_out)</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_key</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Shape: (b, num_tokens, num_kv_groups * head_dim)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_value</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Shape: (b, num_tokens, num_kv_groups * head_dim)</span>

        <span class="c1"># Reshape queries, keys, and values</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="n">queries</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

        <span class="c1">##################### NEW  #####################</span>
        <span class="c1"># keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)</span>
        <span class="c1"># values = values.view(b, num_tokens, self.num_heads, self.head_dim)</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="n">keys</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="c1">################################################</span>

        <span class="c1"># Transpose keys, values, and queries</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="n">keys</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># Shape: (b, num_heads, num_tokens, head_dim)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># Shape: (b, num_heads, num_tokens, head_dim)</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="n">queries</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># Shape: (b, num_query_groups, num_tokens, head_dim)</span>

        <span class="c1"># Apply RoPE</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="n">compute_rope</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cos</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sin</span><span class="p">)</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="n">compute_rope</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cos</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sin</span><span class="p">)</span>

        <span class="c1">##################### NEW  #####################</span>
        <span class="c1"># Expand keys and values to match the number of heads</span>
        <span class="c1"># Shape: (b, num_heads, num_tokens, head_dim)</span>

        <span class="n">keys</span> <span class="o">=</span> <span class="n">keys</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">group_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Shape: (b, num_heads, num_tokens, head_dim)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">group_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Shape: (b, num_heads, num_tokens, head_dim)</span>
        <span class="c1"># For example, before repeat_interleave along dim=1 (query groups):</span>
        <span class="c1">#   [K1, K2]</span>
        <span class="c1"># After repeat_interleave (each query group is repeated group_size times):</span>
        <span class="c1">#   [K1, K1, K2, K2]</span>
        <span class="c1"># If we used regular repeat instead of repeat_interleave, we&#39;d get:</span>
        <span class="c1">#   [K1, K2, K1, K2]</span>
        <span class="c1">################################################</span>

        <span class="c1"># Compute scaled dot-product attention (aka self-attention) with a causal mask</span>
        <span class="c1"># Shape: (b, num_heads, num_tokens, num_tokens)</span>
        <span class="n">attn_scores</span> <span class="o">=</span> <span class="n">queries</span> <span class="o">@</span> <span class="n">keys</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># Dot product for each head</span>

        <span class="c1"># Original mask truncated to the number of tokens and converted to boolean</span>
        <span class="n">mask_bool</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask</span><span class="o">.</span><span class="n">bool</span><span class="p">()[:</span><span class="n">num_tokens</span><span class="p">,</span> <span class="p">:</span><span class="n">num_tokens</span><span class="p">]</span>

        <span class="c1"># Use the mask to fill attention scores</span>
        <span class="n">attn_scores</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">mask_bool</span><span class="p">,</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>

        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_scores</span> <span class="o">/</span> <span class="n">keys</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">keys</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span>

        <span class="c1"># Shape: (b, num_tokens, num_heads, head_dim)</span>
        <span class="n">context_vec</span> <span class="o">=</span> <span class="p">(</span><span class="n">attn_weights</span> <span class="o">@</span> <span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Combine heads, where self.d_out = self.num_heads * self.head_dim</span>
        <span class="n">context_vec</span> <span class="o">=</span> <span class="n">context_vec</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_out</span><span class="p">)</span>
        <span class="n">context_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">context_vec</span><span class="p">)</span>  <span class="c1"># optional projection</span>

        <span class="k">return</span> <span class="n">context_vec</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>To illustrate the parameter savings, consider the following multi-head attention example from the GPT and Llama 2 code:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Settings</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">context_len</span> <span class="o">=</span> <span class="mi">3000</span>
<span class="n">max_context_len</span> <span class="o">=</span> <span class="mi">8192</span>
<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">4096</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">32</span>


<span class="n">example_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">context_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">))</span>

<span class="n">mha</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span>
    <span class="n">d_in</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span>
    <span class="n">d_out</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span>
    <span class="n">context_length</span><span class="o">=</span><span class="n">max_context_len</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span>
<span class="p">)</span>

<span class="n">mha</span><span class="p">(</span><span class="n">example_batch</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W_key:&quot;</span><span class="p">,</span> <span class="n">mha</span><span class="o">.</span><span class="n">W_key</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W_value:&quot;</span><span class="p">,</span> <span class="n">mha</span><span class="o">.</span><span class="n">W_value</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W_query:&quot;</span><span class="p">,</span> <span class="n">mha</span><span class="o">.</span><span class="n">W_query</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>W_key: torch.Size([4096, 4096])
W_value: torch.Size([4096, 4096])
W_query: torch.Size([4096, 4096])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Now, if we use grouped-query attention instead, with 8 kv-groups (that’s how many Llama 3 8B uses), we can see that the number of rows of the key and value matrices are reduced by a factor of 4 (because 32 attention heads divided by 8 kv-groups is 4)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gqa</span> <span class="o">=</span> <span class="n">GroupedQueryAttention</span><span class="p">(</span>
    <span class="n">d_in</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span>
    <span class="n">d_out</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span>
    <span class="n">context_length</span><span class="o">=</span><span class="n">max_context_len</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
    <span class="n">num_kv_groups</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">rope_base</span><span class="o">=</span><span class="n">llama_3_theta_base</span>
<span class="p">)</span>

<span class="n">gqa</span><span class="p">(</span><span class="n">example_batch</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W_key:&quot;</span><span class="p">,</span> <span class="n">gqa</span><span class="o">.</span><span class="n">W_key</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W_value:&quot;</span><span class="p">,</span> <span class="n">gqa</span><span class="o">.</span><span class="n">W_value</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W_query:&quot;</span><span class="p">,</span> <span class="n">gqa</span><span class="o">.</span><span class="n">W_query</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>W_key: torch.Size([1024, 4096])
W_value: torch.Size([1024, 4096])
W_query: torch.Size([4096, 4096])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>As a side note, to make the GroupedQueryAttention equivalent to standard multi-head attention, you can set the number of query groups (<code class="docutils literal notranslate"><span class="pre">num_kv_groups</span></code>) equal to the number of heads (<code class="docutils literal notranslate"><span class="pre">num_heads</span></code>)</p></li>
<li><p>Lastly, let’s compare the number of parameters below:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Total number of parameters:&quot;</span><span class="p">)</span>

<span class="n">mha_total_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">mha</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MHA: </span><span class="si">{</span><span class="n">mha_total_params</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">gqa_total_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">gqa</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GQA: </span><span class="si">{</span><span class="n">gqa_total_params</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total number of parameters:
MHA: 67,108,864
GQA: 41,943,040
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Free up memory:</span>
<span class="k">del</span> <span class="n">mha</span>
<span class="k">del</span> <span class="n">gqa</span>
</pre></div>
</div>
</div>
</div>
<p> </p>
</section>
<section id="update-the-transformerblock-module">
<h2>1.4 Update the TransformerBlock module<a class="headerlink" href="#update-the-transformerblock-module" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Next, we update the <code class="docutils literal notranslate"><span class="pre">TransformerBlock</span></code></p></li>
<li><p>Here, we simply swap <code class="docutils literal notranslate"><span class="pre">MultiHeadAttention</span></code> with <code class="docutils literal notranslate"><span class="pre">GroupedQueryAttention</span></code> and add the new RoPE settings</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">TransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">att</span> <span class="o">=</span>  <span class="n">GroupedQueryAttention</span><span class="p">(</span>  <span class="c1"># MultiHeadAttention(</span>
            <span class="n">d_in</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">],</span>
            <span class="n">d_out</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">],</span>
            <span class="n">context_length</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;context_length&quot;</span><span class="p">],</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;n_heads&quot;</span><span class="p">],</span>
            <span class="n">num_kv_groups</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;n_kv_groups&quot;</span><span class="p">],</span>  <span class="c1"># NEW</span>
            <span class="n">rope_base</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;rope_base&quot;</span><span class="p">],</span>        <span class="c1"># NEW</span>
            <span class="n">rope_config</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;rope_freq&quot;</span><span class="p">],</span>      <span class="c1"># NEW</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;dtype&quot;</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">],</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">],</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Shortcut connection for attention block</span>
        <span class="n">shortcut</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">att</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">))</span>   <span class="c1"># Shape [batch_size, num_tokens, emb_size]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">shortcut</span>  <span class="c1"># Add the original input back</span>

        <span class="c1"># Shortcut connection for feed-forward block</span>
        <span class="n">shortcut</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">shortcut</span>  <span class="c1"># Add the original input back</span>

        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<p> </p>
</section>
<section id="defining-the-model-class">
<h2>1.5 Defining the model class<a class="headerlink" href="#defining-the-model-class" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>When setting up the model class, we fortunately don’t have to do much; we just update the name to <code class="docutils literal notranslate"><span class="pre">Llama3Model</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># class Llama2Model(nn.Module):</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Llama3Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tok_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;vocab_size&quot;</span><span class="p">],</span> <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;dtype&quot;</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">trf_blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="o">*</span><span class="p">[</span><span class="n">TransformerBlock</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;n_layers&quot;</span><span class="p">])])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">final_norm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">],</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">],</span> <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;vocab_size&quot;</span><span class="p">],</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;dtype&quot;</span><span class="p">])</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_idx</span><span class="p">):</span>
        <span class="n">tok_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tok_emb</span><span class="p">(</span><span class="n">in_idx</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tok_embeds</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_head</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">logits</span>
</pre></div>
</div>
</div>
</div>
<p> </p>
</section>
<section id="initialize-model">
<h2>2. Initialize model<a class="headerlink" href="#initialize-model" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Now we can define a Llama 3 config file (the Llama 2 config file is shown for comparison)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">LLAMA2_CONFIG_7B</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">32_000</span><span class="p">,</span>    <span class="c1"># Vocabulary size</span>
    <span class="s2">&quot;context_length&quot;</span><span class="p">:</span> <span class="mi">4096</span><span class="p">,</span>  <span class="c1"># Context length</span>
    <span class="s2">&quot;emb_dim&quot;</span><span class="p">:</span> <span class="mi">4096</span><span class="p">,</span>         <span class="c1"># Embedding dimension</span>
    <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>           <span class="c1"># Number of attention heads</span>
    <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>          <span class="c1"># Number of layers</span>
    <span class="s2">&quot;hidden_dim&quot;</span><span class="p">:</span> <span class="mi">11_008</span><span class="p">,</span>    <span class="c1"># Size of the intermediate dimension in FeedForward</span>
    <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>  <span class="c1"># Lower-precision dtype to reduce memory usage</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">LLAMA3_CONFIG_8B</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">128_256</span><span class="p">,</span>   <span class="c1"># NEW: Larger vocabulary size</span>
    <span class="s2">&quot;context_length&quot;</span><span class="p">:</span> <span class="mi">8192</span><span class="p">,</span>  <span class="c1"># NEW: Larger context length</span>
    <span class="s2">&quot;emb_dim&quot;</span><span class="p">:</span> <span class="mi">4096</span><span class="p">,</span>         <span class="c1"># Embedding dimension</span>
    <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>           <span class="c1"># Number of attention heads</span>
    <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>          <span class="c1"># Number of layers</span>
    <span class="s2">&quot;hidden_dim&quot;</span><span class="p">:</span> <span class="mi">14_336</span><span class="p">,</span>    <span class="c1"># NEW: Larger size of the intermediate dimension in FeedForward</span>
    <span class="s2">&quot;n_kv_groups&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>        <span class="c1"># NEW: Key-Value groups for grouped-query attention</span>
    <span class="s2">&quot;rope_base&quot;</span><span class="p">:</span> <span class="mf">500_000.0</span><span class="p">,</span>  <span class="c1"># NEW: The base in RoPE&#39;s &quot;theta&quot; was increased to 500_000</span>
    <span class="s2">&quot;rope_freq&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>       <span class="c1"># NEW: Additional configuration for adjusting the RoPE frequencies</span>
    <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>  <span class="c1"># Lower-precision dtype to reduce memory usage</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Using these settings, we can now initialize a Llama 3 8B model</p></li>
<li><p>Note that this requires ~34 GB of memory (for comparison, Llama 2 7B required ~26 GB of memory)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Llama3Model</span><span class="p">(</span><span class="n">LLAMA3_CONFIG_8B</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>The following is expected to print True to confirm buffers are reused instead of being (wastefully) recreated:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check buffers</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">mask</span> <span class="ow">is</span> <span class="n">model</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">mask</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">cos</span> <span class="ow">is</span> <span class="n">model</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">cos</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">sin</span> <span class="ow">is</span> <span class="n">model</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">sin</span><span class="p">)</span> 
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Let’s now also compute the number of trainable parameters:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">total_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total number of parameters: </span><span class="si">{</span><span class="n">total_params</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total number of parameters: 8,030,261,248
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>As shown above, the model contains 8 billion parameters</p></li>
<li><p>Additionally, we can calculate the memory requirements for this model using the code below:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">model_memory_size</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
    <span class="n">total_params</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total_grads</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="c1"># Calculate total number of elements per parameter</span>
        <span class="n">param_size</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
        <span class="n">total_params</span> <span class="o">+=</span> <span class="n">param_size</span>
        <span class="c1"># Check if gradients are stored for this parameter</span>
        <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="n">total_grads</span> <span class="o">+=</span> <span class="n">param_size</span>

    <span class="c1"># Calculate buffer size (non-parameters that require memory)</span>
    <span class="n">total_buffers</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">buf</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">buffers</span><span class="p">())</span>

    <span class="c1"># Size in bytes = (Number of elements) * (Size of each element in bytes)</span>
    <span class="c1"># We assume parameters and gradients are stored in the same type as input dtype</span>
    <span class="n">element_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">input_dtype</span><span class="p">)</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span>
    <span class="n">total_memory_bytes</span> <span class="o">=</span> <span class="p">(</span><span class="n">total_params</span> <span class="o">+</span> <span class="n">total_grads</span> <span class="o">+</span> <span class="n">total_buffers</span><span class="p">)</span> <span class="o">*</span> <span class="n">element_size</span>

    <span class="c1"># Convert bytes to gigabytes</span>
    <span class="n">total_memory_gb</span> <span class="o">=</span> <span class="n">total_memory_bytes</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">total_memory_gb</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;float32 (PyTorch default): </span><span class="si">{</span><span class="n">model_memory_size</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">input_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;bfloat16: </span><span class="si">{</span><span class="n">model_memory_size</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">input_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>float32 (PyTorch default): 68.08 GB
bfloat16: 34.04 GB
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Lastly, we can also transfer the model to an NVIDIA or Apple Silicon GPU if applicable:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;mps&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p> </p>
</section>
<section id="load-tokenizer">
<h2>3. Load tokenizer<a class="headerlink" href="#load-tokenizer" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>In this section, we are going to load the tokenizer for the model</p></li>
<li><p>Llama 2 used Google’s <a class="reference external" href="https://github.com/google/sentencepiece">SentencePiece</a> tokenizer instead of OpenAI’s BPE tokenizer based on the <a class="reference external" href="https://github.com/openai/tiktoken">Tiktoken</a> library</p></li>
<li><p>Llama 3, however, reverted back to using the BPE tokenizer from Tiktoken; specifically, it uses the GPT-4 tokenizer with an extended vocabulary</p></li>
<li><p>You can find the original Tiktoken-adaptation by Meta AI <a class="reference external" href="https://github.com/meta-llama/llama3/blob/main/llama/tokenizer.py">here</a> in their official Llama 3 repository</p></li>
<li><p>Below, I rewrote the tokenizer code to make it more readable and minimal for this notebook (but the behavior should be similar)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">tiktoken</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tiktoken.load</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_tiktoken_bpe</span>


<span class="k">class</span><span class="w"> </span><span class="nc">Tokenizer</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_path</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">model_path</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Model file </span><span class="si">{</span><span class="n">model_path</span><span class="si">}</span><span class="s2"> not found&quot;</span>
        <span class="n">mergeable_ranks</span> <span class="o">=</span> <span class="n">load_tiktoken_bpe</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">special_tokens</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;&lt;|begin_of_text|&gt;&quot;</span><span class="p">:</span> <span class="mi">128000</span><span class="p">,</span>
            <span class="s2">&quot;&lt;|end_of_text|&gt;&quot;</span><span class="p">:</span> <span class="mi">128001</span><span class="p">,</span>
            <span class="s2">&quot;&lt;|start_header_id|&gt;&quot;</span><span class="p">:</span> <span class="mi">128006</span><span class="p">,</span>
            <span class="s2">&quot;&lt;|end_header_id|&gt;&quot;</span><span class="p">:</span> <span class="mi">128007</span><span class="p">,</span>
            <span class="s2">&quot;&lt;|eot_id|&gt;&quot;</span><span class="p">:</span> <span class="mi">128009</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">special_tokens</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
            <span class="sa">f</span><span class="s2">&quot;&lt;|reserved_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">|&gt;&quot;</span><span class="p">:</span> <span class="mi">128002</span> <span class="o">+</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span> <span class="k">if</span> <span class="p">(</span><span class="mi">128002</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">special_tokens</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
        <span class="p">})</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">Encoding</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="n">Path</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
            <span class="n">pat_str</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;(?i:&#39;s|&#39;t|&#39;re|&#39;ve|&#39;m|&#39;ll|&#39;d)|[^\r\n\p</span><span class="si">{L}</span><span class="s2">\p</span><span class="si">{N}</span><span class="s2">]?\p</span><span class="si">{L}</span><span class="s2">+|\p</span><span class="si">{N}</span><span class="s2">{1,3}| ?[^\s\p</span><span class="si">{L}</span><span class="s2">\p</span><span class="si">{N}</span><span class="s2">]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+&quot;</span><span class="p">,</span>
            <span class="n">mergeable_ranks</span><span class="o">=</span><span class="n">mergeable_ranks</span><span class="p">,</span>
            <span class="n">special_tokens</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">special_tokens</span>
        <span class="p">)</span>


    <span class="k">def</span><span class="w"> </span><span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">bos</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eos</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">allowed_special</span><span class="o">=</span><span class="nb">set</span><span class="p">(),</span> <span class="n">disallowed_special</span><span class="o">=</span><span class="p">()):</span>
        <span class="k">if</span> <span class="n">bos</span><span class="p">:</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">special_tokens</span><span class="p">[</span><span class="s2">&quot;&lt;|begin_of_text|&gt;&quot;</span><span class="p">]]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">tokens</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">allowed_special</span><span class="o">=</span><span class="n">allowed_special</span><span class="p">,</span> <span class="n">disallowed_special</span><span class="o">=</span><span class="n">disallowed_special</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">eos</span><span class="p">:</span>
            <span class="n">tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">special_tokens</span><span class="p">[</span><span class="s2">&quot;&lt;|end_of_text|&gt;&quot;</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">tokens</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokens</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Meta AI shared the original Llama 3 model weights and tokenizer vocabulary on the Hugging Face Hub</p></li>
<li><p>We will first download the tokenizer vocabulary from the Hub and load it into the code above</p></li>
</ul>
<ul class="simple">
<li><p>Please note that Meta AI requires that you accept the Llama 3 licensing terms before you can download the files; to do this, you have to create a Hugging Face Hub account and visit the <a class="reference external" href="https://huggingface.co/meta-llama/Meta-Llama-3-8B">meta-llama/Meta-Llama-3-8B</a> repository to accept the terms</p></li>
<li><p>Next, you will need to create an access token; to generate an access token with READ permissions, click on the profile picture in the upper right and click on “Settings”</p></li>
</ul>
<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/gpt-to-llama/settings.webp?1" width="300px">
<ul class="simple">
<li><p>Then, create and copy the access token so you can copy &amp; paste it into the next code cell</p></li>
</ul>
<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/gpt-to-llama/access-token.webp?1" width="600px"><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">login</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;config.json&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">config_file</span><span class="p">:</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">config_file</span><span class="p">)</span>
    <span class="n">access_token</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;HF_ACCESS_TOKEN&quot;</span><span class="p">]</span>

<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">access_token</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to /root/.cache/huggingface/token
Login successful
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>After login via the access token, which is necessary to verify that we accepted the Llama 3 licensing terms, we can now download the tokenizer vocabulary:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">hf_hub_download</span>

<span class="n">tokenizer_file_path</span> <span class="o">=</span> <span class="n">hf_hub_download</span><span class="p">(</span>
    <span class="n">repo_id</span><span class="o">=</span><span class="s2">&quot;meta-llama/Meta-Llama-3-8B&quot;</span><span class="p">,</span>
    <span class="n">filename</span><span class="o">=</span><span class="s2">&quot;original/tokenizer.model&quot;</span><span class="p">,</span>
    <span class="n">local_dir</span><span class="o">=</span><span class="s2">&quot;Llama-3-8B&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Note that for using Llama 3 files, we may need the <code class="docutils literal notranslate"><span class="pre">blobfile</span></code> package, which is used when handling datasets or models stored in cloud storage solutions like Google Cloud Storage (GCS), Azure Blob Storage, or Amazon S3</p></li>
<li><p>You can install this dependency by uncommenting and executing the <code class="docutils literal notranslate"><span class="pre">pip</span></code> command below</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># pip install blobfile</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">tokenizer_file_path</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>We can now use the <code class="docutils literal notranslate"><span class="pre">generate</span></code> function to have the Llama 3 model generate new text:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">previous_chapters</span><span class="w"> </span><span class="kn">import</span> <span class="n">generate</span><span class="p">,</span> <span class="n">text_to_token_ids</span><span class="p">,</span> <span class="n">token_ids_to_text</span>
<span class="c1"># If the `previous_chapters.py` file is not available locally,</span>
<span class="c1"># you can import it from the `llms-from-scratch` PyPI package.</span>
<span class="c1"># For details, see: https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg</span>
<span class="c1"># E.g.,</span>
<span class="c1"># from llms_from_scratch.ch05 import generate, text_to_token_ids, token_ids_to_text</span>


<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>

<span class="n">token_ids</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">idx</span><span class="o">=</span><span class="n">text_to_token_ids</span><span class="p">(</span><span class="s2">&quot;Every effort&quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
    <span class="n">context_size</span><span class="o">=</span><span class="n">LLAMA3_CONFIG_8B</span><span class="p">[</span><span class="s2">&quot;context_length&quot;</span><span class="p">],</span>
    <span class="n">top_k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output text:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">token_ids_to_text</span><span class="p">(</span><span class="n">token_ids</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Output text:
 Every effort_dead aeros Ingredients başında.extensionégor clangmissions güc như submodule.and report官方%，.Reader(&quot;,&quot;);
ामल ندار Parliamentary !!! HigginsDynamicZhgmt writeln Globalsletion 사진------
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Of course, as we can see above, the text is nonsensical since we haven’t trained the Llama 3 model yet</p></li>
<li><p>In the next section, instead of training it ourselves, which would cost tens to hundreds of thousands of dollars, we load the pretrained weights from Meta AI</p></li>
</ul>
<p> </p>
</section>
<section id="load-pretrained-weights">
<h2>4. Load pretrained weights<a class="headerlink" href="#load-pretrained-weights" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>We are loading the <a class="reference external" href="https://huggingface.co/meta-llama/Meta-Llama-3-8B">“meta-llama/Meta-Llama-3-8B”</a> base model below, which is a simple text completion model before finetuning</p></li>
<li><p>Alternatively, you can load the instruction-finetuned and aligned <a class="reference external" href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct">“meta-llama/Meta-Llama-3-8B-Instruct”</a> model by modifying the string in the next code cell accordingly</p></li>
<li><p>Combined, the weight files are about 16 GB large</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">safetensors.torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_file</span>

<span class="n">combined_weights</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
    <span class="n">weights_file</span> <span class="o">=</span> <span class="n">hf_hub_download</span><span class="p">(</span>
        <span class="n">repo_id</span><span class="o">=</span><span class="s2">&quot;meta-llama/Meta-Llama-3-8B&quot;</span><span class="p">,</span>
        <span class="n">filename</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;model-0000</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">-of-00004.safetensors&quot;</span><span class="p">,</span>
        <span class="n">local_dir</span><span class="o">=</span><span class="s2">&quot;Llama-3-8B&quot;</span>
    <span class="p">)</span>
    <span class="n">current_weights</span> <span class="o">=</span> <span class="n">load_file</span><span class="p">(</span><span class="n">weights_file</span><span class="p">)</span>
    <span class="n">combined_weights</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">current_weights</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "245443330e4d40c887a5649cc1663e98", "version_major": 2, "version_minor": 0}</script></div>
</div>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">weights</span></code> contains the following tensors (only the first 15 are shown for simplicity):</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">list</span><span class="p">(</span><span class="n">combined_weights</span><span class="o">.</span><span class="n">keys</span><span class="p">())[:</span><span class="mi">15</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;model.embed_tokens.weight&#39;,
 &#39;model.layers.0.input_layernorm.weight&#39;,
 &#39;model.layers.0.mlp.down_proj.weight&#39;,
 &#39;model.layers.0.mlp.gate_proj.weight&#39;,
 &#39;model.layers.0.mlp.up_proj.weight&#39;,
 &#39;model.layers.0.post_attention_layernorm.weight&#39;,
 &#39;model.layers.0.self_attn.k_proj.weight&#39;,
 &#39;model.layers.0.self_attn.o_proj.weight&#39;,
 &#39;model.layers.0.self_attn.q_proj.weight&#39;,
 &#39;model.layers.0.self_attn.v_proj.weight&#39;,
 &#39;model.layers.1.input_layernorm.weight&#39;,
 &#39;model.layers.1.mlp.down_proj.weight&#39;,
 &#39;model.layers.1.mlp.gate_proj.weight&#39;,
 &#39;model.layers.1.mlp.up_proj.weight&#39;,
 &#39;model.layers.1.post_attention_layernorm.weight&#39;]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>The following function, modeled after the <code class="docutils literal notranslate"><span class="pre">load_weights_into_gpt</span></code> function in <a class="reference internal" href="../01_main-chapter-code/ch05.html"><span class="std std-doc">chapter 5</span></a>, loads the pretrained weights into our Llama 3 model:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">assign</span><span class="p">(</span><span class="n">left</span><span class="p">,</span> <span class="n">right</span><span class="p">,</span> <span class="n">tensor_name</span><span class="o">=</span><span class="s2">&quot;unknown&quot;</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">left</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">right</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Shape mismatch in tensor &#39;</span><span class="si">{</span><span class="n">tensor_name</span><span class="si">}</span><span class="s2">&#39;. Left: </span><span class="si">{</span><span class="n">left</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, Right: </span><span class="si">{</span><span class="n">right</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">right</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">right</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">right</span><span class="p">))</span>


<span class="k">def</span><span class="w"> </span><span class="nf">load_weights_into_llama</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">param_config</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">tok_emb</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">tok_emb</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">params</span><span class="p">[</span><span class="s2">&quot;model.embed_tokens.weight&quot;</span><span class="p">],</span> <span class="s2">&quot;model.embed_tokens.weight&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">param_config</span><span class="p">[</span><span class="s2">&quot;n_layers&quot;</span><span class="p">]):</span>

        <span class="c1"># Load attention weights</span>
        <span class="n">model</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">W_query</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span>
            <span class="n">model</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">W_query</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
            <span class="n">params</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;model.layers.</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s2">.self_attn.q_proj.weight&quot;</span><span class="p">],</span>
            <span class="sa">f</span><span class="s2">&quot;model.layers.</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s2">.self_attn.q_proj.weight&quot;</span>
        <span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">W_key</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span>
            <span class="n">model</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">W_key</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
            <span class="n">params</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;model.layers.</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s2">.self_attn.k_proj.weight&quot;</span><span class="p">],</span>
            <span class="sa">f</span><span class="s2">&quot;model.layers.</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s2">.self_attn.k_proj.weight&quot;</span>
        <span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">W_value</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span>
            <span class="n">model</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">W_value</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
            <span class="n">params</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;model.layers.</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s2">.self_attn.v_proj.weight&quot;</span><span class="p">],</span>
            <span class="sa">f</span><span class="s2">&quot;model.layers.</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s2">.self_attn.v_proj.weight&quot;</span>
        <span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span>
            <span class="n">model</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
            <span class="n">params</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;model.layers.</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s2">.self_attn.o_proj.weight&quot;</span><span class="p">],</span>
            <span class="sa">f</span><span class="s2">&quot;model.layers.</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s2">.self_attn.o_proj.weight&quot;</span>
        <span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span>
            <span class="n">model</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
            <span class="n">params</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;model.layers.</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s2">.input_layernorm.weight&quot;</span><span class="p">],</span>
            <span class="sa">f</span><span class="s2">&quot;model.layers.</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s2">.input_layernorm.weight&quot;</span>
        <span class="p">)</span>

        <span class="c1"># Load FeedForward weights</span>
        <span class="n">model</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">ff</span><span class="o">.</span><span class="n">fc1</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span>
            <span class="n">model</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">ff</span><span class="o">.</span><span class="n">fc1</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
            <span class="n">params</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;model.layers.</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s2">.mlp.gate_proj.weight&quot;</span><span class="p">],</span>
            <span class="sa">f</span><span class="s2">&quot;model.layers.</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s2">.mlp.gate_proj.weight&quot;</span>
        <span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">ff</span><span class="o">.</span><span class="n">fc2</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span>
            <span class="n">model</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">ff</span><span class="o">.</span><span class="n">fc2</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
            <span class="n">params</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;model.layers.</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s2">.mlp.up_proj.weight&quot;</span><span class="p">],</span>
            <span class="sa">f</span><span class="s2">&quot;model.layers.</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s2">.mlp.up_proj.weight&quot;</span>
        <span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">ff</span><span class="o">.</span><span class="n">fc3</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span>
            <span class="n">model</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">ff</span><span class="o">.</span><span class="n">fc3</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
            <span class="n">params</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;model.layers.</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s2">.mlp.down_proj.weight&quot;</span><span class="p">],</span>
            <span class="sa">f</span><span class="s2">&quot;model.layers.</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s2">.mlp.down_proj.weight&quot;</span>
        <span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span>
            <span class="n">model</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
            <span class="n">params</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;model.layers.</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s2">.post_attention_layernorm.weight&quot;</span><span class="p">],</span>
            <span class="sa">f</span><span class="s2">&quot;model.layers.</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s2">.post_attention_layernorm.weight&quot;</span>
        <span class="p">)</span>

    <span class="c1"># Load output layer weights</span>
    <span class="n">model</span><span class="o">.</span><span class="n">final_norm</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">final_norm</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">params</span><span class="p">[</span><span class="s2">&quot;model.norm.weight&quot;</span><span class="p">],</span> <span class="s2">&quot;model.norm.weight&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="s2">&quot;lm_head.weight&quot;</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="n">model</span><span class="o">.</span><span class="n">out_head</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">out_head</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">params</span><span class="p">[</span><span class="s2">&quot;lm_head.weight&quot;</span><span class="p">],</span> <span class="s2">&quot;lm_head.weight&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">out_head</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">out_head</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">params</span><span class="p">[</span><span class="s2">&quot;model.embed_tokens.weight&quot;</span><span class="p">],</span> <span class="s2">&quot;model.embed_tokens.weight&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model uses weight tying.&quot;</span><span class="p">)</span>


<span class="n">load_weights_into_llama</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">LLAMA3_CONFIG_8B</span><span class="p">,</span> <span class="n">combined_weights</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">);</span>
<span class="k">del</span> <span class="n">combined_weights</span>  <span class="c1"># free up memory</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Next, we are ready to use the model for text generation</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>

<span class="n">token_ids</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">idx</span><span class="o">=</span><span class="n">text_to_token_ids</span><span class="p">(</span><span class="s2">&quot;Every effort&quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
    <span class="n">context_size</span><span class="o">=</span><span class="n">LLAMA3_CONFIG_8B</span><span class="p">[</span><span class="s2">&quot;context_length&quot;</span><span class="p">],</span>
    <span class="n">top_k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output text:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">token_ids_to_text</span><span class="p">(</span><span class="n">token_ids</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Output text:
 Every effort has been made to trace copyright holders and to obtain their permission for the use of copyright material. The publisher apologizes for any
</pre></div>
</div>
</div>
</div>
<p> </p>
</section>
<section id="using-the-instruction-finetuned-model">
<h2>5. Using the instruction-finetuned model<a class="headerlink" href="#using-the-instruction-finetuned-model" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Above, we used the pretrained base model; if you want to use a model capable of following instructions, use the <code class="docutils literal notranslate"><span class="pre">&quot;meta-llama/Llama-3-8B-Instruct&quot;</span></code> model instead, as shown below</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to free up memory</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">gc</span>

<span class="k">del</span> <span class="n">model</span>

<span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>  <span class="c1"># Run Python garbage collector</span>

<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">combined_weights</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
    <span class="n">weights_file</span> <span class="o">=</span> <span class="n">hf_hub_download</span><span class="p">(</span>
        <span class="n">repo_id</span><span class="o">=</span><span class="s2">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span><span class="p">,</span>
        <span class="n">filename</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;model-0000</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">-of-00004.safetensors&quot;</span><span class="p">,</span>
        <span class="n">local_dir</span><span class="o">=</span><span class="s2">&quot;Llama-3-8B-Instruct&quot;</span>
    <span class="p">)</span>
    <span class="n">current_weights</span> <span class="o">=</span> <span class="n">load_file</span><span class="p">(</span><span class="n">weights_file</span><span class="p">)</span>
    <span class="n">combined_weights</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">current_weights</span><span class="p">)</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">Llama3Model</span><span class="p">(</span><span class="n">LLAMA3_CONFIG_8B</span><span class="p">)</span>
<span class="n">load_weights_into_llama</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">LLAMA3_CONFIG_8B</span><span class="p">,</span> <span class="n">combined_weights</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="k">del</span> <span class="n">combined_weights</span>  <span class="c1"># free up memory</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "f7df6bbf8e63448c8a6cb5d2f6208403", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "4772f31a1c5b4c168c9aabe7a1d2bacc", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "ad49eeb9e1204ea2bd2e371df8ccdea2", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "951b9e81613a40a2a503f61e69677f0a", "version_major": 2, "version_minor": 0}</script></div>
</div>
<ul class="simple">
<li><p>Note that the Llama 3 model should ideally be used with the correct prompt template that was used during finetuning (as discussed in chapter 7)</p></li>
<li><p>Below is a wrapper class around the tokenizer based on Meta AI’s Llama 3-specific <a class="reference external" href="https://github.com/meta-llama/llama3/blob/11817d47e1ba7a4959b025eb1ca308572e0e3963/llama/tokenizer.py#L202">ChatFormat code</a> that constructs the prompt template</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">ChatFormat</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">encode_header</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">message</span><span class="p">):</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">special_tokens</span><span class="p">[</span><span class="s2">&quot;&lt;|start_header_id|&gt;&quot;</span><span class="p">])</span>
        <span class="n">tokens</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">message</span><span class="p">[</span><span class="s2">&quot;role&quot;</span><span class="p">],</span> <span class="n">bos</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eos</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
        <span class="n">tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">special_tokens</span><span class="p">[</span><span class="s2">&quot;&lt;|end_header_id|&gt;&quot;</span><span class="p">])</span>
        <span class="n">tokens</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">bos</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eos</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">tokens</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>
        <span class="n">message</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
            <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">text</span>
        <span class="p">}</span>

        <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode_header</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
        <span class="n">tokens</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">message</span><span class="p">[</span><span class="s2">&quot;content&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">(),</span> <span class="n">bos</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eos</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">special_tokens</span><span class="p">[</span><span class="s2">&quot;&lt;|eot_id|&gt;&quot;</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">tokens</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token_ids</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>


<span class="n">chat_tokenizer</span> <span class="o">=</span> <span class="n">ChatFormat</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>The usage is as follows:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">token_ids</span> <span class="o">=</span> <span class="n">chat_tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;Hello World!&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[128006, 882, 128007, 271, 9906, 4435, 0, 128009]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\nHello World!&lt;|eot_id|&gt;&#39;
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Let’s now see the Llama 3 instruction model in action:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>

<span class="n">token_ids</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">idx</span><span class="o">=</span><span class="n">text_to_token_ids</span><span class="p">(</span><span class="s2">&quot;What do llamas eat?&quot;</span><span class="p">,</span> <span class="n">chat_tokenizer</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span>
    <span class="n">context_size</span><span class="o">=</span><span class="n">LLAMA3_CONFIG_8B</span><span class="p">[</span><span class="s2">&quot;context_length&quot;</span><span class="p">],</span>
    <span class="n">top_k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.</span>
<span class="p">)</span>

<span class="n">output_text</span> <span class="o">=</span> <span class="n">token_ids_to_text</span><span class="p">(</span><span class="n">token_ids</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">clean_text</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">header_end</span><span class="o">=</span><span class="s2">&quot;assistant&lt;|end_header_id|&gt;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">):</span>
    <span class="c1"># Find the index of the first occurrence of &quot;&lt;|end_header_id|&gt;&quot;</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">header_end</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">index</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
        <span class="c1"># Return the substring starting after &quot;&lt;|end_header_id|&gt;&quot;</span>
        <span class="k">return</span> <span class="n">text</span><span class="p">[</span><span class="n">index</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">header_end</span><span class="p">):]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>  <span class="c1"># Strip removes leading/trailing whitespace</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># If the token is not found, return the original text</span>
        <span class="k">return</span> <span class="n">text</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output text:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">clean_text</span><span class="p">(</span><span class="n">output_text</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Output text:
 Llamas are herbivores, which means they primarily eat plants and plant-based foods. Here are some of the things llamas like to eat:

1. Grass: Llamas love to graze on grass, especially in the spring and summer months.
2. Hay: Hay is a staple in a llama&#39;s diet. They like to eat timothy hay, alfalfa hay, and other types of hay.
3. Grains: Llamas may also be fed grains like oats, barley, and corn. However, grains should not make up more than 10-15% of a llama&#39;s diet.
4. Fruits and vegetables: Llamas may enjoy fruits and vegetables as treats, such as
</pre></div>
</div>
</div>
</div>
<p> </p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="llama-3-1-8b">
<h1>Llama 3.1 8B<a class="headerlink" href="#llama-3-1-8b" title="Link to this heading">#</a></h1>
<ul class="simple">
<li><p>A few months after the initial Llama 3 release, Meta AI followed up with their Llama 3.1 suite of models (see the official <a class="reference external" href="https://ai.meta.com/blog/meta-llama-3-1/">Introducing Llama 3.1: Our most capable models to date</a> announcement blog post for details)</p></li>
<li><p>Conveniently, we can reuse our previous Llama 3 code from above to implement Llama 3.1 8B</p></li>
</ul>
<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/gpt-to-llama/llama3-to-llama31.webp" width="700px">
<ul class="simple">
<li><p>The architecture is identical, with the only change being a rescaling of the RoPE frequencies as indicated in the configuration file below</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">LLAMA3_CONFIG_8B</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">128_256</span><span class="p">,</span>   <span class="c1"># Vocabulary size</span>
    <span class="s2">&quot;context_length&quot;</span><span class="p">:</span> <span class="mi">8192</span><span class="p">,</span>  <span class="c1"># Context length</span>
    <span class="s2">&quot;emb_dim&quot;</span><span class="p">:</span> <span class="mi">4096</span><span class="p">,</span>         <span class="c1"># Embedding dimension</span>
    <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>           <span class="c1"># Number of attention heads</span>
    <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>          <span class="c1"># Number of layers</span>
    <span class="s2">&quot;hidden_dim&quot;</span><span class="p">:</span> <span class="mi">14_336</span><span class="p">,</span>    <span class="c1"># Size of the intermediate dimension in FeedForward</span>
    <span class="s2">&quot;n_kv_groups&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>        <span class="c1"># Key-Value groups for grouped-query attention</span>
    <span class="s2">&quot;rope_base&quot;</span><span class="p">:</span> <span class="mf">500_000.0</span><span class="p">,</span>  <span class="c1"># The base in RoPE&#39;s &quot;theta&quot;</span>
    <span class="s2">&quot;rope_freq&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>       <span class="c1"># Additional configuration for adjusting the RoPE frequencies</span>
    <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>  <span class="c1"># Lower-precision dtype to reduce memory usage</span>
<span class="p">}</span>

<span class="n">LLAMA31_CONFIG_8B</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">128_256</span><span class="p">,</span>      <span class="c1"># Vocabulary size</span>
    <span class="s2">&quot;context_length&quot;</span><span class="p">:</span> <span class="mi">131_072</span><span class="p">,</span>  <span class="c1"># NEW: Larger supported context length</span>
    <span class="s2">&quot;emb_dim&quot;</span><span class="p">:</span> <span class="mi">4096</span><span class="p">,</span>            <span class="c1"># Embedding dimension</span>
    <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>              <span class="c1"># Number of attention heads</span>
    <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>             <span class="c1"># Number of layers</span>
    <span class="s2">&quot;hidden_dim&quot;</span><span class="p">:</span> <span class="mi">14_336</span><span class="p">,</span>       <span class="c1"># Size of the intermediate dimension in FeedForward</span>
    <span class="s2">&quot;n_kv_groups&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>           <span class="c1"># Key-Value groups for grouped-query attention</span>
    <span class="s2">&quot;rope_base&quot;</span><span class="p">:</span> <span class="mf">500_000.0</span><span class="p">,</span>     <span class="c1"># The base in RoPE&#39;s &quot;theta&quot;</span>
    <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>    <span class="c1"># Lower-precision dtype to reduce memory usage</span>
    <span class="s2">&quot;rope_freq&quot;</span><span class="p">:</span> <span class="p">{</span>              <span class="c1"># NEW: RoPE frequency scaling</span>
        <span class="s2">&quot;factor&quot;</span><span class="p">:</span> <span class="mf">8.0</span><span class="p">,</span>
        <span class="s2">&quot;low_freq_factor&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="s2">&quot;high_freq_factor&quot;</span><span class="p">:</span> <span class="mf">4.0</span><span class="p">,</span>
        <span class="s2">&quot;original_context_length&quot;</span><span class="p">:</span> <span class="mi">8192</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Reduce the context length so the model would work fine on a MacBook Air (if you have more RAM, feel free to comment out the lines below):</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">old_context_length</span> <span class="o">=</span> <span class="n">LLAMA31_CONFIG_8B</span><span class="p">[</span><span class="s2">&quot;context_length&quot;</span><span class="p">]</span>
<span class="n">LLAMA31_CONFIG_8B</span><span class="p">[</span><span class="s2">&quot;context_length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">8192</span>


<span class="k">def</span><span class="w"> </span><span class="nf">rescale_theta</span><span class="p">(</span><span class="n">theta_old</span><span class="p">,</span> <span class="n">context_length_old</span><span class="p">,</span> <span class="n">context_length_new</span><span class="p">):</span>
    <span class="n">scaling_factor</span> <span class="o">=</span> <span class="n">context_length_new</span> <span class="o">/</span> <span class="n">context_length_old</span>
    <span class="n">theta_new</span> <span class="o">=</span> <span class="n">theta_old</span> <span class="o">*</span> <span class="n">scaling_factor</span>
    <span class="k">return</span> <span class="n">theta_new</span>

<span class="n">LLAMA31_CONFIG_8B</span><span class="p">[</span><span class="s2">&quot;rope_base&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">rescale_theta</span><span class="p">(</span>
    <span class="n">LLAMA31_CONFIG_8B</span><span class="p">[</span><span class="s2">&quot;rope_base&quot;</span><span class="p">],</span>
    <span class="n">old_context_length</span><span class="p">,</span>
    <span class="n">LLAMA31_CONFIG_8B</span><span class="p">[</span><span class="s2">&quot;context_length&quot;</span><span class="p">]</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;New RoPE theta:&quot;</span><span class="p">,</span> <span class="n">LLAMA31_CONFIG_8B</span><span class="p">[</span><span class="s2">&quot;rope_base&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>New RoPE theta: 31250.0
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>As we’ve seen in the code earlier, the RoPE method uses sinusoidal functions (sine and cosine) to embed positional information directly into the attention mechanism</p></li>
<li><p>In Llama 3.1, via the additional configuration, we introduce additional adjustments to the inverse frequency calculations</p></li>
<li><p>These adjustments influence how different frequency components contribute to the positional embeddings (a detailed explanation is a topic for another time)</p></li>
<li><p>Let’s try out the Llama 3.1 model in practice; first, we clear out the old model to free up some GPU memory</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># free up memory</span>
<span class="k">del</span> <span class="n">model</span>

<span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>  <span class="c1"># Run Python garbage collector</span>

<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Next, we download the tokenizer</p></li>
<li><p>Note that since the Llama 3.1 family is distinct from the Llama 3 family, you’d have to go to the <a class="reference external" href="https://huggingface.co/meta-llama/Llama-3.1-8B">meta-llama/Llama-3.1-8B</a> repository and acknowledge the license terms for your Hugging Face access token to work for the download</p></li>
<li><p>Tip: For simplicity, we only load the base model below, but there’s also an instruction-finetuned version you can use by replacing <code class="docutils literal notranslate"><span class="pre">&quot;meta-llama/Llama-3.1-8B&quot;</span></code> with <code class="docutils literal notranslate"><span class="pre">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer_file_path</span> <span class="o">=</span> <span class="n">hf_hub_download</span><span class="p">(</span>
    <span class="n">repo_id</span><span class="o">=</span><span class="s2">&quot;meta-llama/Llama-3.1-8B&quot;</span><span class="p">,</span>
    <span class="n">filename</span><span class="o">=</span><span class="s2">&quot;original/tokenizer.model&quot;</span><span class="p">,</span>
    <span class="n">local_dir</span><span class="o">=</span><span class="s2">&quot;Llama-3.1-8B&quot;</span>
<span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">tokenizer_file_path</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Llama3Model</span><span class="p">(</span><span class="n">LLAMA31_CONFIG_8B</span><span class="p">)</span>

<span class="n">total_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total number of parameters: </span><span class="si">{</span><span class="n">total_params</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total number of parameters: 8,030,261,248
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">combined_weights</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
    <span class="n">weights_file</span> <span class="o">=</span> <span class="n">hf_hub_download</span><span class="p">(</span>
        <span class="n">repo_id</span><span class="o">=</span><span class="s2">&quot;meta-llama/Llama-3.1-8B&quot;</span><span class="p">,</span>
        <span class="n">filename</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;model-0000</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">-of-00004.safetensors&quot;</span><span class="p">,</span>
        <span class="n">local_dir</span><span class="o">=</span><span class="s2">&quot;Llama-3.1-8B&quot;</span>
    <span class="p">)</span>
    <span class="n">current_weights</span> <span class="o">=</span> <span class="n">load_file</span><span class="p">(</span><span class="n">weights_file</span><span class="p">)</span>
    <span class="n">combined_weights</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">current_weights</span><span class="p">)</span>

<span class="n">load_weights_into_llama</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">LLAMA31_CONFIG_8B</span><span class="p">,</span> <span class="n">combined_weights</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">);</span>
<span class="k">del</span> <span class="n">combined_weights</span>  <span class="c1"># free up memory</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "eabfde3ef38b436ea750e6fb50a02b5c", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "e117ad45771747ae95c16f9876e6dc19", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "170185f2f046437dab57c2ad23163c5c", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "6e65f5d6c5af4ab78bc7b3778b98ef86", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>

<span class="n">token_ids</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">idx</span><span class="o">=</span><span class="n">text_to_token_ids</span><span class="p">(</span><span class="s2">&quot;Every effort&quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
    <span class="n">context_size</span><span class="o">=</span><span class="n">LLAMA31_CONFIG_8B</span><span class="p">[</span><span class="s2">&quot;context_length&quot;</span><span class="p">],</span>
    <span class="n">top_k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output text:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">token_ids_to_text</span><span class="p">(</span><span class="n">token_ids</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Output text:
 Every effort has been made to trace copyright holders and to obtain their permission for the use of copyright material. The publisher apologizes for any
</pre></div>
</div>
</div>
</div>
<p> </p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="llama-3-2-1b">
<h1>Llama 3.2 1B<a class="headerlink" href="#llama-3-2-1b" title="Link to this heading">#</a></h1>
<ul class="simple">
<li><p>As of this writing, Meta AI’s latest models are the Llama 3.2 models announced <a class="reference external" href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/">here</a></p></li>
<li><p>The code for the Llama 3.2 text model is similar to that of Llama 3.1, except that the model has shrunk in size (there is a 1B and 3B version)</p></li>
<li><p>The other efficiency tweak was that they added back weight tying (a concept that was original used in the GPT-2 architecture); here, they reuse the same weight parameter values in the input (token) embedding layer and output layer</p></li>
<li><p>The small model size of Llama 3.2 1B is quite convenient, since it can even run on many mobile devices</p></li>
<li><p>The architectural differences between Llama 3.1 8B and Llama 3.2 1B are illustrated in the figure below</p></li>
</ul>
<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/gpt-to-llama/llama31-to-llama32.webp?1" width="700px"><ul class="simple">
<li><p>As we can see based on the figure above, the main difference between the Llama 3.1 8B and Llama 3.2 1B architectures are the respective sizes</p></li>
<li><p>A small additional change is an increased RoPE rescaling factor, which is reflected in the configuration file below</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">LLAMA31_CONFIG_8B</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">128_256</span><span class="p">,</span>      <span class="c1"># Vocabulary size</span>
    <span class="s2">&quot;context_length&quot;</span><span class="p">:</span> <span class="mi">131_072</span><span class="p">,</span>  <span class="c1"># NEW: Larger supported context length</span>
    <span class="s2">&quot;emb_dim&quot;</span><span class="p">:</span> <span class="mi">4096</span><span class="p">,</span>            <span class="c1"># Embedding dimension</span>
    <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>              <span class="c1"># Number of attention heads</span>
    <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>             <span class="c1"># Number of layers</span>
    <span class="s2">&quot;hidden_dim&quot;</span><span class="p">:</span> <span class="mi">14_336</span><span class="p">,</span>       <span class="c1"># Size of the intermediate dimension in FeedForward</span>
    <span class="s2">&quot;n_kv_groups&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>           <span class="c1"># Key-Value groups for grouped-query attention</span>
    <span class="s2">&quot;rope_base&quot;</span><span class="p">:</span> <span class="mf">500_000.0</span><span class="p">,</span>     <span class="c1"># The base in RoPE&#39;s &quot;theta&quot;</span>
    <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>    <span class="c1"># Lower-precision dtype to reduce memory usagey</span>
    <span class="s2">&quot;rope_freq&quot;</span><span class="p">:</span> <span class="p">{</span>              <span class="c1"># NEW: RoPE frequency scaling</span>
        <span class="s2">&quot;factor&quot;</span><span class="p">:</span> <span class="mf">8.0</span><span class="p">,</span>
        <span class="s2">&quot;low_freq_factor&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="s2">&quot;high_freq_factor&quot;</span><span class="p">:</span> <span class="mf">4.0</span><span class="p">,</span>
        <span class="s2">&quot;original_context_length&quot;</span><span class="p">:</span> <span class="mi">8192</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">}</span>


<span class="n">LLAMA32_CONFIG_1B</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">128_256</span><span class="p">,</span>      <span class="c1"># Vocabulary size</span>
    <span class="s2">&quot;context_length&quot;</span><span class="p">:</span> <span class="mi">131_072</span><span class="p">,</span>  <span class="c1"># Context length</span>
    <span class="s2">&quot;emb_dim&quot;</span><span class="p">:</span> <span class="mi">2048</span><span class="p">,</span>            <span class="c1"># NEW: Half the embedding dimension</span>
    <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>              <span class="c1"># Number of attention heads</span>
    <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>             <span class="c1"># NEW: Half the number of layers</span>
    <span class="s2">&quot;hidden_dim&quot;</span><span class="p">:</span> <span class="mi">8192</span><span class="p">,</span>         <span class="c1"># NEW: Almost half the size of the intermediate dimension in FeedForward</span>
    <span class="s2">&quot;n_kv_groups&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>           <span class="c1"># Key-Value groups for grouped-query attention</span>
    <span class="s2">&quot;rope_base&quot;</span><span class="p">:</span> <span class="mf">500_000.0</span><span class="p">,</span>     <span class="c1"># The base in RoPE&#39;s &quot;theta&quot;</span>
    <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>    <span class="c1"># Lower-precision dtype to reduce memory usage</span>
    <span class="s2">&quot;rope_freq&quot;</span><span class="p">:</span> <span class="p">{</span>              <span class="c1"># RoPE frequency scaling</span>
        <span class="s2">&quot;factor&quot;</span><span class="p">:</span> <span class="mf">32.0</span><span class="p">,</span>         <span class="c1"># NEW: Adjustment of the rescaling factor</span>
        <span class="s2">&quot;low_freq_factor&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="s2">&quot;high_freq_factor&quot;</span><span class="p">:</span> <span class="mf">4.0</span><span class="p">,</span>
        <span class="s2">&quot;original_context_length&quot;</span><span class="p">:</span> <span class="mi">8192</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Reduce the context length so the model would work fine on a MacBook Air (if you have more RAM, feel free to comment out the lines below):</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">old_context_length</span> <span class="o">=</span> <span class="n">LLAMA32_CONFIG_1B</span><span class="p">[</span><span class="s2">&quot;context_length&quot;</span><span class="p">]</span>
<span class="n">LLAMA32_CONFIG_1B</span><span class="p">[</span><span class="s2">&quot;context_length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">8192</span>

<span class="n">LLAMA32_CONFIG_1B</span><span class="p">[</span><span class="s2">&quot;rope_base&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">rescale_theta</span><span class="p">(</span>
    <span class="n">LLAMA32_CONFIG_1B</span><span class="p">[</span><span class="s2">&quot;rope_base&quot;</span><span class="p">],</span>
    <span class="n">old_context_length</span><span class="p">,</span>
    <span class="n">LLAMA32_CONFIG_1B</span><span class="p">[</span><span class="s2">&quot;context_length&quot;</span><span class="p">]</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;New RoPE theta:&quot;</span><span class="p">,</span> <span class="n">LLAMA32_CONFIG_1B</span><span class="p">[</span><span class="s2">&quot;rope_base&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>New RoPE theta: 31250.0
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Below, we can reuse the code from the Llama 3.1 8B section to load the Llama 3.2 1B model</p></li>
<li><p>Again, since the Llama 3.2 family is distinct from the Llama 3.1 family, you’d have to go to the <a class="reference external" href="https://huggingface.co/meta-llama/Llama-3.2-1B">meta-llama/Llama-3.2-1B</a> repository and acknowledge the license terms for your Hugging Face access token to work for the download</p></li>
<li><p>Tip: For simplicity, we only load the base model below, but there’s also an instruction-finetuned version you can use by replacing <code class="docutils literal notranslate"><span class="pre">&quot;meta-llama/Llama-3.2-1B&quot;</span></code> with <code class="docutils literal notranslate"><span class="pre">&quot;meta-llama/Llama-3.2-1B-Instruct&quot;</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># free up memory</span>
<span class="k">del</span> <span class="n">model</span>


<span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>  <span class="c1"># Run Python garbage collector</span>

<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer_file_path</span> <span class="o">=</span> <span class="n">hf_hub_download</span><span class="p">(</span>
    <span class="n">repo_id</span><span class="o">=</span><span class="s2">&quot;meta-llama/Llama-3.2-1B&quot;</span><span class="p">,</span>
    <span class="n">filename</span><span class="o">=</span><span class="s2">&quot;original/tokenizer.model&quot;</span><span class="p">,</span>
    <span class="n">local_dir</span><span class="o">=</span><span class="s2">&quot;Llama-3.2-1B&quot;</span>
<span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">tokenizer_file_path</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Llama3Model</span><span class="p">(</span><span class="n">LLAMA32_CONFIG_1B</span><span class="p">)</span>

<span class="n">total_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total number of parameters: </span><span class="si">{</span><span class="n">total_params</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Account for weight tying</span>
<span class="n">total_params_normalized</span> <span class="o">=</span> <span class="n">total_params</span> <span class="o">-</span> <span class="n">model</span><span class="o">.</span><span class="n">tok_emb</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Total number of unique parameters: </span><span class="si">{</span><span class="n">total_params_normalized</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total number of parameters: 1,498,482,688

Total number of unique parameters: 1,235,814,400
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">weights_file</span> <span class="o">=</span> <span class="n">hf_hub_download</span><span class="p">(</span>
    <span class="n">repo_id</span><span class="o">=</span><span class="s2">&quot;meta-llama/Llama-3.2-1B&quot;</span><span class="p">,</span>
    <span class="n">filename</span><span class="o">=</span><span class="s2">&quot;model.safetensors&quot;</span><span class="p">,</span>
    <span class="n">local_dir</span><span class="o">=</span><span class="s2">&quot;Llama-3.2-1B&quot;</span>
<span class="p">)</span>
<span class="n">current_weights</span> <span class="o">=</span> <span class="n">load_file</span><span class="p">(</span><span class="n">weights_file</span><span class="p">)</span>

<span class="n">load_weights_into_llama</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">LLAMA32_CONFIG_1B</span><span class="p">,</span> <span class="n">current_weights</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">);</span>
<span class="k">del</span> <span class="n">current_weights</span>  <span class="c1"># free up memory</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "c309c56a6cdf426e8ba7967b6a21864e", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model uses weight tying.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Weight tying:&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">tok_emb</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">out_head</span><span class="o">.</span><span class="n">weight</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Weight tying: True
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>

<span class="n">token_ids</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">idx</span><span class="o">=</span><span class="n">text_to_token_ids</span><span class="p">(</span><span class="s2">&quot;Every effort&quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
    <span class="n">context_size</span><span class="o">=</span><span class="n">LLAMA32_CONFIG_1B</span><span class="p">[</span><span class="s2">&quot;context_length&quot;</span><span class="p">],</span>
    <span class="n">top_k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output text:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">token_ids_to_text</span><span class="p">(</span><span class="n">token_ids</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Output text:
 Every effort is made to ensure that the information on this website is accurate. However, we cannot guarantee that the information is accurate, complete
</pre></div>
</div>
</div>
</div>
<p> </p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="what-s-next">
<h1>What’s next?<a class="headerlink" href="#what-s-next" title="Link to this heading">#</a></h1>
<ul class="simple">
<li><p>This notebook concludes the conversion from GPT to Llama 3.2</p></li>
<li><p>If you are interested in a more compact, standalone notebook, which only contains the Llama 3.2 code, check out the <a class="reference internal" href="standalone-llama32.html"><span class="std std-doc">standalone-llama32.ipynb</span></a> notebook</p></li>
</ul>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./ch05/07_gpt_to_llama"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="converting-gpt-to-llama2.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Converting a From-Scratch GPT Architecture to Llama 2</p>
      </div>
    </a>
    <a class="right-next"
       href="standalone-llama32.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Llama 3.2 From Scratch (A Standalone Notebook)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Converting Llama 2 to Llama 3.2 From Scratch</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#convert-the-llama-model-implementation-step-by-step">1. Convert the Llama model implementation step by step</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reusing-llama-2-components">1.1 Reusing Llama 2 components</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modified-rope">1.2 Modified RoPE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#grouped-query-attention">1.3 Grouped-query attention</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#update-the-transformerblock-module">1.4 Update the TransformerBlock module</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-the-model-class">1.5 Defining the model class</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#initialize-model">2. Initialize model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#load-tokenizer">3. Load tokenizer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#load-pretrained-weights">4. Load pretrained weights</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-the-instruction-finetuned-model">5. Using the instruction-finetuned model</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#llama-3-1-8b">Llama 3.1 8B</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#llama-3-2-1b">Llama 3.2 1B</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#what-s-next">What’s next?</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sebastian Raschka
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>